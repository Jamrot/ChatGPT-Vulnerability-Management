{"Ambari": {"12": {"ground_truth": "0", "bug_report": "Add additional transition states\nFor running a server (daemon)  the current states are: START and STARTED. It would be nice to have transition state: STARTING  and STOPPING.", "id": "12"}, "46": {"ground_truth": "0", "bug_report": "Preserve cluster id  blueprint name  and blueprint revision in agent local disk\nAfter agent execute commands on behave of the controller  agent should store cluster id  blueprint name and blueprint revision in the local disk to keep track of the current deployment state on the agent.", "id": "46"}, "92": {"ground_truth": "0", "bug_report": "Agent should retry heartbeat message  if controller did not receive the heartbeat\nIf the heartbeat was not received correctly by the controller  then it should retry.", "id": "92"}, "103": {"ground_truth": "0", "bug_report": "Remove agent entities beans from public schema xsd\nAgent entities beans are located ambari-client: org.apache.ambari.common.rest.entities.agent. Hence  schema xsd generation for public rest api will include agent entities beans. We should move it to org.apache.ambari.common.rest.agent to exclude agent entities to expose to public.", "id": "103"}, "192": {"ground_truth": "0", "bug_report": "Check for NN safemode during restarts\nThere is no checks for safemode when we reconfigure the cluster.", "id": "192"}, "198": {"ground_truth": "0", "bug_report": "Dependency of templeton on hcat client\nhcat client should be installed at the templeton server node.", "id": "198"}, "199": {"ground_truth": "0", "bug_report": "Remove import of mysql puppet module from manifest.\nRemove import of mysql puppet module from manifest because this module is deprecated.", "id": "199"}, "202": {"ground_truth": "0", "bug_report": "Add check to verify jdk path after install\nAfter the jdk install  we do not validate the path. This causes problems during service start in later stages.", "id": "202"}, "207": {"ground_truth": "0", "bug_report": "PHP Notice: Undefined variable: manifest in /usr/share/hmc/php/puppet/genmanifest/hostsConfig.php\nUndefined variable used.", "id": "207"}, "208": {"ground_truth": "0", "bug_report": "Support filtering hosts based on discovery status\nApi to get hosts should allow filtering out bad hosts", "id": "208"}, "217": {"ground_truth": "0", "bug_report": "Alert table on the right needs to be tied visually/verbally to the context/content it is displaying\nCurrently in Dashboard when service entry is clicked  right side alerts table caption does not indicate that it is showing alerts related to that service. So Cpation of the table should change indicating the corresponding service name.", "id": "217"}, "222": {"ground_truth": "0", "bug_report": "Remove the word alert from all the Nagios alerts descriptions.\nIT is sort of redundant..", "id": "222"}, "226": {"ground_truth": "0", "bug_report": "Make the daemon names and other field names consistent\nFollowing names need to be consistent: Hdfs -&gt; HDFS Mapreduce -&gt; MapReduce Zookeeper -&gt; ZooKeeper HADOOP -&gt; Hadoop", "id": "226"}, "232": {"ground_truth": "0", "bug_report": "Enable LZO should show checkbox instead of text\nCurrently the enable lzo option shows a text box that needs to be filled with true/false. Changing the UI element to checkbox.", "id": "232"}, "236": {"ground_truth": "0", "bug_report": "Increase puppet agent timeout.\nPuppet agent timeout should be increases as sometimes (possibly due to a bug in ruby) puppet master takes long time to compile and send back the catalog.", "id": "236"}, "237": {"ground_truth": "0", "bug_report": "Refactor puppet kick loop to easily change retries and timeouts.\nRefactor puppet kick loop to easily change retries and timeouts.", "id": "237"}, "245": {"ground_truth": "0", "bug_report": "Support data cleanup if installation fails.\nWe need to support data cleanup so that a cluster can be re-installed in case of failures.", "id": "245"}, "247": {"ground_truth": "0", "bug_report": "Replace index.php with clusters.php\nLike the title says: overwrite index.php with the contents of clusters.php  making sure to add a link to the AddNodesWizard as well.", "id": "247"}, "249": {"ground_truth": "0", "bug_report": "Uninstall support from UI\nUninstall/wipeout support from UI.", "id": "249"}, "252": {"ground_truth": "0", "bug_report": "Remove 'Playground' files from HMC\nThere's a bunch of (temporary playground) files that got wrongly committed to the HMC codebase  so this is to remove all of them and get things into a cleaner state (at least on the face of things).", "id": "252"}, "253": {"ground_truth": "0", "bug_report": "Support uninstall state in mysql modules\nCurrently  there is no support for uninstall of mysql package.", "id": "253"}, "255": {"ground_truth": "0", "bug_report": "Rename/Relocate files as appropriate\nThere's some images in the html/ directory (that should be in images/)  there's .html files whose extension should be changed to .htmli  and such  that would be nice to clean up.", "id": "255"}, "256": {"ground_truth": "1", "bug_report": "Update hive config to enable authorization\nIn /etc/hive/conf/hive-site.xml&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;enable or disable the hive client authorization&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt; &lt;value&gt;org.apache.hcatalog.security.HdfsAuthorizationProvider&lt;/value&gt; &lt;description&gt;the hive client authorization manager class name. The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider. &lt;/description&gt;&lt;/property&gt;In /etc/hive/conf/hive-env.sh  add -export HIVE_AUX_JARS_PATH=/usr/lib/hcatalog/share/hcatalog/hcatalog-0.4.0.jar", "id": "256"}, "257": {"ground_truth": "0", "bug_report": "Manage services section will have any empty section when no client only components installed\nOn the manage services page we have a section for client only services that have no long running processes.If the user has no such component there is a heading but no content. Maybe we can hide the heading when no such service is present.", "id": "257"}, "262": {"ground_truth": "1", "bug_report": "Init Wizard: Advanced Config validation errors can be bypassed\nMake the Naigos password (and re-type password) different so as you cause a validation error.This will let the user move on to the next screen by ignoring all other validation errors on this page.", "id": "262"}, "264": {"ground_truth": "0", "bug_report": "Nagios Admin Contact should be checked to ensure it is always an email address\nNagios Admin Contact should be checked to ensure it is always an email address", "id": "264"}, "274": {"ground_truth": "0", "bug_report": "Templeton data on hdfs needs to be readable by all users\nContent of /user/templeton dir-bash-3.2$ hadoop dfs -ls /user/templetonFound 4 items-rw-r--r-- 3 templeton hdfs 107373 2012-05-14 19:53 /user/templeton/hadoop-streaming.jar-rw------- 3 templeton hdfs 35352096 2012-05-14 19:53 /user/templeton/hive.tar.gz-rw------- 3 templeton hdfs 47909478 2012-05-14 19:53 /user/templeton/pig.tar.gz-rw------- 3 templeton hdfs 127652 2012-05-14 19:53 /user/templeton/ugi.jarOnly templeton user can use the jars", "id": "274"}, "276": {"ground_truth": "0", "bug_report": "Update HDFS parameter configuration description\nUpdating the short description and tooltip long description as follows:Changing the text that affects WebUI as follows: filesystem -&gt; file system HDFS Append Enabled -&gt; Append enabled HDFS WebHDFS Enabled -&gt; WebHDFS enabled Hadoop maximum Java heap size -&gt; Hadoop maximum Java heap size (MB)  Java Heap Size for slave daemons -&gt; Maximum Java heap size for daemons such as Balancer in MB (-Xmx)   NameNode maximum Java heap size -&gt; NameNode initial Java heap size (MB)  Java Heap Size for NameNode -&gt; Initial and minimum Java heap size for NameNode in MB (-Xms)   Hadoop Young Generation heap size -&gt; NameNode new generation size (MB)  Maximum size for New Generation for java heap size -&gt; Default size of Java new generation in MB for NameNode (-XX:NewSize)   DataNode Java heap size -&gt; DataNode maximum Java heap size (MB)  Java Heap Size for DataNode -&gt; Maximum Java heap size for DataNode in MB (-Xmx)", "id": "276"}, "280": {"ground_truth": "0", "bug_report": "Cleanup of utilities\nWe need an api for the UI to figure out the status of the cluster.", "id": "280"}, "283": {"ground_truth": "0", "bug_report": "Fixup review and deploy rendering\nCurrently render of the review and deploy page is messy.", "id": "283"}, "287": {"ground_truth": "0", "bug_report": "Add link to uninstall on index page\nUninstall link is now hooked into the index page.", "id": "287"}, "290": {"ground_truth": "0", "bug_report": "Comment in addNodesWizardInit.js.\nComment in addNodesWizardInit.js.", "id": "290"}, "292": {"ground_truth": "0", "bug_report": "HTML being spewed in the Review+Deploy page.\nHTML being spewed in the Review+Deploy page.", "id": "292"}, "300": {"ground_truth": "0", "bug_report": "Change the status message (success/error) location so that it shows below the page summary box  rather than above  more better visibility\nChange the status message (success/error) location so that it shows below the page summary box  rather than above  more better visibility", "id": "300"}, "302": {"ground_truth": "0", "bug_report": "regionservers config in the hbase only has localhost in it\n/etc/hbase/conf/regionserver should correctly populate the slaves list", "id": "302"}, "304": {"ground_truth": "0", "bug_report": "Upgrade to yui-3.5.1\nUpgrade to yui-3.5.1", "id": "304"}, "310": {"ground_truth": "0", "bug_report": "Externalize message resources for the welcome page.  Update styles on various pages.\nExternalize message resources for the welcome page.  Update styles on various pages.", "id": "310"}, "312": {"ground_truth": "0", "bug_report": "Uninstall's wipe flag should be correctly passed to puppet\nUninstall's wipe flag should be correctly passed to puppet", "id": "312"}, "316": {"ground_truth": "0", "bug_report": "Grid mount points page doesn't let one pass with only a custom mount point\nGrid mount points page doesn't let one pass with only a custom mount point", "id": "316"}, "319": {"ground_truth": "0", "bug_report": "Scale puppet master to large number of nodes.\nScale puppet master to large number of nodes.", "id": "319"}, "323": {"ground_truth": "0", "bug_report": "During any process in the cluster initialization wizard  if the user goes back to the '1 Create Cluster' tab  the user is stuck.\nDuring any process in the cluster initialization wizard  if the user goes back to the '1 Create Cluster' tab  the user is stuck.", "id": "323"}, "326": {"ground_truth": "0", "bug_report": "Dependencies should be added only during install phase\nHost level dependencies are being set in running stage as well  which is redundant. It can be assumed that dependencies were installed at install stage.", "id": "326"}, "330": {"ground_truth": "0", "bug_report": "Provide a way to resume if browser crashes/is closed during the deploy-in-progress\nCurrently when the browser is closed one cannot view the install in progress. They will have to look at the logs on the hmc machine to figure out whats going on. It would be nice if we can provide a way to get back to the install progress.", "id": "330"}, "335": {"ground_truth": "0", "bug_report": "Redundant downloads even though the artifacts are already installed\nArtifacts (such as mysql-connector.zip  hive.tar.gz  pig.tar.gz  ext.zip) are being downloaded even though they are previously installed leading to additional execution time.", "id": "335"}, "338": {"ground_truth": "0", "bug_report": "Cluster status update needs to happen for all stages of installation wizard.\nCluster status should be updated in db for each stage of the installation wizard. This is used to enable restart of browser and also showing status of the cluster on the index page.", "id": "338"}, "339": {"ground_truth": "0", "bug_report": "Making transitionToNextStage more robust\nIf the currentStage is null  we should not proceed with a transition.", "id": "339"}, "349": {"ground_truth": "0", "bug_report": "Logging in case of error during uninstall needs to be fixed.\nLogs disappear post uninstall because the transaction is also cleaned from the db.", "id": "349"}, "352": {"ground_truth": "0", "bug_report": "Add flow control - force redirects to appropriate pages based on cluster configuration status for better usability\nIf no cluster has been set up yet  redirect to the welcome page.If a cluster is being configured (but has not gone thru deployment)  redirect to Step 1 of the cluster init wizard.If a cluster is being deployed  redirect to the deployment progress page.If a cluster has gone thru deployment but failed  redirect to the re-install page.If a cluster has gone thru deployment and succeed  do not perform any forced redirect.", "id": "352"}, "357": {"ground_truth": "0", "bug_report": "Redesign master service assignment page so that it takes up less vertical space\nRedesign master service assignment page so that it takes up less vertical space", "id": "357"}, "362": {"ground_truth": "0", "bug_report": "Create lock file as part of rpm install\nLock file is being created as part of create cluster. This is brittle and needs to be done as part of the rpm install.", "id": "362"}, "366": {"ground_truth": "0", "bug_report": "Package up the fonts/ subdirectory in the HMC RPM\nThe new fonts/ subdirectory used for the buttons on the ManageServices page isn't packaged up.", "id": "366"}, "369": {"ground_truth": "0", "bug_report": "Improve Service Management page and general popup styling\nImprove Service Management page and general popup styling", "id": "369"}, "371": {"ground_truth": "0", "bug_report": "Mysql packages not being sent during install and uninstall\nMysql packages not being sent during install and uninstall.", "id": "371"}, "377": {"ground_truth": "0", "bug_report": "Uninstall does not handle component dependencies.\nUninstall does not handle component dependencies.", "id": "377"}, "386": {"ground_truth": "0", "bug_report": "On Single Node install when install all the components the recommended num for Map/Reduce Tasks is too high\nUse lower count of maps/reduce slots for a single node install.", "id": "386"}, "393": {"ground_truth": "0", "bug_report": "ZooKeeper myid files not existent on ZK install.\nZooKeeper myid files not existent on ZK install.", "id": "393"}, "394": {"ground_truth": "0", "bug_report": "Add nodes fails to find node in db\nHost not found in db error on assign nodes page", "id": "394"}, "399": {"ground_truth": "0", "bug_report": "Cannot uninstall - the page hangs with the spinning icon\nCannot uninstall - the page hangs with the spinning icon", "id": "399"}, "401": {"ground_truth": "0", "bug_report": "Manual config changes for nn get reset on stop/start from hmc\nManual config changes for nn get reset on stop/start from hmc", "id": "401"}, "402": {"ground_truth": "0", "bug_report": "Completing successful add node takes one to initialize cluster page starting from scratch\nCompleting successful add node takes one to initialize cluster page starting from scratch", "id": "402"}, "404": {"ground_truth": "0", "bug_report": "Unify the top nav for both Monitoring and Cluster Management\nUnify the top nav for both Monitoring and Cluster Management", "id": "404"}, "414": {"ground_truth": "0", "bug_report": "Add rpm spec for hmc agent.\nAdd rpm spec for hmc agent.", "id": "414"}, "415": {"ground_truth": "0", "bug_report": "Reset service back to original state after reconfiguration\nReset service back to original state after reconfiguration", "id": "415"}, "420": {"ground_truth": "0", "bug_report": "Improve style on error log popups\nImprove style on error log popups", "id": "420"}, "426": {"ground_truth": "0", "bug_report": "Reinstall of cluster after failure to install results in failure\nAccording to Bikas:I tried to install a single node cluster. That failed for some random issue.I was presented an option to reinstall.I clicked on that option and it asked me to uninstall.I chose uninstall and wipe data option.Uninstall failed.This happened because the install did not complete in the first place.Thu May 24 17:45:28 -0400 2012 /Stage17/Hdp-oozie::Service/Hdp-oozie::Service::Createsymlinks/usr/lib/oozie/oozie-server/lib/mapred-site.xml/File/usr/lib/oozie/oozie-server/lib/mapred-site.xml/ensure (err): change from absent to present failed: Could not set 'present on ensure: No such file or directory - /usr/lib/oozie/oozie-server/lib/mapred-site.xml at /etc/puppet/agent/modules/hdp-oozie/manifests/service.pp:61", "id": "426"}, "429": {"ground_truth": "0", "bug_report": "Fix bug with jmx parsing on HBase.\nFix bug with jmx parsing on HBase.", "id": "429"}, "433": {"ground_truth": "0", "bug_report": "Using service stop instead of killall for uninstall\nUsing service stop instead of killall for uninstall", "id": "433"}, "435": {"ground_truth": "0", "bug_report": "Uninstall needs to update status for failure.\nThis is to enable the routing layer to redirect appropriately.", "id": "435"}, "442": {"ground_truth": "0", "bug_report": "Duplicate definition: Class[Hdp-hbase::Regionserver::Enable-ganglia]\nDuplicate definition: Class&#91;Hdp-hbase::Regionserver::Enable-ganglia&#93; is already defined; cannot redefine at /etc/puppet/agent/modules/hdp-ganglia/manifests/monitor.pp:37 on node ip-10-64-19-248.ec2.internal", "id": "442"}, "449": {"ground_truth": "0", "bug_report": "Post cluster install/deploy the URL hmc/html/initializeCluster.php should be disabled\nInstall a cluster. Then go to http://&lt;host&gt;/hmc/html/initializeCluster.php URL. You get a page enter cluster name. If you enter a cluster name  existing install is wiped out. We need to disable this URL on an installed cluster.Only way to enable this should be uninstall.", "id": "449"}, "450": {"ground_truth": "0", "bug_report": "Boldify/Redify restart HMC message when nagios/ganglia is on the hmc host\nBoldify/Redify restart HMC message when nagios/ganglia is on the hmc host", "id": "450"}, "455": {"ground_truth": "0", "bug_report": "nagios shows service status critical if hbase is not installed\nnagios shows service status critical if hbase is not installed", "id": "455"}, "466": {"ground_truth": "0", "bug_report": "Add nodes page alerts removed in case of adding duplicate nodes\nAlerts that are being shown needs to be changed to be compatible with current implementation of showing errors. A link now appears to 'Show the duplicate nodes' which shows the duplicate nodes.", "id": "466"}, "467": {"ground_truth": "0", "bug_report": "Fix hive stop to escape $.\nFix hive stop to escape $.", "id": "467"}, "468": {"ground_truth": "0", "bug_report": "Post-Install Add Nodes - update progress title and success/error messages to reflect what it's actually doing/has done\nPost-Install Add Nodes - update progress title and success/error messages to reflect what it's actually doing/has done", "id": "468"}, "475": {"ground_truth": "0", "bug_report": "Add missing JS file for making post cluster install Add Nodes work\nAdd missing JS file for making post cluster install Add Nodes work", "id": "475"}, "482": {"ground_truth": "0", "bug_report": "Show the same welcome page to the user if the user starts configuring a cluster but has not started deploy yet\nShow the same welcome page to the user if the user starts configuring a cluster but has not started deploy yet", "id": "482"}, "486": {"ground_truth": "0", "bug_report": "Add Node installs MySQL Server for Hive\nAdding slave nodes post cluster install will install MySQL Server if Hive was selected as a service at the time of cluster installation.", "id": "486"}, "488": {"ground_truth": "0", "bug_report": "Manage service needs a way to recover from terminated browser sessions\nWe need to block users from being able to start/stop services when we have a batch of start/stop activities are already in progress.Do something similar to deployment progress display and bring the user back to that modal status display which will end when we detect success/failure.", "id": "488"}, "491": {"ground_truth": "0", "bug_report": "Service Reconfiguration screens should respect the 'reconfigurable' attributes set in ConfigProperties table\nSome service config parameters are editable (can be customized on initial install)  but cannot be reconfigured post cluster install.This info is stored in ConfigProperties table  but the Service Reconfiguration screens are allowing these non-reconfigurable parameters to be changed.", "id": "491"}, "492": {"ground_truth": "0", "bug_report": "make support for os check a bit more robust\nmake support for os check a bit more robust", "id": "492"}, "493": {"ground_truth": "0", "bug_report": "Add rack_info as column in Hosts table\nAdd rack_info as column in Hosts table", "id": "493"}, "494": {"ground_truth": "1", "bug_report": "Fix node assignments not not allow slaves on master.\nFix node assignments not not allow slaves on master.", "id": "494"}, "501": {"ground_truth": "0", "bug_report": "Speed up page load/reload times\nSpeed up page load/reload times", "id": "501"}, "508": {"ground_truth": "0", "bug_report": "Support Resume For Add Nodes\nJust like for Manage Services + Deploy + Uninstall.", "id": "508"}, "510": {"ground_truth": "0", "bug_report": "Modify the router to force redirection to 'Add Nodes Progress' popup\nModify the router to force redirection to 'Add Nodes Progress' popup", "id": "510"}, "512": {"ground_truth": "0", "bug_report": "Fix puppet manifests for tarball downloads via rpms.\nFix puppet manifests for tarball downloads via rpms.", "id": "512"}, "519": {"ground_truth": "0", "bug_report": "update to fix the ganglia monitor_and_server anchor problem\nupdate to fix the ganglia monitor_and_server anchor problem", "id": "519"}, "528": {"ground_truth": "0", "bug_report": "Fix oozie smoke test failure\nOozie smoke test failing with 'Error: E0301 : E0301: Invalid resource &#91;usr/lib/oozie/conf&#93;'", "id": "528"}, "529": {"ground_truth": "0", "bug_report": "Fix Advanced Config: HDFS reserved space is in bytes. Too many bytes to count.\nSimplify user input.", "id": "529"}, "539": {"ground_truth": "0", "bug_report": "Create a spec file with less dependencies for HMC\nSimplify process for users that want to use different dependencies for PHP and Ruby.e.g PHP-5.3  ruby-1.8.7", "id": "539"}, "543": {"ground_truth": "0", "bug_report": "Rpm naming needs to be corrected.\nRpm naming needs to be corrected.", "id": "543"}, "544": {"ground_truth": "0", "bug_report": "Templeton configs for pig archive not correct in HMC\nFrom Arpit:The configs for pig in templeton are wrong.The deployed configs are&lt;property&gt; &lt;name&gt;templeton.pig.archive&lt;/name&gt; &lt;value&gt;hdfs:///apps/templeton/&lt;/value&gt; &lt;description&gt;The path to the Pig archive.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.pig.path&lt;/name&gt; &lt;value&gt;/pig-0.9.2/bin/pig&lt;/value&gt; &lt;description&gt;The path to the Pig executable.&lt;/description&gt; &lt;/property&gt;They are missing the pig tar ball. For example they should be&lt;property&gt; &lt;name&gt;templeton.pig.archive&lt;/name&gt; &lt;value&gt;hdfs:///apps/templeton/pig.tar.gz&lt;/value&gt; &lt;description&gt;The path to the Pig archive.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.pig.path&lt;/name&gt; &lt;value&gt;pig.tar.gz/pig-0.9.2/bin/pig&lt;/value&gt; &lt;description&gt;The path to the Pig executable.&lt;/description&gt; &lt;/property&gt;So both the properties are missing 'pig-0.9.2.tar.gz'", "id": "544"}, "546": {"ground_truth": "0", "bug_report": "Puppet fails to install 32-bit JDK properly on RHEL6\nmkdir -p /usr/jdk32 ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-i586.bin; cd /usr/jdk32 ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-i586.bin -noregister 2&gt;&amp;1Unpacking...Checksumming...Extracting.../var/www/html/downloads/jdk-6u31-linux-i586.bin: ./install.sfx.1794: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directoryFailed to extract the files. Please refer to the Troubleshooting section ofthe Installation Instructions on the download page for more information.Puppet should ensure glibc.i686 is installed before trying to install jdk.", "id": "546"}, "547": {"ground_truth": "0", "bug_report": "Change os type check during node bootstrap to allow RHEL6 or CentOS6 nodes\nhmc/php/frontend/addNodes/verifyAndUpdateNodesInfo.php hardcodes checks to only allow rhel5 or centos5 nodes.", "id": "547"}, "548": {"ground_truth": "0", "bug_report": "Puppet agent install script should use correct epel repo\nhmc/ShellScripts/puppet_agent_install.sh hardcodes to epel-release rpm for CentOS-5.4. Should be more intelligent to handle different OS types.", "id": "548"}, "550": {"ground_truth": "0", "bug_report": "Add support to jump to a specified state in the wizard for development purposes\nAdd support to jump to a specified state in the wizard for development purposes", "id": "550"}, "552": {"ground_truth": "0", "bug_report": "Update README to point to trunk\nWe need to fix the README to point to trunk after the merge of ambari-186 branch.", "id": "552"}, "565": {"ground_truth": "0", "bug_report": "Remove YUI source files from SVN\nCurrently YUI 3.5.1 source files are in SVN.We don't really need to have them checked into SVN as we don't track changes. We are pre-concatenating and minifying the YUI js files  so we don't need the files to run Ambari anyhow.We should just remove them from SVN and instead just checkin the tarball.", "id": "565"}, "566": {"ground_truth": "0", "bug_report": "Update documentation\nModify pom.xml to create output files under ../site  rather than ./target.Make minor wording modifications.", "id": "566"}, "569": {"ground_truth": "0", "bug_report": "Nagios install fails on RHEL6\nPuppet layer  when trying to install nagios  tries to install php-pecl-json when it is not needed and fails when trying to do so. On RHEL6  php-5.3 is default which has the json module built into it.", "id": "569"}, "570": {"ground_truth": "0", "bug_report": "Consolidate head tags for organization and combine CSS files for faster load\nConsolidate head tags for organization and combine CSS files for faster load", "id": "570"}, "573": {"ground_truth": "0", "bug_report": "Puppet error: Cannot reassign variable zookeeper_hosts at modules/hdp/manifests/params.pp:47\nSeems like a typo: $zookeeper_hosts = hdp_default('zookeeper_hosts')Should be $public_zookeeper_hosts", "id": "573"}, "576": {"ground_truth": "0", "bug_report": "In Custom config for Nagios: emails with multiple periods before the '@' fails validation\nIn Custom config for Nagios: emails with multiple periods before the '@' fails validation", "id": "576"}, "578": {"ground_truth": "1", "bug_report": "Custom Config page: don't allow form submission if there are client-side validation errors\nCurrently the button looks disabled when there are client-side validation errors  but it is clickable. This is a problem because there is no server-side validation to make sure that the passwords match.", "id": "578"}, "581": {"ground_truth": "0", "bug_report": "special characters in hosts files created on some common windows editors causes issues\nspecial characters in hosts files created on some common windows editors causes issues", "id": "581"}, "587": {"ground_truth": "0", "bug_report": "Rat compliance patch\nAdding apache license to all the files within Ambari for rat tool compliance.", "id": "587"}, "588": {"ground_truth": "0", "bug_report": "Externalize the manager service name and point the Help link to a valid URL\nThe master role assignment page (Step 5 of install) and the cluster topology summary page are referencing the manager service name as 'HMC Server'. This needs to be changed to 'Ambari' server.The post-install success message on a single-node install has the same issue.Currently  the Help link in the top nav launches a new tab and loads the page that the user is on. Instead  load the Help page on Ambari project website (for now  we'll point it to the Install Guide as a placeholder).", "id": "588"}, "591": {"ground_truth": "1", "bug_report": "License header for PHP files should use PHP comments  not HTML comments\nSome PHP files have the HTML-style comments. This is problematic sincethe license headers are becoming part of the HTML response. Worse yet  the headers are repeated multiple times in the response. This can also cause unexpected behavior.", "id": "591"}, "592": {"ground_truth": "0", "bug_report": "Add a link to NOTICE file on every page\nThis is so that stuff with different compatible licenses can be attributed appropriately.", "id": "592"}, "597": {"ground_truth": "0", "bug_report": "Remove /usr/bin/php dependency from the rpm's\nRemove /usr/bin/php dependency from the rpm's", "id": "597"}, "600": {"ground_truth": "0", "bug_report": "Fix lzo installs to work correctly on RHEL6\nFix lzo installs to work correctly on RHEL6", "id": "600"}, "607": {"ground_truth": "0", "bug_report": "Increase puppet timeouts to handle single-node installs timing out\nIncrease puppet timeouts to handle single-node installs timing out", "id": "607"}, "614": {"ground_truth": "0", "bug_report": "The database set up script has a duplicate definition of AmbariConfig so install fails\nThe database set up script has a duplicate definition of AmbariConfig so install fails", "id": "614"}, "615": {"ground_truth": "0", "bug_report": "Eliminate redundant and unused definition for the columns in the table ConfigProperties\nConfigProperties table has a column named 'display_type'.There's also a JSON-encoded 'display_attributes' column.The 'display_attributes' column has the attributes 'isPassword'  'displayType'  and 'noDisplay'. This is duplicate information is already stored by the 'display_type' column so these attributes are not needed.Upon inspecting the code  these attributes are not used so changing them have no effect. We should simply get rid of these attributes.", "id": "615"}, "628": {"ground_truth": "1", "bug_report": "hdp-nagios and hdp-monitoring has wrong configuration file location  also owner:group permissions are wrong.\nSuse environment has wrong configuration location for hdp-dashboard and hdp-nagios. Also  owner:group permissions were wrongly set to root:root instead of 'wwwrun' and group is 'www", "id": "628"}, "633": {"ground_truth": "0", "bug_report": "Fix invalid HTML markup on Monitoring Dashboard\nFix invalid HTML markup on Monitoring Dashboard", "id": "633"}, "635": {"ground_truth": "0", "bug_report": "Add Nodes Progress: for partial failure that lets the user continue  display an orange bar rather than a red bar in the progress popup\nAdd Nodes Progress: for partial failure that lets the user continue  display an orange bar rather than a red bar in the progress popup", "id": "635"}, "636": {"ground_truth": "1", "bug_report": "Support for Hadoop Security (front-end changes)\nSupport for Hadoop Security (front-end changes)", "id": "636"}, "638": {"ground_truth": "1", "bug_report": "Weirdness with Custom Config page when the user goes back to previous stages\nIssue 1. Going back and forth between different stages in the Cluster Install Wizard  it is possible to get into a state where Custom Config form has the submit button disabled but no field errors are shown.Steps to replicate: Go up to Stage 6  but do not submit the form Go back to Stage 3 Go up to Stage 6 again. No field errors are shown when they should be.Issue 2. Custom Config stage is skipped once you get to 'Review and Deploy' and go back to a stage preceding Custom Config.Steps to replicate: Go thru the Cluster Install Wizard up to Stage 7 ('Review and Deploy')  but do not submit the form. Go back to Stage 4 or 5. Once you submit the form on Stage 5  Stage 6 is skipped and goes directly to Stage 7. If you go back to Stage 3 or earlier  then Stage 6 will not be skipped.", "id": "638"}, "668": {"ground_truth": "0", "bug_report": "Ambari should install yum priorities on all nodes to ensure main repo is picked first\nDepending on which main hadoop repo is used to setup the cluster  sometimes  wrong packages may be pulled from add-on repos even if the main repo has a higher priority and has the same package. This can create problems by incompatible dependencies getting installed at times.", "id": "668"}, "675": {"ground_truth": "0", "bug_report": "Make puppet generate more logs on command failures\nMake puppet generate more logs on command failures", "id": "675"}, "689": {"ground_truth": "0", "bug_report": "Fix ambari agent init.d scripts and the bootstrapping.\nFix ambari agent init.d scripts and the bootstrapping.", "id": "689"}, "701": {"ground_truth": "0", "bug_report": "Ambari does not handle a pre-setup user-supplied Hive Metastore\nAmbari treats a Hive Metastore as something that it still needs to install and setup even though it may not have the necessary permissions to do so.", "id": "701"}, "1072": {"ground_truth": "0", "bug_report": "Change text on alerts 'about XX hours ago'\nAlerts should have text like :OK for about 17 hoursWARN for about 17 hoursCRIT for about 17 hours", "id": "1072"}, "1081": {"ground_truth": "0", "bug_report": "HDFS disk capacity on dashboard is seen as negative number\nSometimes disk capacity calculations end up in negative numbers.", "id": "1081"}, "1085": {"ground_truth": "0", "bug_report": "Remove files from ambari-web that were not meant to be checked in\nambari-web/node_modules  ambari-web/public  ambari-web/ambari.iml were not meant to be checked in. Must remove.", "id": "1085"}, "1092": {"ground_truth": "0", "bug_report": "dashboard > Summary > capacity pie chart keeps changing colors\ndashboard > Summary > capacity pie chart keeps changing colors", "id": "1092"}, "1096": {"ground_truth": "0", "bug_report": "Create heatmap legend entries for missing data/invalid hosts\nCurrently we dont fill anything which is ambiguous.", "id": "1096"}, "1098": {"ground_truth": "0", "bug_report": "Switching services does not update various UI elements\nWhen you switch services in the UI  sometimes alerts are mismatched compared to selected service. Also the highlights in left-bar are not working.", "id": "1098"}, "1102": {"ground_truth": "0", "bug_report": "Error handling when errors are encountered during preparation for deploy\nCurrently  if any errors are encountered during preparation for deploy  the user is taken to the deploy page and the hosts will be shown as 'Waiting' but nothing happens. This is bad UX.At a minimum  we should prevent the user from proceeding and display an appropriate error message if any error is encountered after 'Deploy' is clicked  but before we transition to Step 9.We should also think about how a user can recover from this situation.At this point  the deploy has not initiated  but certain API calls may have succeeded  so we may have incomplete info in the database. Currently there's no convenient way to 'rollback'.We can either ask the user to clean the slate by reinitializing the database and try again (should succeed if the original problem was temporary).We can also build more logic in the UI to retry  check if records already exist  etc...", "id": "1102"}, "1103": {"ground_truth": "0", "bug_report": "Need to be able to reliably recover from the case when the browser is closed during deploy (Step 8 post submission  Step 9) of the wizard\nNeed to be able to reliably recover from the case when the browser is closed during deploy (Step 8 post submission  Step 9) of the wizard.Even after submitting  Step 10  its taking to Step 9 after browser restart. Ideally it should take to monitoring page.", "id": "1103"}, "1106": {"ground_truth": "0", "bug_report": "User-specified custom configs (such as hdfs-site.xml overrides) should be persisted to maintain what the user specified\nUser-specified custom configs (such as hdfs-site.xml overrides) should be persisted to maintain what the user specified", "id": "1106"}, "1113": {"ground_truth": "0", "bug_report": "Install Wizard: Confirm host stuck at Preparing stage\nWith the install wizard went to assign slaves page successfully  returned back to Welcome page  reentered data and then the UI got stuck at Confirm hosts page.", "id": "1113"}, "1115": {"ground_truth": "0", "bug_report": "Host component live status is broken\nWhen datanode is stopped on a host  its status keeps jumping.", "id": "1115"}, "1123": {"ground_truth": "0", "bug_report": "Ambari heatmaps and host information shows infinity for disk space used\nWhen cluster is started after install  it has disk_total of null which results in Infinity.", "id": "1123"}, "1125": {"ground_truth": "0", "bug_report": "Graphs 'degrade' over time\nLeave a page with graphs open for several minutes.The graphs become really coarse.It probably has something to do with the fact that Ganglia only provides 6-minute averages beyond the first 61 minutes (first 61 minutes  Ganglia keeps 15-second samples) and we may not be exactly querying for the last 60 minutes.", "id": "1125"}, "1142": {"ground_truth": "0", "bug_report": "On Notification Popup  clicking 'go to nagios UI' doesn't load nagios UI\n1) Cause notification to occur (for example  stop oozie)2) On dashboard  click notification icon3) On the notification popup  click 'go to nagios web UI'4) nothing happens.This issue is on IE9  Safari and Chrome. Works fine on Firefox.", "id": "1142"}, "1143": {"ground_truth": "0", "bug_report": "tmpfs filesystem being added to the list in the dir used by Ambari\nI saw this on a sles cluster. On EC2 they have a tmpfs mounted and Ambari picked it up.Not sure what the ideal solution is but i feel tmpfs should not be included in the available mount points.Also the tmpfs is being used in various directories that will have to change during the install.Attached screenshots.", "id": "1143"}, "1150": {"ground_truth": "0", "bug_report": "Installer Wizard - Retry feature in Deploy step (Step 9) is broken\nInstaller Wizard - Retry feature in Deploy step (Step 9) is broken", "id": "1150"}, "1151": {"ground_truth": "1", "bug_report": "Reconfigure fails silently; it's not firing any API calls due to a JS error\nReconfigure fails silently; it's not firing any API calls due to a JS error", "id": "1151"}, "1153": {"ground_truth": "0", "bug_report": "Host jams in status 'Preparing' if host name is wrong\nHost jams in status 'Preparing' if host name is wrong", "id": "1153"}, "1154": {"ground_truth": "0", "bug_report": "The check boxes to check/uncheck one of the members in a multi artifact graphs is not very readable. It should be more apparent on which one the user clicked on\nThe check boxes to check/uncheck one of the members in a multi artifact graphs is not very readable. It should be more apparent on which one the user clicked on", "id": "1154"}, "1159": {"ground_truth": "0", "bug_report": "Check the log/run dir locations to make sure its an abs path\nCheck the log/run dir locations to make sure its an abs path", "id": "1159"}, "1164": {"ground_truth": "1", "bug_report": "Disk Info Metrics and memory usage sometimes do not show up for an hour or so.\nDisk Info Metrics and memory usage sometimes do not show up for an hour or so.", "id": "1164"}, "1181": {"ground_truth": "0", "bug_report": "Clean up table header UI for sorting and filter clear 'x' for Hosts table\nClean up table header UI for sorting and filter clear 'x' for Hosts table", "id": "1181"}, "1182": {"ground_truth": "0", "bug_report": "Clean up table header UI for sorting and filter clear 'x' for Jobs table\nClean up table header UI for sorting and filter clear 'x' for Jobs table", "id": "1182"}, "1184": {"ground_truth": "0", "bug_report": "After adding hosts  the host count shown in the Dashboard is incorrect\nAfter adding hosts  the host count shown in the Dashboard is incorrect", "id": "1184"}, "1190": {"ground_truth": "0", "bug_report": "Detailed log view dialogs are not center-aligned\nDetailed log view dialogs are not center-aligned", "id": "1190"}, "1196": {"ground_truth": "0", "bug_report": "Automatically update host-level popup info/logs\nFor host-level popup info/logs that appear during the deploy step (Step 9) of the Install and Add Hosts Wizards  the information shown is not automatically updated based on the latest values in the models when the background poller updates the models.Currently  the user needs to close the popup and then re-open to see the updated info.", "id": "1196"}, "1207": {"ground_truth": "0", "bug_report": "Remove /hdp as the httpd conf for any of the nagios urls - should replace it with ambarinagios or something else.\nRemove /hdp as the httpd conf for any of the nagios urls - should replace it with ambarinagios or something else.", "id": "1207"}, "1210": {"ground_truth": "0", "bug_report": "Allow capacity scheduler to be attached to host role configs for CS configurability in the API's.\nAllow capacity scheduler to be attached to host role configs for CS configurability in the API's.", "id": "1210"}, "1214": {"ground_truth": "0", "bug_report": "In any starts fails  'warn' the host and the overall install\nIf any service start fails  'warn' the host and 'warn' the overall install.Allow the other start tasks to complete.Allow the user to click next.", "id": "1214"}, "1216": {"ground_truth": "0", "bug_report": "Add filters module\nAdd filters module", "id": "1216"}, "1218": {"ground_truth": "0", "bug_report": "Refactor Job Browser User filter\nRefactor Job Browser User filter", "id": "1218"}, "1223": {"ground_truth": "0", "bug_report": "Confirm Hosts page: It looks like hosts disappear if you are on 'Fail' filter and click on 'Retry Failed' button\nConfirm Hosts page: It looks like hosts disappear if you are on 'Fail' filter and click on 'Retry Failed' button", "id": "1223"}, "1224": {"ground_truth": "0", "bug_report": "Drop the 'all' option from Hosts > Component Filter and Jobs > Users Filter\nDrop the 'all' option from Hosts > Component Filter and Jobs > Users Filter", "id": "1224"}, "1229": {"ground_truth": "0", "bug_report": "Dashboard - make disk usage pie chart in HDFS summary easier to understand\nDashboard - make disk usage pie chart in HDFS summary easier to understand", "id": "1229"}, "1231": {"ground_truth": "0", "bug_report": "Replace sudo with su in the ambari setup script since ambari server setup is already run as root.\nReplace sudo with su in the ambari setup script since ambari server setup is already run as root.", "id": "1231"}, "1233": {"ground_truth": "1", "bug_report": "Directory permissions on httpd /var/www/cgi-bin should not be touched by Ambari.\nDirectory permissions on httpd /var/www/cgi-bin should not be touched by Ambari.", "id": "1233"}, "1235": {"ground_truth": "0", "bug_report": "Host health indicator should have a tooltip showing details\nHost health indicator should have a tooltip showing details", "id": "1235"}, "1244": {"ground_truth": "0", "bug_report": "Install Options - line up the Target Hosts section with the rest of the page\nInstall Options - line up the Target Hosts section with the rest of the page", "id": "1244"}, "1253": {"ground_truth": "0", "bug_report": "Use ember-precompiler-brunch npm plugin\nUse the Ember Handlebars precompiler plugin (ember-precompiler-brunch) that's in the NPM registry so that we can specify a specific plugin version to use in package.json  rather than the current plugin that Ambari Web uses (which is not in the NPM registry - it is retrieved from a git repo and Ambari Web can break if the plugin gets updated).", "id": "1253"}, "1259": {"ground_truth": "0", "bug_report": "Fix the host roles live status not go back to INSTALLED if it was in START_FAILED state.\nFix the host roles live status not go back to INSTALLED if it was in START_FAILED state.", "id": "1259"}, "1260": {"ground_truth": "0", "bug_report": "Remove hard coded JMX port mappings\nThe JMXPropertyProvider contains a map of component names to ports ... JMX_PORTS.put('NAMENODE'  '50070'); JMX_PORTS.put('DATANODE'  '50075'); JMX_PORTS.put('JOBTRACKER'  '50030'); JMX_PORTS.put('TASKTRACKER'  '50060'); JMX_PORTS.put('HBASE_MASTER'  '60010');These ports can change in configuration. Need to create the mapping dynamically.This is required for secure HDP cluster to work.", "id": "1260"}, "1264": {"ground_truth": "0", "bug_report": "Service graphs refresh with spinners\nService graphs are refreshing with spinners  rather than simply shifting to the left.See the attached movie clip.", "id": "1264"}, "1265": {"ground_truth": "0", "bug_report": "Job Browser - Filter by Input  output and duration\n&lt;expression&gt; ::= [&lt;operator&gt;] [&lt;whitespace&gt;]* &lt;value&gt; [&lt;duration-unit&gt; | &lt;io-unit&gt;]&lt;operator&gt; ::= '&gt;' | '&lt;' | '='&lt;whitespace&gt; ::= ' '&lt;value&gt; :== int | float&lt;duration-unit&gt; ::= 's' | 'm' | 'h'&lt;io-unit&gt; ::= 'k' | 'kb' | 'm' | 'mb' | 'g' | 'gb'If &lt;operator&gt; is ommitted  '=' is assumed.If &lt;duration-unit&gt; is omitted  's' is assumed.If &lt;io-unit&gt; is omitted  'k' is assumed.&lt;duration-unit&gt; and &lt;io-unit&gt; are case-insensitive.", "id": "1265"}, "1266": {"ground_truth": "0", "bug_report": "Agent checks packages as part of host check but doesn't tell which ones are needed or conflicting\nAgent checks packages as part of host check but doesn't tell which ones are needed or conflicting", "id": "1266"}, "1267": {"ground_truth": "0", "bug_report": "Store example Hive Queries somewhere in Ambari that's easily accessible for demo/test purposes\nStore example Hive Queries somewhere in Ambari that's easily accessible for demo/test purposes", "id": "1267"}, "1271": {"ground_truth": "0", "bug_report": "On Confirm Hosts page  add a link to show the Host Checks popup in the success message\nOn Confirm Hosts page  add a link to show the Host Checks popup in the success message", "id": "1271"}, "1273": {"ground_truth": "1", "bug_report": "Edit User: No error message is shown when the user does not enter the correct 'old password'\nNo error message is shown when the user does not enter the correct 'old password' when editing an user.", "id": "1273"}, "1274": {"ground_truth": "0", "bug_report": "Shrink top nav height\nNavbar's height should be shrunk to ~40px.", "id": "1274"}, "1275": {"ground_truth": "0", "bug_report": "Incorrect displaying 'Background operations' window after changing state of component\nSteps:1. Change state (start/stop operations are preferable) for lot of components so that the number of background operations was approximately 20-30;2. Change size of browser (800-900 px);3. Go to down of page;4. Change state for next component;5. Confirm changing;Result:'Background operations' window was opened  but it 'OK' button is not visible.Expected result:'Background operations' window contains scroll bar for all background operations. 'OK' button is available for using.", "id": "1275"}, "1277": {"ground_truth": "0", "bug_report": "Failing build due to url moved on Suse.\nFailing build due to url moved on Suse. Looks like centos5 and 6 handle redirection all fine but doesnt look like the maven plugin handles that on SUSE.", "id": "1277"}, "1279": {"ground_truth": "0", "bug_report": "Make sure that Ambari Web renders all elements correctly when the browser width is 1024px or narrower\nCurrent behavior is to dynamically layout elements as the browser viewport size changes. This causes elements to overlap  go outside the bounding box  positioned in a weird way  etc.Let's set minimum width to be 1024px (or maybe 980px to account for the scrollbar) and make sure that all elements are laid out correctly.Bigger than 1024px should still utilize more screen space as it does today.", "id": "1279"}, "1299": {"ground_truth": "0", "bug_report": "Bootstrap can hang indefinitely\nI was bootstrapping 4 nodes. One of them got stuck in 'Installing' phase and won't time out even after ~30 minutes.It seems like starting of ambari-agent was hanging.", "id": "1299"}, "1300": {"ground_truth": "0", "bug_report": "Service status / host component status can get stuck in the green blinking state if stop fails - no further operation can be performed\nThis happens when a service/host component is running  but stop fails. This leaves the desired_state in the INSTALLED (i.e.  STOPPED) state  while the live state is STARTED.Currently  UI assumes when desired_state==INSTALLED and state==STARTED  it is STARTING. This was a trick to get around the problem of backend live state update lagging and to make UI more responsive.", "id": "1300"}, "1305": {"ground_truth": "0", "bug_report": "Make sure that Ambari Web renders all elements correctly when the browser width is 1024px or narrower (refactor)\nCurrent behavior is to dynamically layout elements as the browser viewport size changes. This causes elements to overlap  go outside the bounding box  positioned in a weird way  etc.Let's set minimum width to be 1024px (or maybe 980px to account for the scrollbar) and make sure that all elements are laid out correctly.Bigger than 1024px should still utilize more screen space as it does today.", "id": "1305"}, "1308": {"ground_truth": "0", "bug_report": "Properly display Apps page aggregate summary and data table when there are no data to be shown\nProperly display Apps page aggregate summary and data table when there are no data to be shown", "id": "1308"}, "1309": {"ground_truth": "0", "bug_report": "Remove all text from Apps views  controllers  templates to messages.js\nRemove all text from Apps views  controllers  templates to messages.js", "id": "1309"}, "1311": {"ground_truth": "0", "bug_report": "Host health indicator should have a tooltip showing few details (refactoring)\nHost health indicator should have a tooltip showing few details (refactoring)", "id": "1311"}, "1321": {"ground_truth": "0", "bug_report": "Switching out of Jobs page does not launch popup anymore\nOn the jobs page click on Job-X and see the popup with all the job information. Now switch to Services page and come back to the jobs page. Now clicking on Job-X will not launch popup.", "id": "1321"}, "1330": {"ground_truth": "0", "bug_report": "Cluster missing hosts after successful install and restart.\nCluster missing hosts after successful install and restart. This bug got introduced due to my patch in AMBARI-1301.", "id": "1330"}, "1333": {"ground_truth": "1", "bug_report": "Add username validation for Ambari local users\nAdd username validation for Ambari local users", "id": "1333"}, "1335": {"ground_truth": "0", "bug_report": "Show validation error when the user specifies target hosts that are already part of the cluster\nShow validation error when the user specifies target hosts that are already part of the cluster", "id": "1335"}, "1337": {"ground_truth": "0", "bug_report": "Refactor Job Browser filter\nRefactor Job Browser filter", "id": "1337"}, "1340": {"ground_truth": "0", "bug_report": "Enhance Install/Start/Test progress display\nWhen we are showing progress for cluster install  it takes a while for any of the host-level or overall progress to show any progress completion percentage (they can get stuck at 0% for several minutes). This is because host-level completion is not displayed until the first install task for the task is complete. Instead  we should advance percentage complete when the tasks changes the status (from PENDING-&gt;QUEUED-&gt;IN_PROGRESS) to better reflect the install progress to the end user.", "id": "1340"}, "1343": {"ground_truth": "0", "bug_report": "Service Check fails after secure install due to wrong kinit path\nFor manually installed kdc  the kinit path in the puppet script is /usr/kerberos/bin/kinitActual location:root@ip-10-38-13-250 data]# whereis kinitkinit: /usr/bin/kiniterr: /Stage&#91;2&#93;/Hdp-hbase::Hbase::Service_check/Hdp-hadoop::Exec-hadoop&#91;hbase::service_check::test&#93;/Hdp::Exec&#91;/usr/kerberos/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs; hadoop --config /etc/hadoop/conf fs -test -e /apps/hbase/data/usertable&#93;/Exec&#91;/usr/kerberos/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs; hadoop --config /etc/hadoop/conf fs -test -e /apps/hbase/data/usertable&#93;: Failed to call refresh: Could not find command '/usr/kerberos/bin/kinit'", "id": "1343"}, "1348": {"ground_truth": "0", "bug_report": "Externalize strings to messages.js\nExternalize string resources to messages.js.", "id": "1348"}, "1351": {"ground_truth": "0", "bug_report": "Provide consistent ordering of hosts in heatmap\nProvide consistent ordering of hosts in heatmap", "id": "1351"}, "1354": {"ground_truth": "0", "bug_report": "'No alerts' badge on the Host Detail page should be green  not red\n'No alerts' badge on the Host Detail page should be green  not red", "id": "1354"}, "1359": {"ground_truth": "0", "bug_report": "App Browser rows colours should alternate from dark grey to light grey and back\nApp Browser rows colours should alternate from dark grey to light grey and back", "id": "1359"}, "1360": {"ground_truth": "0", "bug_report": "Mouse cursor hover behavior is strange on Job Browser\nWhen hovering the mouse cursor around Show All  Filtered  the mouse cursor changes to the 'hand' icon as expected. To the right  the cursor turns into a hand even when hovering over areas where there's no link. Hovering over 'Clear filters'  the cursor does not turn into the 'hand'.", "id": "1360"}, "1373": {"ground_truth": "1", "bug_report": "Since there is the ability to log in to Ambari Web as different users the current user should be indicated\nSince there is the ability to log in to Ambari Web as different users the current user should be indicated", "id": "1373"}, "1376": {"ground_truth": "0", "bug_report": "Wrong calculation of duration filter on apps page\nWrong calculation for the duration filter if we enter just number  without h  m or s to specify the unit. By default we should take seconds as the default unit.", "id": "1376"}, "1432": {"ground_truth": "0", "bug_report": "Ambari Agent registration hangs due to Acceptor bug in Jetty for not reading through accepted connections.\nAmbari Agent registration hangs due to Acceptor bug in Jetty for not reading through accepted connections.", "id": "1432"}, "1441": {"ground_truth": "1", "bug_report": "Validation for username used in service configs is broken\nValidation for username used in service configs is broken", "id": "1441"}, "1449": {"ground_truth": "0", "bug_report": "Failure popup shown for reconfiguring HDFS when MapReduce is not installed\nFailure popup shown for reconfiguring HDFS when MapReduce is not installed", "id": "1449"}, "1450": {"ground_truth": "0", "bug_report": "Remove hard-coded stack version\nRemove hard-coded stack version", "id": "1450"}, "1456": {"ground_truth": "0", "bug_report": "Cannot proceed after bootstrapping in some cases due to a run-time error while running host checks\nCannot proceed after bootstrapping in some cases due to a run-time error while running host checks", "id": "1456"}, "1460": {"ground_truth": "0", "bug_report": "Optimize query call for retrieving host information\nEvery 15 seconds  Ambari Web makes a call to retrieve information about all hosts in the cluster. With 400+ nodes  this call retrieves about 10MB of info  since the API returns last_agent_env  which is host check results used during bootstrap and makes the payload huge.By optimizing the query string for this API call  we can cut down on the payload by more than 90%.", "id": "1460"}, "1461": {"ground_truth": "0", "bug_report": "Optimize query for getting service and host component status back from the server\nThe API response for getting service/host component status back from the server is unnecessarily big due to nagios_alerts being included as part of the response. This optimization can cut the payload in half or more  and also eases load on the server since it does not have to get Nagios alerts as part of fulfilling the API call.", "id": "1461"}, "1465": {"ground_truth": "0", "bug_report": "Minimize Read and Write locks for createHosts\nInvocation count and exec time for ClustersImpl.mapHostToCluser very highorg.apache.ambari.server.state.cluster.ClustersImpl.mapHostToCluster(String  String) Time(ms): 252 925 Avg Time(ms): 5 269 Own Time(ms): 211Invocation Count: 48Each time host is mapped to cluster we refresh the entity manager. This results in the createHosts call taking excess of 10 minutes", "id": "1465"}, "1466": {"ground_truth": "0", "bug_report": "Optimize ganglia rrd script to be able to respond within reasonable time to queries made by the UI.\nOptimize ganglia rrd script to be able to respond within reasonable time to queries made by the UI.", "id": "1466"}, "1473": {"ground_truth": "0", "bug_report": "Further optimization of querying host information from the server\nFurther work on optimizing query for getting host information on top of BUG-1460.", "id": "1473"}, "1486": {"ground_truth": "0", "bug_report": "Fix TestHostName to take care of issues when gethostname and getfqdn do not match.\nFix TestHostName to take care of issues when gethostname and getfqdn do not match.", "id": "1486"}, "1487": {"ground_truth": "0", "bug_report": "Fix alerts at host level if MapReduce is not selected not to alert for tasktrackers not running.\nFix alerts at host level if MapReduce is not selected not to alert for tasktrackers not running.", "id": "1487"}, "1488": {"ground_truth": "0", "bug_report": "Nagios script causes unwanted  Datanode logs.\nNagios script causes unwanted Datanode logs.", "id": "1488"}, "1489": {"ground_truth": "0", "bug_report": "Add hadoop-lzo to be one of the rpms to check for before installation.\nAdd hadoop-lzo to be one of the rpms to check for before installation.", "id": "1489"}, "1496": {"ground_truth": "0", "bug_report": "Make all service properties reconfigurable.\nMake all service properties reconfigurable.", "id": "1496"}, "1497": {"ground_truth": "0", "bug_report": "Fix start up option for ambari-server where there is a missing space.\nFix start up option for ambari-server where there is a missing space.", "id": "1497"}, "1499": {"ground_truth": "0", "bug_report": "Add hosts is broken\nThis is due to performance enhancements for querying hosts.Hosts/disk_info for the hosts being added is expected in the Add Hosts wizard  but it is now missing.", "id": "1499"}, "1505": {"ground_truth": "0", "bug_report": "Hosts page: add filtering by host status\nShow the host status filter at the top of the Hosts table  showing the status and the number of hosts in that status.", "id": "1505"}, "1519": {"ground_truth": "0", "bug_report": "Ambari Web goes back and forth between frozen and usable state peridocially on a large cluster\nThe background polling to update the live status of services and host components runs every 6 seconds. When this happens  the whole UI freezes for several seconds periodically.", "id": "1519"}, "1520": {"ground_truth": "0", "bug_report": "Alerts take around 20-30 seconds to show up everytime you refresh the dashboard.\nAlerts take around 20-30 seconds to show up everytime you refresh the dashboard.", "id": "1520"}, "1536": {"ground_truth": "0", "bug_report": "Hosts page layout fixes\n1.Use Bootstrap-style tooltip for the hover tooltip on the host status dots as well as the disk usage bars. The current tooltip does not show up unless you rest your mouse cursor for more than a second - users will simply assume there's no tooltip hover when quickly moving the mouse cursor over them.2.We can kill some space between the status dots and the hostnames in the table as well.3.Also  there is too much margin at the top compared to the rest of the pages.", "id": "1536"}, "1537": {"ground_truth": "0", "bug_report": "Constrain the width of all wizard popups\nCurrently  the popup for any wizard resizes to fill up the width of the browser width with some horizontal margin. Vertically  it resizes to fit the height of the content; in some cases  it looks really silly on a wide screen.We should specify the max width of the wizard popup. Ambari Web supports two width configurations (wide and narrow)  so the wizard popup should look good in these two configurations.", "id": "1537"}, "1559": {"ground_truth": "0", "bug_report": "Jobs failed count always returns 0 in the jobtracker API metrics\nSee the attachments  but after running both successful and failed MapReduce jobs  the jobs submitted count includes all jobs  the jobs successful appears correct  but the jobs failed count is still 0.", "id": "1559"}, "1580": {"ground_truth": "0", "bug_report": "Stack Upgrade Wizard - resume upon page refresh / login\nStack Upgrade Wizard - resume upon page refresh / login", "id": "1580"}, "1581": {"ground_truth": "0", "bug_report": "Host progress popup - generic component for showing progress on async operations\nThis is for the popup that shows up upon clicking already-performed or currently-in-progress async operations.We should take what we already have for host-level popup in Step 9 as a base and make it reusable  with a new higher level showing all hosts.", "id": "1581"}, "1582": {"ground_truth": "0", "bug_report": "Cannot start hadoop services after hdfs re-configuration and amabri server restart.\nCannot start hadoop services after several restarts since the agents.", "id": "1582"}, "1597": {"ground_truth": "1", "bug_report": "Templeton smoke test fails for secure cluster\nTempleton start fails due to multiple errors.[0;36mnotice: /Stage&#91;2&#93;/Hdp-templeton::Copy-hdfs-directories/Hdp-hadoop::Hdfs::Copyfromlocal&#91;/usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar&#93;/Hdp-hadoop::Exec-hadoop&#91;fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/Hdp::Exec&#91;/usr/bin/kinit -kt /etc/security/keytabs/hcat.headless.keytab hcat; hadoop --config /etc/hadoop/conf fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/Exec&#91;/usr/bin/kinit -kt /etc/security/keytabs/hcat.headless.keytab hcat; hadoop --config /etc/hadoop/conf fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/returns: kinit: Client not found in Kerberos database while getting initial credentials^[[0m", "id": "1597"}, "1621": {"ground_truth": "0", "bug_report": "Config/Reconfig UI should not allow certain configs to have host-level overrides\nConfig/Reconfig UI should not allow certain configs to have host-level overrides", "id": "1621"}, "1631": {"ground_truth": "0", "bug_report": "Security Wizard - integrate host progress popup\nSecurity Wizard - integrate host progress popup", "id": "1631"}, "1641": {"ground_truth": "1", "bug_report": "Some map and reduce task metrics are missing for the tasktrackers in the API\nWith Ambari 1.2.2  I cant get the metrics.mapred object to show up for the tasktracker component. Our code is hitting the URL: http://sdll4474.labs.teradata.com:8080/api/v1/clusters/sdll4474.labs.teradata.com/services/MAPREDUCE/components/TASKTRACKER?fields=host_components/*. Heres one of the objects in the host_components array. Note that the data in metrics.mapred.tasktracker is providing some data Ive never seen before. In previous versions we say properties such as reduces_running  reduceTaskSlots  maps_running  etc. in this object. { 'href' : 'http://aster39h1.td.teradata.com:8080/api/v1/clusters/aster39h1.td.teradata.com/hosts/byn001-17/host_components/TASKTRACKER'  'HostRoles' : { 'cluster_name' : 'aster39h1.td.teradata.com'  'desired_state' : 'STARTED'  'state' : 'STARTED'  'component_name' : 'TASKTRACKER'  'service_name' : 'MAPREDUCE'  'host_name' : 'byn001-17' }  'metrics' : { 'boottime' : 1.360089758E9  'process' : { 'proc_total' : 845.211111111  'proc_run' : 0.0 }  'rpc' : { 'rpcAuthorizationSuccesses' : 9  'SentBytes' : 6842  'rpcAuthorizationFailures' : 0  'ReceivedBytes' : 26187  'NumOpenConnections' : 0  'callQueueLen' : 0  'RpcQueueTime_num_ops' : 59  'rpcAuthenticationSuccesses' : 0  'RpcProcessingTime_num_ops' : 59  'rpcAuthenticationFailures' : 0  'RpcProcessingTime_avg_time' : 0.6666666666666666  'RpcQueueTime_avg_time' : 0.0 }  'mapred' : { 'shuffleOutput' : { 'shuffle_success_outputs' : 1  'shuffle_handler_busy_percent' : 0.0  'shuffle_output_bytes' : 1400  'shuffle_failed_outputs' : 0  'shuffle_exceptions_caught' : 0 }  'tasktracker' : { 'ConfigVersion' : 'default'  'HttpPort' : 50060  'TasksInfoJson' : '{/'running/':0 /'failed/':0 /'commit_pending/':0}'  'JobTrackerUrl' : 'aster39h1.td.teradata.com:50300'  'Healthy' : true  'Version' : '1.1.2.22  r'  'Hostname' : 'byn001-17'  'RpcPort' : 48526 } }  'ugi' : { 'loginFailure_num_ops' : 0  'loginSuccess_num_ops' : 0  'loginSuccess_avg_time' : 0.0  'loginFailure_avg_time' : 0.0 }  'disk' : { 'disk_total' : 36841.767  'disk_free' : 36776.9775333  'part_max_used' : 70.7 }  'cpu' : { 'cpu_speed' : 1999.0  'cpu_wio' : 0.0  'cpu_num' : 24.0  'cpu_idle' : 99.8886111111  'cpu_nice' : 0.0  'cpu_aidle' : 0.0  'cpu_system' : 0.1  'cpu_user' : 0.0227777777778 }  'rpcdetailed' : { 'getTask_avg_time' : 1.0  'ping_avg_time' : 0.0  'done_avg_time' : 1.0  'getProtocolVersion_avg_time' : 0.0  'getMapCompletionEvents_avg_time' : 0.0  'done_num_ops' : 9  'getMapCompletionEvents_num_ops' : 6  'canCommit_num_ops' : 6  'ping_num_ops' : 2  'commitPending_avg_time' : 1.0  'statusUpdate_num_ops' : 15  'statusUpdate_avg_time' : 1.0  'getTask_num_ops' : 9  'getProtocolVersion_num_ops' : 9  'commitPending_num_ops' : 3  'canCommit_avg_time' : 0.0 }  'load' : { 'load_fifteen' : 0.0  'load_one' : 0.0  'load_five' : 0.0 }  'jvm' : { 'memHeapCommittedM' : 100.4375  'NonHeapMemoryUsed' : 25214472  'logFatal' : 0  'threadsWaiting' : 17  'gcCount' : 122400  'threadsBlocked' : 0  'HeapMemoryUsed' : 77617416  'logWarn' : 0  'logError' : 0  'HeapMemoryMax' : 954466304  'memNonHeapCommittedM' : 26.125  'memNonHeapUsedM' : 24.046394  'gcTimeMillis' : 81352  'NonHeapMemoryMax' : 136314880  'logInfo' : 3  'memHeapUsedM' : 73.81818  'threadsNew' : 0  'threadsTerminated' : 0  'maxMemoryM' : 758.4375  'threadsTimedWaiting' : 10  'threadsRunnable' : 6 }  'network' : { 'pkts_out' : 111.684472222  'bytes_in' : 1428.83666667  'bytes_out' : 23201.8668056  'pkts_in' : 13.9853333333 }  'memory' : { 'mem_total' : 1.31854096E8  'swap_free' : 6291448.0  'mem_buffers' : 574794.711111  'mem_shared' : 0.0  'swap_total' : 6291448.0  'mem_cached' : 6061952.85556  'mem_free' : 1.23072573378E8 } }  'component' : [ { 'href' : 'http://aster39h1.td.teradata.com:8080/api/v1/clusters/aster39h1.td.teradata.com/services/MAPREDUCE/components/TASKTRACKER'  'ServiceComponentInfo' : { 'cluster_name' : 'aster39h1.td.teradata.com'  'component_name' : 'TASKTRACKER'  'service_name' : 'MAPREDUCE' } } ] }", "id": "1641"}, "1645": {"ground_truth": "0", "bug_report": "Undo should not be allowed on component hosts\nUndo should not be allowed on component hosts", "id": "1645"}, "1666": {"ground_truth": "0", "bug_report": "Oozie properties for principal and keytab not read from oozie-site\nOozie principal is create by the UI as oozie/${hostname}@realm.comPuppet script has a bug that does not read this property and uses default 'oozie' as the principal", "id": "1666"}, "1667": {"ground_truth": "0", "bug_report": "Starting all services fails on secure cluster (excluding HBase and ZooKeeper)\nHDFS service check failure leads to this.After the failure of the 'HDFS service check' task  stage fails. But HDFS comes up.The Ambari server hostname for secure cluster: ec2-54-234-164-5.compute-1.amazonaws.com", "id": "1667"}, "1702": {"ground_truth": "0", "bug_report": "Ambari/GSInstallers need to set the value of mapred.jobtracker.completeuserjobs.maximum\nmapred.jobtracker.completeuserjobs.maximum is currently set to 0. This causes issues with failed(/successful) jobs from pig and other job submitters because the references to the failed job is cleared up asap in jobtracker preventing one from accessing the failure reason. This value should be bumped up to about 100 according to the MapReduce team.", "id": "1702"}, "1724": {"ground_truth": "0", "bug_report": "Agent has it hard-coded that HDP repo file can only be downloaded once\nUpgrade requires agents to download the repo file anytime.", "id": "1724"}, "1726": {"ground_truth": "0", "bug_report": "It seems upgrades available at the FE is hard-coded to 1.3.0\nIn order to test upgrade  I modified the available stack definitions to allow upgrade from 1.2.0 to 1.2.2 and removed upgrade to 1.3.0. However  the FE still says '(Upgrade available: HDP-1.3.0)'. However  http://server:8080/api/v1/stacks2/HDP/versions/1.3.0/ has min_upgrade_version as null.{ 'href' : 'http://server:8080/api/v1/stacks2/HDP/versions/1.3.0/'  'Versions' : { 'stack_version' : '1.3.0'  'stack_name' : 'HDP'  'min_upgrade_version' : null } ...", "id": "1726"}, "1739": {"ground_truth": "0", "bug_report": "HBase and Zk failed to start on secure install\nError during starting hbase and zookeeper during secure install.Incorrectly parsed Files:hbase-env.shzookeeper-env.sh", "id": "1739"}, "1752": {"ground_truth": "0", "bug_report": "Backend support for MySQL and Oracle for Oozie and Hive\nAdd ability to use Oracle DB for Hive and Oozie in Ambari", "id": "1752"}, "1757": {"ground_truth": "0", "bug_report": "Add support for Stack 1.2.2 to Ambari\nAdd support for Stack 1.2.2 to Ambari.", "id": "1757"}, "1764": {"ground_truth": "0", "bug_report": "Unable to get all tasks from more than one request_id by one request\nWhen trying to get tasks from more than one request_id returns tasks only for one.request 'api/v1/clusters/mycluster/requests?Requests/id=1|Requests/id=2'returns:{'href' : 'http://ec2-23-20-223-127.compute-1.amazonaws.com:8080/api/v1/clusters/mycluster/requests?Requests/id=1|Requests/id=2' 'items' : [{'href' : 'http://ec2-23-20-223-127.compute-1.amazonaws.com:8080/api/v1/clusters/mycluster/requests/2' 'Requests' :{ 'id' : 2  'cluster_name' : 'mycluster' }}]}", "id": "1764"}, "1770": {"ground_truth": "0", "bug_report": "Hue installation fails due to manifest errors\nHue installation fails due to following errors:1. Change in the name of rpm bundle2. Empty values in the hue.ini sections", "id": "1770"}, "1775": {"ground_truth": "0", "bug_report": "Security wizard - Javascript error is thrown when zooKeeper is included as a secure service.\nSecurity wizard - Javascript error is thrown when zooKeeper is included as a secure service.", "id": "1775"}, "1789": {"ground_truth": "0", "bug_report": "Stopping and then Starting all services doesn't start NameNode\nSecurity wizard stops all services  applies configuration and then starts all services.Sometimes it has been noticed that the action to stop all service completes successfully but the action to start all services never sends the task to start NameNode.", "id": "1789"}, "1791": {"ground_truth": "0", "bug_report": "Can not specify request context for smoke test request\nRegarding BUG-3509 when we send request to server likeapi/v1/clusters/cl1/services/HDFS/actions/HDFS_SERVICE_CHECKRequest Method:POST Form Data:{'RequestInfo':{'context':'Smoke Test'}}This request is not setting request_context.", "id": "1791"}, "1816": {"ground_truth": "1", "bug_report": "Security wizard: Add missing secure configs to Hbase service and make 'zookeeper' as default primary name for zookeeper principal.\nSecurity wizard: Add missing secure configs to Hbase service and make 'zookeeper' as default primary name for zookeeper principal.", "id": "1816"}, "1818": {"ground_truth": "0", "bug_report": "HBase master shuts down immediately after start in a secure cluster.\nHBase master shuts down immediately after start in a secure cluster.Wrong settings in the hbase_master_jaas. Need to replace the 'HOST with actual fqdn", "id": "1818"}, "1837": {"ground_truth": "0", "bug_report": "Few core-site properties vanished after seemingly benign reconfiguration.\nUI metadata properties are not being reconfigured and retained on saving services.", "id": "1837"}, "1838": {"ground_truth": "0", "bug_report": "Cluster Management > Services > MapReduce > Config throws JS error and the page comes up blank\nCluster Management > Services > MapReduce > Config throws JS error and the page comes up blank", "id": "1838"}, "1840": {"ground_truth": "0", "bug_report": "For global properties show restart for appropriate services only\nCurrently when one service changes a property which doesnt effect all other services  the rest of the services are marked for restart. This is because the global tags are changed. Using work done in AMBARI-1797  only appropriate services should be marked as needing restart.", "id": "1840"}, "1847": {"ground_truth": "0", "bug_report": "Make single PUT call for multiple host overrides\nCurrently when we save host-overrides configuration  we have to do PUT of the delta on each host.This is problematic if admin provides an exception to 100 hosts. This will require 100 PUT calls which is expensive. There is a bulk update mechanism  but that requires the same content for all 100 hosts. This will not be the case if any hosts have other properties that are overridden.To save on network calls  we have to make one PUT call via the new API provided by AMBARI-1844.", "id": "1847"}, "1849": {"ground_truth": "0", "bug_report": "Cosmetic problems on HBase Dashboard\nCosmetic problems on HBase Dashboard", "id": "1849"}, "1855": {"ground_truth": "0", "bug_report": "Capacity Scheduler: when adding a new queue  populate fields\nWhen creating a new queue  populate all fields with default values except for:Queue NameCapacityMax CapacityUsersGroupsAdmin UsersAdmin Groups", "id": "1855"}, "1859": {"ground_truth": "1", "bug_report": "Cannot load Nagios Alerts due to 400 Bad Request\nGiven the API call: http://dev.hortonworks.com:8080/api/v1/clusters/test403_2/host_components?HostRoles/component_name=NAGIOS_SERVER&amp;fields=HostRoles/nagios_alertsThe feedback is:{ 'status' : 400  'message' : 'The properties [HostRoles/nagios_alerts] specified in the request or predicate are not supported for the resource type HostComponent.' }", "id": "1859"}, "1860": {"ground_truth": "0", "bug_report": "Master broken - Cannot deploy services\nDatanode install fails on multi-node clusters because the following configuration property:$ambari_db_rca_url= 'jdbc:postgresql://localhost/ambarirca'Ensure all of these variables are wired up:'ambari_db_rca_url''ambari_db_rca_driver''ambari_db_rca_username''ambari_db_rca_password'", "id": "1860"}, "1870": {"ground_truth": "0", "bug_report": "ambari-agent RPM claims ownership of /usr/sbin\nThis may impact other versions  only branch-1.2 was changed.The ambari-agent.spec (generated from rpm-maven-plugin) claims ownership of /usr/sbin $ grep sbin target/rpm/ambari-agent/SPECS/ambari-agent.spec | grep attr%attr(755 root root) /usr/sbinThis is a problem because the filesystem RPM owns /usr/sbin.According to rpm-maven-plugin documentation&#91;0&#93;  this is because the only file under /usr/sbin is ambari-agent and'directoryIncludedIf the value is true then the attribute string will be written for the directory if the sources identify all of the files in the directory (that is  no other mapping contributed files to the directory). This is the default behavior.'The 'no other mapping contributed files to the directory' bit is important.The solution is to add directoryInclude=false to the mapping.&#91;0&#93; http://mojo.codehaus.org/rpm-maven-plugin/map-params.html", "id": "1870"}, "1872": {"ground_truth": "0", "bug_report": "Ambari FE is not setting proper value for fs.checkpoint.edits.dir\nAmbari FE is not setting proper value for fs.checkpoint.edits.dir", "id": "1872"}, "1873": {"ground_truth": "0", "bug_report": "HUE pid and log dir labels are flip flopped\nHUE pid and log dir labels are flip flopped", "id": "1873"}, "1876": {"ground_truth": "0", "bug_report": "Capacity Scheduler: implement user/group and admin user/group validation rules\nCapacity Scheduler: implement user/group and admin user/group validation rules", "id": "1876"}, "1877": {"ground_truth": "0", "bug_report": "Reassign Master Wizard  Step 2: prevent proceed next without changing target host\nReassign Master Wizard  Step 2: prevent proceed next without changing target host", "id": "1877"}, "1880": {"ground_truth": "0", "bug_report": "stacks2 API uses 'type' to refer to config tags and no longer exposes 'filename' as a property\nFE needs to be modified to not to use 'filename' in stacks API to get the config tags. The new property is 'type'.", "id": "1880"}, "1881": {"ground_truth": "0", "bug_report": "API to map global properties to services is partially complete.\nAPI to map global properties to services is partially complete. The API gives 'global.xml' to approximately 47 properties. However the 'global' site has many more properties - so I dont know if some are fully missed.", "id": "1881"}, "1891": {"ground_truth": "0", "bug_report": "Impossibility to scroll metric window after browser width changing\nOpen detailed view of any metric diagram on Services page  for example 'Total Space Utilization'. After that change width of browser less than width of detailed view of metric diagram. Right arrow for graph time paging is invisible and we can't to resolve this problem using horizontal scrollbar.", "id": "1891"}, "1896": {"ground_truth": "1", "bug_report": "Disable editing Capacity Scheduler on host configs\nGo to Configs tab of Host.Result: Queues in Capacity Scheduler category are editable.Expected: Queues in Capacity Scheduler category shouldn't be editable.", "id": "1896"}, "1904": {"ground_truth": "0", "bug_report": "Update default stack version to 1.3.0\nUpdate default stack version to 1.3.0", "id": "1904"}, "1912": {"ground_truth": "0", "bug_report": "HBase master doesn't come up after disabling security.\nHBase master doesn't come up after disabling security.", "id": "1912"}, "1915": {"ground_truth": "0", "bug_report": "Client install tasks are shown twice in install progress popup\nClient is re-configured by re-installing all client only hosts. This results in multiple tasks for client install in the UI.API response:{ 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2'  'Requests' : { 'id' : 2  'cluster_name' : 'yusaku' }  'tasks' : [ { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/43'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: /Stage[2]/Hdp-oozie/Configgenerator::Configfile[oozie-site]/File[/etc/oozie/conf/oozie-site.xml]/content: content changed '{md5}eaf59cc452c92e64b559071586150a08' to '{md5}cb15303aab1c384d19c102a6ce650ed2'/nnotice: Finished catalog run in 2.04 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 43  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'OOZIE_CLIENT'  'start_time' : 1365743356709  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/51'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 51  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407043  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/41'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.22 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 41  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HCAT'  'start_time' : 1365743356684  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/54'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 54  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_SERVER'  'start_time' : 1365743407078  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/55'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 55  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'NAGIOS_SERVER'  'start_time' : 1365743407100  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/50'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 50  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'DATANODE'  'start_time' : 1365743407032  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/67'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 67  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_REGIONSERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/47'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $hadoop_heapsize is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hive/Configgenerator::Configfile[hive-site]/File[/etc/hive/conf/hive-site.xml]/content: content changed '{md5}29d1def766d4aadfddbf38db13a2712e' to '{md5}2d26829fd012bf5f195e760fc8eeb7f9'/nnotice: Finished catalog run in 2.84 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 47  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HIVE_CLIENT'  'start_time' : 1365743356758  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/45'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.24 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 45  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'SQOOP'  'start_time' : 1365743356733  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/44'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.13 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 44  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'PIG'  'start_time' : 1365743356721  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/56'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 56  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'NAMENODE'  'start_time' : 1365743407109  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/52'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 52  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407062  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/42'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $hadoop_heapsize is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hive/Configgenerator::Configfile[hive-site]/File[/etc/hive/conf/hive-site.xml]/content: content changed '{md5}27e517fec40f6157f75eb3116d5387bf' to '{md5}54ff14d0a6d9968e900f28853125b294'/nnotice: Finished catalog run in 2.28 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 42  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HIVE_CLIENT'  'start_time' : 1365743356697  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/63'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 63  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_MASTER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/62'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 62  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'TASKTRACKER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/46'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.77 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 46  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HCAT'  'start_time' : 1365743356744  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/58'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 58  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407125  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/60'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 60  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407164  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/69'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 69  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'OOZIE_SERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/64'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 64  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HIVE_METASTORE'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/48'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $service_state at /var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/init.pp:213 is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $tasktracker_port is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_url is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_driver is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_username is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_password is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[core-site]/File[/etc/hadoop/conf/core-site.xml]/content: content changed '{md5}95bdcddd064261ac3a00d8c0a7f79fee' to '{md5}d6f5b9646bf280e915e3b0d42ed622a9'/nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[hdfs-site]/File[/etc/hadoop/conf/hdfs-site.xml]/content: content changed '{md5}5f83b57cbac46a0b7007ed94720a8c3b' to '{md5}e0e38c4dc10fc81b12637e34796ced70'/nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/content: content changed '{md5}42b54b8e096eafa7ba53c8f5b53bda3e' to '{md5}409f4680bc7871db2864afecbcefdb36'/nnotice: Finished catalog run in 3.67 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 48  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'MAPREDUCE_CLIENT'  'start_time' : 1365743356769  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/70'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 70  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'WEBHCAT_SERVER'  'start_time' : -1  'stage_id' : 5 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/61'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 61  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_MASTER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/66'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 66  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'SECONDARY_NAMENODE'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/49'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: /Stage[2]/Hdp-oozie/Configgenerator::Configfile[oozie-site]/File[/etc/oozie/conf/oozie-site.xml]/content: content changed '{md5}001c4940080d2ea315a9720676f1bcad' to '{md5}282f1e354fe7f9da0f6de43425a40d40'/nnotice: Finished catalog run in 2.24 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 49  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'OOZIE_CLIENT'  'start_time' : 1365743356796  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/53'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 53  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407070  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/68'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 68  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HIVE_SERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/65'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 65  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'JOBTRACKER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/57'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 57  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407117  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/59'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 59  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'MYSQL_SERVER'  'start_time' : 1365743407156  'stage_id' : 2 } } ]}", "id": "1915"}, "1917": {"ground_truth": "0", "bug_report": "Ambari Core-Site.xml Missing Property for LZO (enabled) - io.compression.codecs\nAmbari Core-Site.xml Missing Property for LZO (enabled) - io.compression.codecs", "id": "1917"}, "1919": {"ground_truth": "0", "bug_report": "JobTracker History Server failed to come up on 1.3.0 stack and the request for service stall is stalled\nAttempted to install a cluster with 1.3.0 stack.Service install was all green  but JobTracker History Server failed to come up.The request for the service start is stalled  with no tasks in QUEUED or IN_PROGRESS state  but with some tasks in PENDING state.", "id": "1919"}, "1923": {"ground_truth": "0", "bug_report": "Allow for users to customize Nagios user accounts\nAllow for customers to install the users used for Nagios.", "id": "1923"}, "1924": {"ground_truth": "0", "bug_report": "Allow for users to customize Ganglia gmetad + gmond user accounts\nAllow customization of ganglia gmetad and gmond users.For reference: looks like this is available in gsInstaller so the pattern exists to follow for Ambari impl. Defaults to nobody/nobody", "id": "1924"}, "1933": {"ground_truth": "0", "bug_report": "Test failure : testCascadeDeleteStages\nmvn clean install produces the following failure ...testCascadeDeleteStages(org.apache.ambari.server.actionmanager.TestActionManager):Exception [EclipseLink-4002] (Eclipse Persistence Services -2.4.0.v20120608-r11652):org.eclipse.persistence.exceptions.DatabaseException(..)", "id": "1933"}, "1934": {"ground_truth": "1", "bug_report": "Security vulnerability with Ganglia and Nagios\nGanglia Issue : Unspecified vulnerability in Ganglia Web before 3.5.1 allows remote attackers to execute arbitrary PHP code via unknown attack vectors. http://ganglia.info/?p=549 Ganglia Web 3.5.1 Release ?ESecurity Advisory There is a security issue in Ganglia Web going back to at least 3.1.7 which can lead to arbitrary script being executed with web user privileges possibly leading to a machine compromise. Issue has been fixed in the latest version of Ganglia Web which can be downloaded from https://sourceforge.net/projects/ganglia/files/ganglia-web/3.5.1/ Solution: Need to get upgraded rpms with the Ganglia Web version 3.5.7 which has the fix for this vulnerability.Nagios: Multiple stack-based buffer overflows in the get_history function in history.cgi in Nagios Core before 3.4.4  and Icinga 1.6.x before 1.6.2  1.7.x before 1.7.4  and 1.8.x before 1.8.4  might allow remote attackers to execute arbitrary code via a long (1) host_name variable (host parameter) or (2) svc_description variable. http://www.nagios.org/projects/nagioscore/history/core-3x http://lists.grok.org.uk/pipermail/full-disclosure/2012-December/089125.html Vulnerable software and versions - nagios:nagios:3.4.3 and previous versions", "id": "1934"}, "1944": {"ground_truth": "0", "bug_report": "All Service Smoke tests fail when run with service start\nhas_key(): expects the first argument to be a hash  got '' which is of type String at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:38 on node ip-10-38-25-227.ec2.internalsite-pp#12.04.2013 03:16:38import '/var/lib/ambari-agent/puppet/modules/hdp/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hbase/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-zookeeper/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-pig/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-sqoop/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-templeton/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hive/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hcat/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-mysql/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-monitor-webserver/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-repos/manifests/*.pp'$ambari_db_rca_password= ['mapred']$nagios_server_host= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_url= ['jdbc:postgresql://ip-10-38-25-227.ec2.internal/ambarirca']$webhcat_server_host= ['ip-10-38-25-227.ec2.internal']$hbase_rs_hosts= ['ip-10-38-25-227.ec2.internal']$slave_hosts= ['ip-10-38-25-227.ec2.internal']$namenode_host= ['ip-10-38-25-227.ec2.internal']$ganglia_server_host= ['ip-10-38-25-227.ec2.internal']$hbase_master_hosts= ['ip-10-38-25-227.ec2.internal']$hive_mysql_host= ['ip-10-38-25-227.ec2.internal']$oozie_server= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_driver= ['org.postgresql.Driver']$zookeeper_hosts= ['ip-10-38-25-227.ec2.internal']$jtnode_host= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_username= ['mapred']$hive_server_host= ['ip-10-38-25-227.ec2.internal']node /default/ { stage{1 :} -&gt; stage{2 :}class {'hdp': stage =&gt; 1}class {'hdp-zookeeper::quorum::service_check': stage =&gt; 2}}", "id": "1944"}, "1947": {"ground_truth": "0", "bug_report": "Oozie Smoke test fails with errors on the start services/install page.\nOzzie smoke tests fail", "id": "1947"}, "1948": {"ground_truth": "0", "bug_report": "System logs are not present on tasktracker\nRun a mapreduce job.Find the attempt id and look for system logs on tasktracker.http://ec2-54-224-138-78.compute-1.amazonaws.com:50060/tasklog?attemptid=attempt_201304121816_0003_m_000000_0Actual result:The syslogs are not present here.Only stdout and stderr logs are present.", "id": "1948"}, "1952": {"ground_truth": "0", "bug_report": "hadoop dependency version for ambari-log4j is hardcoded  making it regular expression based to pick latest from the repository.\nAmbari-log4j has hardcoded hadoop-core and hadoop-tools dependency. Make it version as regular expression to pick from the range from 1.0 &lt;= x &lt; 2.0.Also  updating the repository url.", "id": "1952"}, "1956": {"ground_truth": "0", "bug_report": "Wrong install status shown in Add Service Wizard\nUpon master component install failure  the host status becomes 'warning' instead of 'failed' for Add Service Wizard.", "id": "1956"}, "1957": {"ground_truth": "0", "bug_report": "Hosts table: whether the alert filter is in effect or not is not clear\nCurrently  when the red badge inside the Hosts tab is clicked  it shows hosts that have at least one alert and no other hosts are displayed.The fact that the alert filter is in effect is not clear to the user and causes confusion.", "id": "1957"}, "1966": {"ground_truth": "0", "bug_report": "Client install tasks are shown twice in progress popup during start phase of install wizard (update API call to include params/reconfigure_client)\nClient install tasks are shown twice in progress popup during start phase of install wizard (update API call to include params/reconfigure_client)", "id": "1966"}, "1978": {"ground_truth": "0", "bug_report": "Deploying HDP-1.3.0 results in several alerts - is it related to hard-coded port\nTaskTracker  RegionServer  HBase master process down because check_tcp failure.Looks like the hadoop-services.cfg.erb has:check_command check_tcp!&lt;%=scope.function_hdp_template_var('jtnode_port')%&gt;!-w 1 -c 1and looks like the ports are not getting replaced and end up being empty.", "id": "1978"}, "1980": {"ground_truth": "0", "bug_report": "When nagios is unavailable  return null instead of throwing an Exception\nNAGIOS_SERVER alerts are retrieved from the nagios server when requesting the host_component. There is a SystemException thrown in the case of an IOException  which propagates as a 500 error for the entire request.In this case  set the nagios_alerts element to null instead of the 500 error.", "id": "1980"}, "1988": {"ground_truth": "0", "bug_report": "Hostname pattern expression is broken\ndev[01-03].domain.com expanded to:dev1.domain.comdev2.domain.comdev3.domain.comShould be:dev01.domain.comdev02.domain.comdev03.domain.com", "id": "1988"}, "1997": {"ground_truth": "0", "bug_report": "Filtered hosts get out of sync with the filter selection\n1. Browse to Hosts2. Click one of the filters3. Browse to Services and back to Hosts4. The filter links show All as the current filter but the filter didn't reset and still shows a sub-set of hosts", "id": "1997"}, "1998": {"ground_truth": "0", "bug_report": "Action buttons on host details page not formatted properly on Firefox\nAction buttons on host details page not formatted properly on Firefox", "id": "1998"}, "1999": {"ground_truth": "0", "bug_report": "Clicking on Cancel on the Service Config page should not reload the entire app\nWhen Cancel is clicked on the Service Config page  simply reload the config (not the entire app).", "id": "1999"}, "2001": {"ground_truth": "0", "bug_report": "Filtering on Jobs table does not work under certain situations\nFiltering on Jobs table does not work under certain situations", "id": "2001"}, "2003": {"ground_truth": "0", "bug_report": "Hosts tab: clicking on red badge should not toggle 'Alerts' filter\nClicking on the red badge in the Hosts tab should not toggle the 'Alerts' filter on the Hosts page (clicking anywhere in Hosts tab should go to Hosts page with 'All' selected).", "id": "2003"}, "2008": {"ground_truth": "0", "bug_report": "Using mixed OS overwrites ambari.repo during install\nPerformed install on mixed OS environment with 8 hosts.Ambari Server = RHEL6Three Hosts = RHEL6Four Hosts = RHEL5Performed manual ambari-agent bootstrap of the Four RHEL5 hosts. I was able to successfully register all hosts. When install started  the four RHEL5 hosts failed on installing their first component. Looking at the servers  looks like the right HDP.repo and HDP-epel.repo files are put in place.But looks like the ambari.repo file had been overwritten at some point during the install process  and now is point to the RHEL6 repos  causing failures.", "id": "2008"}, "2013": {"ground_truth": "0", "bug_report": "Cannot delete cluster with components in UNKNOWN state\nWhen components are marked in an UNKNOWN state  it is not possible to delete the cluster - this should be possible.", "id": "2013"}, "2019": {"ground_truth": "0", "bug_report": "Cannot decommission data node (ensure recommission also works)\nstderr: $configuration&#91;hdfs-site&#93; is not an hash or array when accessing it with dfs.hosts.exclude at /var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/hdfs/decommission.pp:24 on node ip-10-82-213-66.ec2.internal stdout:None", "id": "2019"}, "2024": {"ground_truth": "0", "bug_report": "Ambari Server becomes unresponsive after crashing on http reads on jersey.\nThe api's are being handled by a queuedthreadpool. The queuedthread pool size is 25.Somehow the http connections are being torn down from the UI side but the server still is hanging onto that socket and reading (most likely UI will also need to close http connections if its not using them - which might be an issue as well but doesnt have to addressed as urgent). The server has a read timeout of 0 which means it will just hang on to that socket for read. This causes all the threads to block at one time or the other. Simple solution is add read timeouts to all the SelectChannelConnector and SslSelectChannelConnector we use.Exception trace: SEVERE: The exception contained within MappableContainerException could not be mapped to a response  re-throwing to the HTTP containerorg.eclipse.jetty.io.EofException: early EOF at org.eclipse.jetty.server.HttpInput.read(HttpInput.java:65) at org.codehaus.jackson.impl.ByteSourceBootstrapper.ensureLoaded(ByteSourceBootstrapper.java:507) at org.codehaus.jackson.impl.ByteSourceBootstrapper.detectEncoding(ByteSourceBootstrapper.java:129) at org.codehaus.jackson.impl.ByteSourceBootstrapper.constructParser(ByteSourceBootstrapper.java:224) at org.codehaus.jackson.JsonFactory._createJsonParser(JsonFactory.java:785) at org.codehaus.jackson.JsonFactory.createJsonParser(JsonFactory.java:561) at org.codehaus.jackson.jaxrs.JacksonJsonProvider.readFrom(JacksonJsonProvider.java:414) at com.sun.jersey.json.impl.provider.entity.JacksonProviderProxy.readFrom(JacksonProviderProxy.java:139) at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474)Notice the API's is being called all the time - meaning they probalby had a browser up and running for a long time.There might be a possibilility that the browser might have some issues after running for a long time. Something to keep in mind when this happens again. Easy way to check that is to call Ambari server API's and also bring up a new browser window (new instance) and try hitting the browser UI.", "id": "2024"}, "2027": {"ground_truth": "0", "bug_report": "Add validation checks for Add Property on custom site configs\nAdd validation checks for Add Property on custom site configs", "id": "2027"}, "2029": {"ground_truth": "0", "bug_report": "Error when loading /main/services directly\nError when loading /main/services directly", "id": "2029"}, "2030": {"ground_truth": "0", "bug_report": "Make frontend changes to account for the host component status UNKNOWN\nService status: if any of the master components are in UNKNOWN state  show the unknown icon. The action buttons for the service are disabled. Host status: if the host status is UNKNOWN or the heartbeat has not been received in more than 180 seconds  show the unknown icon. Host component status: if the host component status is UNKNOWN  show the unknown icon. The action button for the host component is disabled.", "id": "2030"}, "2031": {"ground_truth": "0", "bug_report": "Add clover code coverage profile\nmvn test -Pclover -Dclover.license=&lt;clover.coverage.license&gt; should run the unit tests and return html/xml code coverage reports", "id": "2031"}, "2034": {"ground_truth": "0", "bug_report": "Disable 'Add Component' button in the Host Details page if the host is in UNKNOWN state or !isHeartbeating\nDisable 'Add Component' button in the Host Details page if the host is in UNKNOWN state or !isHeartbeating", "id": "2034"}, "2035": {"ground_truth": "0", "bug_report": "'Add local user' button is enabled but nothing happens upon clicking it under certain conditions\nSteps to reproduce1. Go to Admin tab2. Click on 'Add Local User' button3. Click on Admin tab again4. Clicking on 'Add Local User' button does nothing", "id": "2035"}, "2038": {"ground_truth": "0", "bug_report": "Services links on Dashboard connected to incorrect pages\nClick on any of the service links shown on the Dashboard page.It transitions to the service page and the content displayed is correct for the service chosen  but the URL indicates that it is another service and the side-menu shows a different service highlighted.", "id": "2038"}, "2045": {"ground_truth": "0", "bug_report": "Add Unit test to verify  client re-install for install failed client\nAdd Unit test to verify  When INSTALL is schedules on client components it should also be scheduled on components that are in INSTALL_FAILED state", "id": "2045"}, "2054": {"ground_truth": "0", "bug_report": "If 'Install from Local Repository' selected in install wizard  Add Host wizard not working\nIf 'Install from Local Repository' selected in install wizard  Add Host wizard not working", "id": "2054"}, "2058": {"ground_truth": "0", "bug_report": "Host Detail page: if the host component is in INSTALL_FAILED state  we should let the user reinstall it\nHost Detail page: if the host component is in INSTALL_FAILED state  we should let the user reinstall it", "id": "2058"}, "2061": {"ground_truth": "0", "bug_report": "HBase Heatmaps: clean up labels and units\nHBase Heatmaps: clean up labels and units", "id": "2061"}, "2065": {"ground_truth": "0", "bug_report": "Hadoop group customization does not take affect\nTo customize the hadoop group  when it was changed from 'hadoop' to 'hadoopgroup'  it didn't look like it worked.root@ip-10-85-135-237 hdfsuser# id hdfsuid=495(hdfs) gid=494(hdfs) groups=494(hdfs) 495(hadoop)And looking at the /etc/group filepuppet:x:497:hadoopgroup:x:500:rrdcached:x:496:apache:x:48:hadoop:x:495:mapred hdfs", "id": "2065"}, "2068": {"ground_truth": "0", "bug_report": "'Preparing to install ' message needs spacing\n'Preparing to install ' message needs spacing", "id": "2068"}, "2070": {"ground_truth": "0", "bug_report": "Changing service directories should popup a confirmation/warning dialog upon save\nPost-install  if the user tries to reconfigure NN  SNN  MapReduce local/system directories  we should popup a confirmation/warning upon Save as this is potentially a dangerous operation.", "id": "2070"}, "2075": {"ground_truth": "0", "bug_report": "Admin role can't be assigned to LDAP user\nAdmin role can't be assigned to LDAP user", "id": "2075"}, "2081": {"ground_truth": "0", "bug_report": "changeUid.sh failing during installation\nOn SUSE  I received a puppet error on each agent that /tmp/changeUid.sh failed during installation. (Sorry I no longer have the error  I made puppet change and restarted to get by it). But in a nutshell  running the command manually gave:ip-10-82-233-26:/tmp # /tmp/changeUid.sh ambari-qa 1012 /tmp/ambari-qa /home/ambari-qa /var/spool/mail/ambari-qaChanging uid of ambari-qa from 1012 to 1012Changing directory permisions for /tmp/ambari-qa /home/ambari-qa /var/spool/mail/ambari-qausermod: UID 1012 is not unique.Note that the usermod is trying to change to an existing UID  so the command is failing everywhere", "id": "2081"}, "2087": {"ground_truth": "0", "bug_report": "Tasks are not filtered by parent request id\nSTEPS:1) Get tasks for first request  /api/v1/clusters/&lt;cluster&gt;/requests/&lt;firstRequest&gt;  i.e. task1 ... taskN12) Get tasks for second request by task from first requst  like /api/v1/clusters/&lt;cluster&gt;/requests/&lt;secondRequest&gt;/tasks/task1 (note task1 belongs to first request  not second)3) Notice that task from first request are present in second request.", "id": "2087"}, "2089": {"ground_truth": "0", "bug_report": "Post Ambari upgrade  Hive and Oozie fail to start after reconfigure\nPost Ambari upgrade  Hive and Oozie fail to start after reconfigure", "id": "2089"}, "2095": {"ground_truth": "0", "bug_report": "It's possible to get into a state where install retry is not possible if the agent stops heartbeating\nThis affects both Install and Add Host Wizards.Steps to reproduce:While installing components  stop the agent on one of the hosts.Waiting for a while puts the components on the host into the UNKNOWN state.Click Retry from the UI. This causes a server-side error and the UI gets confused (the hosts are shown with 'Waiting' message and no 'Retry' button is available).The user is not able to get out of this state.", "id": "2095"}, "2101": {"ground_truth": "1", "bug_report": "Hive service check (still) failing with file permissions\nStack upgrade testing is still showing this to be an issue.warning: Unrecognised escape sequence '/;' in file /var/lib/ambari-agent/puppet/modules/hdp-hive/manifests/hive/service_check.pp at line 32warning: Dynamic lookup of $configuration is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes.notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: ls: cannot access /usr/share/java/*oracle*: No such file or directorynotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: 13/05/09 15:01:52 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore.notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: log4j:ERROR setFile(null true) call failed.notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: java.io.FileNotFoundException: /tmp/ambari_qa/hive.log (Permission denied)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.openAppend(Native Method)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:192)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:116)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.FileAppender.setFile(FileAppender.java:290)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:164)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:216)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:257)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:133)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:97)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:689)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:647)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:544)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:440)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:476)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:354)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jDefault(LogUtils.java:124)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(LogUtils.java:77)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4j(LogUtils.java:58)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:61)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.lang.reflect.Method.invoke(Method.java:597)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.util.RunJar.main(RunJar.java:160)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: log4j:ERROR Either File or DatePattern options are not set for appender [DRFA].notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: Exception in thread 'main' java.lang.RuntimeException: java.io.IOException: Permission deniednotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:272)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:79)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)", "id": "2101"}, "2103": {"ground_truth": "0", "bug_report": "Support for configuring and running Ambari Web Server https\nNeed to be able to run Ambari Web (and access Ambari REST APIs) over HTTPS. Should document assuming the user comes with their own certificate. User should also be able to configure which port to expose HTTPS.", "id": "2103"}, "2111": {"ground_truth": "0", "bug_report": "Enable customization of smoke test user\nEnable customization of smoke test user", "id": "2111"}, "2118": {"ground_truth": "0", "bug_report": "ambari-web modifications to allow for Hadoop Compatible Filesystems (HCFS)\nMake modifications to ambari-web that will allow for the selection of a Hadoop Compatible Filesystem. These changes include allowing HDFS to be unselected (either HDFS or HCFS must be selected). If HCFS is chosen  push appropriate configuration (site-conf.xml) files during the install so that systems will work with HCFS as the underlying filesystem rather than HDFS.", "id": "2118"}, "2130": {"ground_truth": "0", "bug_report": "Use modified dependencies if a stack contains an HCFS service (Hadoop Compatible File System)\nUse stack metadata to determine if a stack contains an HCFS service and generate modified RoleCommandOrder dependencies if it does.", "id": "2130"}, "2134": {"ground_truth": "0", "bug_report": "Set default value of oozie property 'oozie.service.AuthorizationService.authorization.enabled' to true.\nSet default value of oozie property 'oozie.service.AuthorizationService.authorization.enabled' to true.", "id": "2134"}, "2136": {"ground_truth": "0", "bug_report": "Home paths are not set correctly in /etc/sqoop/conf/sqoop-env.sh\nAmbari sets the followings:#Set path to where bin/hadoop is availableexport HADOOP_HOME=${HADOOP_HOME:-/usr}#set the path to where bin/hbase is availableexport HBASE_HOME=${HBASE_HOME:-/usr}#Set the path to where bin/hive is availableexport HIVE_HOME=${HIVE_HOME:-/usr}# add libthrift in hive to sqoop class path first so hive imports workexport SQOOP_USER_CLASSPATH=''ls ${HIVE_HOME}/lib/libthrift-*.jar 2&gt; /dev/null':${SQOOP_USER_CLASSPATH}'#Set the path for where zookeper config dir isexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}It should be the followings (also screenshot is available):#Set path to where bin/hadoop is availableexport HADOOP_HOME=${HADOOP_HOME:-/usr/lib/hadoop}#set the path to where bin/hbase is availableexport HBASE_HOME=${HBASE_HOME:-/usr/lib/hbase}#Set the path to where bin/hive is availableexport HIVE_HOME=${HIVE_HOME:-/usr/lib/hive}#Set the path for where zookeper config dir isexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}# add libthrift in hive to sqoop class path first so hive imports workexport SQOOP_USER_CLASSPATH=''ls ${HIVE_HOME}/lib/libthrift-*.jar 2&gt; /dev/null':${SQOOP_USER_CLASSPATH}", "id": "2136"}, "2143": {"ground_truth": "0", "bug_report": "HBASE fails to start on master\nHBASE master fails to start on master. From log:2013-05-14 21:06:43 487 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=hbase  access=EXECUTE  inode='/apps/hbase/data':hdfs:hdfs:drwx------ at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57) at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1134) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:556) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:779) at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:287) at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:329) at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:434) at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:146) at org.apache.hadoop.hbase.master.MasterFileSystem.&lt;init&gt;(MasterFileSystem.java:131) at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:532) at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:391) at java.lang.Thread.run(Thread.java:662)", "id": "2143"}, "2144": {"ground_truth": "0", "bug_report": "Installation with existing Oracle DB fails\nOn Step8 'Customize Services' for HIVE and OOZIE we can use option 'Existing Oracle DB'.But after Cluster install HIVE and OOZIE didn't start (with option 'Existing Oracle DB').In the logs I found such:/var/log/hive/hive.logCaused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the 'DBCP' plugin to create a ConnectionPool gave an error : The specified datastore driver ('oracle.jdbc.driver.OracleDriver') was not found in the CLASSPATH. Please check your CLASSPATH specification  and the name of the driver./var/log/oozie/oozie.log013-05-14 11:25:59 618 FATAL Services:533 - USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] E0103: Could not load service classes  Cannot load JDBC driver class 'oracle.jdbc.driver.OracleDriver'org.apache.oozie.service.ServiceException: E0103: Could not load service classes  Cannot load JDBC driver class 'oracle.jdbc.driver.OracleDriver'", "id": "2144"}, "2146": {"ground_truth": "0", "bug_report": "When hive and oozie users have been changed after upgrade hive metastore and oozie cannot start properly\nOozie start failure:^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_BASE_URL: http://ip-10-212-166-111.ec2.internal:11000/oozie^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_BASE: /var/lib/oozie/oozie-server^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_HTTPS_KEYSTORE_FILE: /home/ooziexx/.keystore^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_HTTPS_KEYSTORE_PASS: password^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting CATALINA_OUT: /var/log/oozie//catalina.out^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_PID: /var/run/oozie/oozie.pid^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: ^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_OPTS: -Dderby.stream.error.file=/var/log/oozie//derby.log^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Adding to CATALINA_OPTS: -Doozie.home.dir=/usr/lib/oozie -Doozie.config.dir=/etc/oozie/conf -Doozie.log.dir=/var/log/oozie/ -Doozie.data.dir=/grid/0/hadoop/oozie/data/ -Doozie.config.file=oozie-site.xml -Doozie.log4j.file=oozie-log4j.properties -Doozie.log4j.reload=10 -Doozie.http.hostname=ip-10-212-166-111.ec2.internal -Doozie.admin.port=11001 -Doozie.http.port=11000 -Doozie.https.port=11443 -Doozie.base.url=http://ip-10-212-166-111.ec2.internal:11000/oozie -Doozie.https.keystore.file=/home/ooziexx/.keystore -Doozie.https.keystore.pass=password -Djava.library.path=/usr/lib/hadoop/lib/native/Linux-amd64-64^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: ^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: /usr/lib/oozie/oozie-server/bin/catalina.sh: line 386: /var/run/oozie/oozie.pid: Permission denied^[[0m^[[1;35merr: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: change from notrun to 0 failed: su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:340^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'::end]: Dependency Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'] has failures: true^[[0m^[[0;33mwarning: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'::end]: Skipping because of failed dependencies^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/log/oozie]/Hdp::Directory_recursive_create[/var/log/oozie]/Hdp::Directory[/var/log/oozie]/File[/var/log/oozie]/owner: owner changed 'oozie' to 'ooziexx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/log/oozie]/Hdp::Directory_recursive_create[/var/log/oozie]/Hdp::Directory[/var/log/oozie]/File[/var/log/oozie]/group: group changed 'oozie' to 'hadoopxx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/run/oozie]/Hdp::Directory_recursive_create[/var/run/oozie]/Hdp::Directory[/var/run/oozie]/File[/var/run/oozie]/owner: owner changed 'oozie' to 'ooziexx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/run/oozie]/Hdp::Directory_recursive_create[/var/run/oozie]/Hdp::Directory[/var/run/oozie]/File[/var/run/oozie]/group: group changed 'oozie' to 'hadoopxx'^[[0m^[[0;36mnotice: Finished catalog run in 9.36 seconds^[[0m", "id": "2146"}, "2147": {"ground_truth": "0", "bug_report": "Capture user for auditing config changes\nAdd the ability to capture username and save in the table for config mappings. This applies to cluster and host level", "id": "2147"}, "2149": {"ground_truth": "0", "bug_report": "Ambari needs to set right path for GC log directory of Hbase process.\nAmbari needs to set right path for GC log directory of Hbase process.", "id": "2149"}, "2152": {"ground_truth": "0", "bug_report": "Sometimes stale host / host component indicators are shown\nSometimes stale host / host component indicators are shown", "id": "2152"}, "2159": {"ground_truth": "0", "bug_report": "After upgrading ambari from 1.2.2.5 to 1.2.3.6 the server throws 500 error when starting/stopping any service\nAfter upgrading ambari from 1.2.2.5 to 1.2.3.6 the server throws 500 error when starting/stopping any service", "id": "2159"}, "2161": {"ground_truth": "1", "bug_report": "Datanode Start fails in secure cluster.\nDatanode Start fails in secure cluster.", "id": "2161"}, "2171": {"ground_truth": "0", "bug_report": "Host status filter not restored on Hosts page when navigating back\nHost status filter not restored on Hosts page when navigating back", "id": "2171"}, "2172": {"ground_truth": "0", "bug_report": "Fix unit tests for Ambari Web\nFix currently failing unit tests.", "id": "2172"}, "2173": {"ground_truth": "0", "bug_report": "TEST BROKEN : FAIL: test_upgradeCommand_executeCommand (TestActionQueue.TestActionQueue)\nTEST BROKEN : FAIL: test_upgradeCommand_executeCommand (TestActionQueue.TestActionQueue)", "id": "2173"}, "2180": {"ground_truth": "0", "bug_report": "Remove '0.1' stack definition since its never been used and is redundant.\nRemove '0.1' stack definition since its never been used and is redundant.", "id": "2180"}, "2187": {"ground_truth": "0", "bug_report": "Hadoop2 Monitoring: Jobs page should be hidden when HDP 2.0.x stack is installed\nWhen a HDP 2.0.x stack is installed  the Jobs page should be hidden.", "id": "2187"}, "2188": {"ground_truth": "0", "bug_report": "Update mock json data for Test mode\nUpdate mock json data for Test mode", "id": "2188"}, "2192": {"ground_truth": "0", "bug_report": "Agent heartbeat lost during install\nAgent heartbeat can become lost during install. The underlying issue is that during install  various yum commands are executed for component installation. However  the heartbeat ALSO performs a 'yum -C repolist'. If that command is taking a long time  the yum process can become deadlocked. The results of the repolist'ing are not used at this time  so remove it until needed.", "id": "2192"}, "2195": {"ground_truth": "0", "bug_report": "Ambari has a deadlock when re-installing after reboot of cluster nodes\nJava stack information for the threads listed above:==================================================='Thread-2': at org.apache.ambari.server.state.ServiceImpl.getDesiredConfigs(ServiceImpl.java:240) waiting to lock &lt;0x000000077b356dd0&gt; (a org.apache.ambari.server.state.ServiceImpl$$EnhancerByGuice$$9e2acafa) at org.apache.ambari.server.state.ServiceComponentImpl.getDesiredConfigs(ServiceComponentImpl.java:292) locked &lt;0x000000077b39bce8&gt; (a org.apache.ambari.server.state.ServiceComponentImpl$$EnhancerByGuice$$af7a745c) at org.apache.ambari.server.state.svccomphost.ServiceComponentHostImpl.getDesiredConfigs(ServiceComponentHostImpl.java:1057) at org.apache.ambari.server.agent.HeartbeatMonitor.generateStatusCommands(HeartbeatMonitor.java:166) at org.apache.ambari.server.agent.HeartbeatMonitor.doWork(HeartbeatMonitor.java:137) at org.apache.ambari.server.agent.HeartbeatMonitor.run(HeartbeatMonitor.java:85) at java.lang.Thread.run(Thread.java:662)'main': at org.apache.ambari.server.state.ServiceComponentImpl.debugDump(ServiceComponentImpl.java:376) waiting to lock &lt;0x000000077b39bce8&gt; (a org.apache.ambari.server.state.ServiceComponentImpl$$EnhancerByGuice$$af7a745c) at org.apache.ambari.server.state.ServiceImpl.debugDump(ServiceImpl.java:354) locked &lt;0x000000077b356dd0&gt; (a org.apache.ambari.server.state.ServiceImpl$$EnhancerByGuice$$9e2acafa) at org.apache.ambari.server.state.cluster.ClusterImpl.debugDump(ClusterImpl.java:693) at org.apache.ambari.server.state.cluster.ClustersImpl.debugDump(ClustersImpl.java:517) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:320) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:432)Found 1 deadlock.", "id": "2195"}, "2200": {"ground_truth": "0", "bug_report": "ambari-server start script (ambari-server.py) will never use SERVER_START_CMD_DEBUG\nThe ambari-server.py start script has a command defined for starting the ambari-server in debug mode (SERVER_START_CMD_DEBUG  which turns on remote debugging)  but there is currently no option supported that will force the script to use the debug start commaand. I propose adding a --debug option so that you can run 'ambari-server start --debug' to activate remote debugging.", "id": "2200"}, "2203": {"ground_truth": "0", "bug_report": "Background operations popup does not automatically refresh the task log\nBackground operations popup does not automatically refresh the task log", "id": "2203"}, "2207": {"ground_truth": "0", "bug_report": "Add unit tests for Utils\nAdd unit tests for Utils", "id": "2207"}, "2208": {"ground_truth": "0", "bug_report": "Reassign Master Wizard: refreshing page on step 2  3 or 4 breaks wizard\nReassign Master Wizard: refreshing page on step 2  3 or 4 breaks wizard", "id": "2208"}, "2212": {"ground_truth": "0", "bug_report": "Change config loading mechanism to allow for different stack versions\nChange config loading mechanism to allow for different stack versions", "id": "2212"}, "2217": {"ground_truth": "0", "bug_report": "Increase ambari-agent test coverage\nActionQueue.py missing 'Unrecognized command' testcase (L. 173) /src/test/python/TestActionQueue.py:42 unused test_RetryAction stub (retry is implemented in another way) /src/main/python/ambari_agent/ActionQueue.py:221 not covered case if commandresult&#91;&#39;exitcode&#39;&#93; != 0: /src/main/python/ambari_agent/ActionQueue.py:247 not covered case if command.has_key('roleCommand') and command&#91;&#39;roleCommand&#39;&#93; == 'START':PuppetExecutor.py configureEnviron/generate_repo_manifests/run_manifest/runCommand are not covered.PythonExecutor.py isSuccessfull is not testedRepoInstaller.py prepareReposInfo/generateFiles are not coveredshell.py is not covered", "id": "2217"}, "2223": {"ground_truth": "0", "bug_report": "Using an external MySQL / Oracle database for Oozie does not work\nWhen setting up Oozie with an external database  the following commands are run:cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-setup.sh -hadoop 0.20.200 /usr/lib/hadoop/ -extjs /usr/share/HDP-oozie/ext.zip -jars /usr/lib/hadoop/lib/hadoop-lzo-0.5.0.jar:/usr/share/java/mysql-connector-java.jarThe above command succeeds.However  the next command fails:cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/ooziedb.sh create -sqlfile oozie.sql -run setting OOZIE_CONFIG=${OOZIE_CONFIG:-/etc/oozie/conf} setting OOZIE_DATA=${OOZIE_DATA:-/var/lib/oozie} setting OOZIE_LOG=${OOZIE_LOG:-/var/log/oozie} setting CATALINA_BASE=${CATALINA_BASE:-/var/lib/oozie/oozie-server} setting CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie} setting CATALINA_PID=${CATALINA_PID:-/var/run/oozie/oozie.pid} setting JAVA_HOME=/usr/jdk/jdk1.6.0_31 setting OOZIE_LOG=/var/log/oozie/ setting CATALINA_PID=/var/run/oozie/oozie.pid setting OOZIE_DATA=/grid/0/hadoop/oozie/data/ setting JAVA_LIBRARY_PATH=/usr/lib/hadoop/lib/native/Linux-amd64-64Validate DB ConnectionError: Could not connect to the database: java.lang.ClassNotFoundException: com.mysql.jdbc.DriverStack trace for the error was (for debug purposes):--------------------------------------java.lang.Exception: Could not connect to the database: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at org.apache.oozie.tools.OozieDBCLI.validateConnection(OozieDBCLI.java:358) at org.apache.oozie.tools.OozieDBCLI.createDB(OozieDBCLI.java:168) at org.apache.oozie.tools.OozieDBCLI.run(OozieDBCLI.java:112) at org.apache.oozie.tools.OozieDBCLI.main(OozieDBCLI.java:63)Caused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:169) at org.apache.oozie.tools.OozieDBCLI.createConnection(OozieDBCLI.java:347) at org.apache.oozie.tools.OozieDBCLI.validateConnection(OozieDBCLI.java:354) ... 3 more--------------------------------------", "id": "2223"}, "2228": {"ground_truth": "0", "bug_report": "Fix MySQL and Oracle DDL scripts according to last DB changes\nuser_name column was added to clusterconfigmapping and hostconfigmapping tables. This changes should be made to DDL scripts for Oracle and MySQL also.", "id": "2228"}, "2233": {"ground_truth": "0", "bug_report": "Ensure version values are used appropriately throughout Ambari\nThe current version of Ambari build is being used in several scenarios: Ensure Ambari Server installs the correct version of Ambari Agent Ensure that Ambari Server only accepts registration from correct version of Ambari Agent Ensure that DB version is compatible with Ambari Server  The DB version itself will be used to control DB upgrades  Towards this end the following open issues remain: Get the build version be automatically embedded in the version file Use the above for deploying agent as well as allowing agents to register Regarding DB version there are two possible paths:  Separate out DB version and have it be modified manually as needed Have the build version be used as DB version - this may make writing upgrade scripts little complicated as build version may change due to some proj mgmt decision", "id": "2233"}, "2240": {"ground_truth": "0", "bug_report": "Allow Security related configs to be modified via custom settings\nAllow Security related configs to be modified via custom settings", "id": "2240"}, "2259": {"ground_truth": "0", "bug_report": "Start/Stop button may stay enabled for 30-40 seconds after it has been clicked\nStart/Stop button stays enabled for atleast 30-40 seconds after its been clicked already.", "id": "2259"}, "2260": {"ground_truth": "0", "bug_report": "Bad hosts query example in API docs\nThe example ...'hosts' : [ { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/host1'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'some.cluster.host' } }  { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/host2'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'another.cluster.host' } ]... should read ... 'hosts' : [ { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/some.host'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'some.host' } }  { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/another.host'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'another.host' } } ]", "id": "2260"}, "2262": {"ground_truth": "1", "bug_report": "On 'install Options' page  when selecting 'Perform manual registration on hosts and do not use SSH' is setting 'Path to 64-bit JDK' disabled\nOn 'install Options' page  when selecting 'Perform manual registration on hosts and do not use SSH' is setting 'Path to 64-bit JDK' disabled(Screen Shot 2013-06-03 at 10.54.49 AM.png).Also path to 64-bit JDK JAVA_HOME' input field is enabled with unchecked check box(unchecked.png).Steps:1. Go to 'Install Options' page.Result:'Path to 64-bit JDK JAVA_HOME' input field is available for editing when check box is unchecked.", "id": "2262"}, "2270": {"ground_truth": "1", "bug_report": "Provide way to optionally enable two-way SSL for Server-Agent communication\nThe two-way SSL mechanism used during server-agent registration exists to protect communication. This is useful in production environments but in typical 'first use' or POC scenarios  having this level of security is not necessary. As well  certificate generation can be problematic causing failures.We need to provide a way to make this mechanism optional:1) By default  ship with Server-Agent Two-Way SSL off.2) At any time post install  a user should be able to turn on Two-Way SSL and turn it back off  etc.", "id": "2270"}, "2279": {"ground_truth": "0", "bug_report": "Configuration mapping metadata on ambari-web should be computed as per the stack selection.\nConfiguration mapping metadata on ambari-web should be computed as per the stack selection.", "id": "2279"}, "2290": {"ground_truth": "0", "bug_report": "Ambari Upgrade prcoess should preserve the old configs and add the new config options to the old config files.\nThe Ambari Upgrade process should preserve the old configs and add the new config options to the old config files.Currently we have it the other way around  that we copy the needed 3 properties from the old config files - this is wrong. We need to use the older config file and add the new options to the old config file. This is because the older config file can have all kinds of config option that the user might have used. We have to really really keep in mind usability of the product when fixing issues.", "id": "2290"}, "2300": {"ground_truth": "0", "bug_report": "500 Exception creating service component during install\nI was installing a new cluster in my VM when the progress blocked on step 12. Looking on browser log  the PUTs for clusters desired_configs succeeded  but the very next call to create service component failed.&#91;POST&#93; http://dev.hortonworks.com:8080/api/v1/clusters/vmc/services?ServiceInfo/service_name=HDFSStatus Code:500 Invalid arguments  clustername and componentname should be non-null and non-empty when trying to create a componentData uploaded:{'components':[{'ServiceComponentInfo':{'component_name':'NAMENODE'}} {'ServiceComponentInfo':{'component_name':'SECONDARY_NAMENODE'}} {'ServiceComponentInfo':{'component_name':'DATANODE'}} {'ServiceComponentInfo':{'component_name':'HDFS_CLIENT'}}]}:Exception on server console:Mar 21  2013 11:25:09 AM com.sun.jersey.spi.container.ContainerResponse mapMappableContainerExceptionSEVERE: The RuntimeException could not be mapped to a response  re-throwing to the HTTP containerjava.lang.IllegalArgumentException: Invalid arguments  clustername and componentname should be non-null and non-empty when trying to create a component at org.apache.ambari.server.controller.AmbariManagementControllerImpl.createComponents(AmbariManagementControllerImpl.java:387) at org.apache.ambari.server.controller.internal.ComponentResourceProvider$1.invoke(ComponentResourceProvider.java:88) at org.apache.ambari.server.controller.internal.ComponentResourceProvider$1.invoke(ComponentResourceProvider.java:85) at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:229) at org.apache.ambari.server.controller.internal.ComponentResourceProvider.createResources(ComponentResourceProvider.java:85) at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:131) at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:75) at org.apache.ambari.server.api.handlers.QueryCreateHandler.persist(QueryCreateHandler.java:163) at org.apache.ambari.server.api.handlers.QueryCreateHandler.handleRequest(QueryCreateHandler.java:68) at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:98) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:73) at org.apache.ambari.server.api.services.ServiceService.createServices(ServiceService.java:114) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597)", "id": "2300"}, "2313": {"ground_truth": "0", "bug_report": "UI allows adding already existing properties to custom core-site.xml /hdfs-site.xml settings and creates confusion\nSteps:1. Go to 'Services' page.2. Select 'HDFS' service.3. Select 'Configs' tab.4. Open 'Custom core-site.xml' panel.5. Add property with name 'ipc.client.idlethreshold' to this panel.Result:Property with name 'ipc.client.idlethreshold' was added to 'Custom core-site.xml' panel. But there is already presented property with same name in 'core-site.xml' file. After saving added property was disappeared from UI  and after service starting value of the old was changed to new (on UI and in 'core-site.xml' file).Expected result:UI should not allow to add property with existing name in specified file.", "id": "2313"}, "2337": {"ground_truth": "1", "bug_report": "Security Wizard: navigation not locked down  causes artifacts  and other unwanted side effects\nSecurity Wizard: navigation not locked down  causes artifacts  and other unwanted side effects", "id": "2337"}, "2346": {"ground_truth": "0", "bug_report": "API call to get 'metrics/cpu' does not work for NameNode and JobTracker host components\n1. http://ambari:8080/api/v1/clusters/cluster/services?fields=components/host_components/metricsincludes all metrics for NameNode and JobTracker  including cpu metrics.2. http://ambari:8080/api/v1/clusters/cluster/services?fields=components/host_components/metrics/cpudoes not return cpu metrics for NameNode and JobTracker", "id": "2346"}, "2349": {"ground_truth": "0", "bug_report": "Enhance processing of ojdbc.jar before starting ambari server\nEnhancements: Read RESOURCE_DIR from the ambari.properties Ask user twice to place drivers to /usr/share/java", "id": "2349"}, "2362": {"ground_truth": "0", "bug_report": "Unit Tests: Added tests to install wizard for step 3  5  10\nUnit Tests: Added tests to install wizard for step 3  5  10", "id": "2362"}, "2363": {"ground_truth": "0", "bug_report": "Intermittent test failure with HBase port Scanner test.\nThis test fails sometimes and is not very reliable.", "id": "2363"}, "2371": {"ground_truth": "1", "bug_report": "Security Wizard: webhcat Server start fails on enabling security\nThis happens when templeton.kerberos.principal property is set to HTTP/_HOST@&lt;realm name&gt; instead of HTTP/&lt;internal host name&gt;@&lt;realm name&gt;", "id": "2371"}, "2372": {"ground_truth": "0", "bug_report": "Show installed stack and its services\nList the installed stack and its services in Admin &gt; Cluster page.", "id": "2372"}, "2375": {"ground_truth": "0", "bug_report": "Unit Tests: Added tests to models\nAdded unit tests to models: Rack  Host  HostComponent.", "id": "2375"}, "2377": {"ground_truth": "1", "bug_report": "Add Host Wizard: show info about manual steps required on a secure cluster\nWhen a cluster is in secure mode  there are additional manual steps that need to be performed when adding new hosts to the cluster.In the Review page  add a prominent box with a red background and display the following text:You are running your cluster in secure mode. You must set up the keytabs for all the hosts you are adding before you proceed.'Upon clicking 'Deploy'  show a confirmation popup with the text:Before you proceed  please make sure that the keytabs have been set up on the hosts you are adding per the instructions on the Review page. Otherwise  the assigned components will not be able to start properly on the hosts being added. OK CancelNote that the extra message and confirmation popup should show only when security is enabled on the cluster.", "id": "2377"}, "2383": {"ground_truth": "0", "bug_report": "Add unit tests for ambari-server python changes\nAMBARI-2174 - Add missing unit tests for the code changes", "id": "2383"}, "2391": {"ground_truth": "0", "bug_report": "Bootstrap is broken for ambari web with RHEL-5.8\nSteps:1. Install ambari-server.2. Go to 'Install Options' page.3. Set the hosts list and ssh-key  click 'Next' button.Result:Ambari Web is blocked in 'Confirm Hosts'. Confirming was not ended after not less than 40 minutes.", "id": "2391"}, "2394": {"ground_truth": "0", "bug_report": "ambari-server setup borken in trunk\ninstall_jce_manually() in the download_jdk() result in fatal exception.Checking JDK...INFO: Loading properties from /etc/ambari-server/conf/ambari.propertiesERROR: Error getting ambari propertiesERROR: Exiting with exit code -1. Reason: Downloading or installing JDK failed: 'Fatal exception: Error getting ambari properties  exit code -1'. Exiting.", "id": "2394"}, "2402": {"ground_truth": "0", "bug_report": "Add support for 'classic' dashboard\nAdd support for 'classic' dashboard", "id": "2402"}, "2403": {"ground_truth": "1", "bug_report": "ambari-server setup should allow user to change database password\nIf we run setup second time. Amabri should allow user to change the DB password for the ambari-user.Currently  the executed script should allow for this.command: &#91;&#39;su&#39;  &#39;-&#39;  &#39;postgres&#39;  &#39;--command=psql -f /var/lib/ambari-server/resources/Ambari-DDL-Postgres-CREATE.sql -v username=/&#39;&quot;ambari-server&quot;/&#39; -v password=&quot;/&#39;/&#39;&quot;&#39;&#93;", "id": "2403"}, "2408": {"ground_truth": "0", "bug_report": "Kerberos globals are shown in HDFS config page during install\nKerberos globals are shown in HDFS config page during install", "id": "2408"}, "2413": {"ground_truth": "0", "bug_report": "Installer Wizard step-6: NameNode and SNameNode should not be co-hosted by default on multinode cluster.\nInstaller Wizard step-6: NameNode and SNameNode should not be co-hosted by default on multinode cluster.", "id": "2413"}, "2414": {"ground_truth": "0", "bug_report": "HDFS Config page is broken in testMode on trunk\nHDFS Config page (post-install) does not load when App.testMode = true", "id": "2414"}, "2426": {"ground_truth": "0", "bug_report": "Set default widgets to show for the Dashboard\nSet default widgets to show for the Dashboard", "id": "2426"}, "2433": {"ground_truth": "0", "bug_report": "Bootstrap failed on rhel 5.6\nSTDOUTTraceback (most recent call last):File '/tmp/setupAgent.py'  line 192  in ?main(sys.argv)File '/tmp/setupAgent.py'  line 188  in mainsys.exit(runAgent(passPhrase  expected_hostname))File '/tmp/setupAgent.py'  line 80  in runAgentagent_retcode = subprocess.call('/usr/sbin/ambari-agent start --expected-hostname={0}'.format(expected_hostname)  shell=True)AttributeError: 'str' object has no attribute 'format'Error seems to be caused by python 2.4 version.", "id": "2433"}, "2441": {"ground_truth": "0", "bug_report": "Ambari server start fails with reconfigured user\nSTR:1) Run ambari-server setup.2) Choose custom user  user1.3) Delete ambari.user from ambari.properties.4) Run ambari-server setup.5) Choose custom user  different from choosen in step 2  user2.6) Run ambari-server start.Got error:ambari-server startUsing python /usr/bin/python2.6Starting ambari-serverHave root privileges.Checking iptables...iptables is disabled nowRunning server: &#91;&#39;/bin/su&#39;  &#39;user2&#39;  &#39;-s&#39;  &#39;/bin/sh&#39;  &#39;-c&#39;  &#39;/usr/jdk64/jdk1.6.0_31/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Xms512m -Xmx2048m -cp /etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/usr/lib/ambari-server/* org.apache.ambari.server.controller.AmbariServer &gt;/var/log/ambari-server/ambari-server.out 2&gt;&amp;1 &amp; echo $! &gt; /var/run/ambari-server/ambari-server.pid&#39;&#93;done.sh: /var/run/ambari-server/ambari-server.pid: Permission deniedsh: /var/log/ambari-server/ambari-server.out: Permission denied", "id": "2441"}, "2443": {"ground_truth": "1", "bug_report": "Security wizard: smoke test for services fails with customized service user names.\nSecurity wizard: smoke test for services fails with customized service user names.", "id": "2443"}, "2447": {"ground_truth": "0", "bug_report": "Upgrade from 1.2.2/1.2.3.7 to 1.2.5 fails because the ddl script does not work - metainfo version change is broken.\nUpgrade from 1.2.2/1.2.3.7 to 1.2.5 fails because the ddl script does not work - metainfo version change is broken.Again we need thorugh testing around this. Please make sure we have tested 1.2.2 and 1.2.3  1.2.4 upgrade to 1.2.5.", "id": "2447"}, "2461": {"ground_truth": "0", "bug_report": "Add unit tests for bootstrap and setupAgent python scripts for the server.\nAdd unit tests for bootstrap and setupAgent python scripts for the server.We need to find gaps on the bootstrap and setupagent script and add unit tests for them.", "id": "2461"}, "2465": {"ground_truth": "0", "bug_report": "Create command line script to manipulate Ambari configurations (get  set  add  delete)\nWe need a script which can manually set the configs when UI is not able to  or does not support.", "id": "2465"}, "2466": {"ground_truth": "0", "bug_report": "Hive/Oozie database settings should accept custom JDBC URLs\nRan into issues setting up Hive and Oozie with an Oracle database.1. We are hard-coding port 1521 for the JDBC URL. 2. There are two types of JDBC URLs for Oracle: jdbc:oracle:thin:@&#91;HOST&#93;&#91;:PORT&#93;:SID jdbc:oracle:thin:@//&#91;HOST&#93;&#91;:PORT&#93;/SERVICEWe are making the assumption that it is the latter  but this may not work depending on how Oracle is set up.3. We prompt for the 'Database Name'. In Oracle context  this could be the SID or SERVICE NAME  but it's not clear what this is.As a solution to all of the above  we will construct the JDBC URL based on the database type  host  and name for Hive/Oozie and present it to the user as an editable text field during install.Post-install  the JDBC URL remains editable  but does not change automatically as changes other database-related parameters.", "id": "2466"}, "2471": {"ground_truth": "0", "bug_report": "Remove unnecessary check for hostnames/ service name which is wrong.\nRemove unnecessary check for hostnames/ service name which is wrong.", "id": "2471"}, "2475": {"ground_truth": "0", "bug_report": "Ambari bootstrap actions report success even if a failure happened\nSometimes  bootstrap actions reports success even if it has failed host tasks:&lt;bootStrapRequest&gt; &lt;status&gt;SUCCESS&lt;/status&gt; &lt;hostsStatus&gt; &lt;hostName&gt;andromeda54.hi.inet&lt;/hostName&gt; &lt;status&gt;FAILED&lt;/status&gt; &lt;statusCode&gt;1&lt;/statusCode&gt; &lt;log&gt; [...] &lt;/log&gt; &lt;/hostsStatus&gt; &lt;hostsStatus&gt; &lt;hostName&gt;andromeda55.hi.inet&lt;/hostName&gt; &lt;status&gt;FAILED&lt;/status&gt; &lt;statusCode&gt;1&lt;/statusCode&gt; &lt;log&gt; [...] &lt;/log&gt; &lt;/hostsStatus&gt; &lt;log&gt; [...] &lt;/log&gt;&lt;/bootStrapRequest&gt;", "id": "2475"}, "2480": {"ground_truth": "0", "bug_report": "Dashboard page has a lot of footer padding\nThis ticket solved:1. Dashboard page has padding space under footer.2. When a widget got deleted  the page scroll to top.(page should stay in place)", "id": "2480"}, "2486": {"ground_truth": "0", "bug_report": "shell.killprocessgrp is not working in a reliable way\nWe have noticed an issue where shell.killprocessgrp is not working correctly. We have seen a scenario where namenode start takes a long time when on a secure cluster the jce policy is unavailable. After 10 minutes when the agent tries to kill the puppet process it invariably fails.We need to run some experiment (perhaps using long running puppet processes) to ensure that shell.killprocessgrp works as expected.Also  we need to verify that the behavior is as expected on both RHEL and Suse.", "id": "2486"}, "2490": {"ground_truth": "0", "bug_report": "Issues with setup ldap\nI got this blow-up (when bind anon was either false or I just pressed returnBind anonymously true/false (false):====================Review Settings====================authentication.ldap.primaryUrl: my.ldap:389authentication.ldap.secondaryUrl: asdauthentication.ldap.useSSL: falseauthentication.ldap.usernameAttribute: uidauthentication.ldap.baseDn: basednauthorization.userRoleName: userauthorization.adminRoleName: adminauthentication.ldap.bindAnonymously: falseTraceback (most recent call last):File '/usr/sbin/ambari-server.py'  line 3047  in &lt;module&gt;main()File '/usr/sbin/ambari-server.py'  line 2891  in mainsetup_ldap()File '/usr/sbin/ambari-server.py'  line 2353  in setup_ldapprint('%s: %s' % (property  ldap_property_value_mapproperty))KeyError: 'authentication.ldap.managerDn'", "id": "2490"}, "2491": {"ground_truth": "0", "bug_report": "Security Wizard: show which principals and keytabs need to be created on which hosts\nCurrently it is very difficult to know what principals and keytabs need to be created on which hosts.We should present this information to the end user in a format that is easy to consume.The user running the wizard may not be the one who will be creating keytabs and principals. We can expose the capability to download a csv file and send it to the appropriate person who may parse the data to create a script to generate principals/keytabs (or do so manually).Display the attached as a popup after Configure Services step is done.Let's show it as a popup so that we don't affect any existing navigation/flow.For generating the content:Keytab paths are based on the user inputPrincipal names are based on the user inputNameNode host: show the nn and HTTP principals and keytab pathsJobTracker host: show the jt principal and keytab pathOozie Server host: show the oozie and HTTP principals and keytab pathsNagios Server host: show the nagios principal and keytab pathHBase Master host: show the hbase principal and keytab pathHive Server host: show the hive principal and keytab pathWebHCat Server host: show the HTTP principal and keytab pathZooKeeper Server host: show the zookeeper principal and keytab pathDataNode host: show the dn principal and keytab pathTaskTracker host: show the tt principal and keytab pathRegionServer host: show the hbase principal and keytab pathIf there are duplicated principals on the same host  display it only once.Clickng on 'Download CSV' downloads the CSV file ('host-principal-keytab-list.csv'). The same content  except each row is a comma-delimited list with a /n at the end.", "id": "2491"}, "2497": {"ground_truth": "0", "bug_report": "Remove dependence on dfs_datanode_http_address global for Nagios checks\nambari-agent/src/main/puppet/modules/hdp-nagios/templates/hadoop-services.cfg.erb shows the following:# HDFS::DATANODE Checksdefine service { hostgroup_name slaves use hadoop-service service_description DATANODE::DataNode process down servicegroups HDFS check_command check_tcp!&lt;%=scope.function_hdp_template_var('dfs_datanode_http_address')%&gt;!-w 1 -c 1 normal_check_interval 1 retry_check_interval 0.5 max_check_attempts 3}define service { hostgroup_name slaves use hadoop-service service_description DATANODE::DataNode storage full servicegroups HDFS check_command check_datanode_storage!&lt;%=scope.function_hdp_template_var('dfs_datanode_http_address')%&gt;!90%!90% normal_check_interval 5 retry_check_interval 1 max_check_attempts 2}We need to remove dependence on dfs_datanode_http_address and use the actual config property like:hdp_get_port_from_url($hdfs-site['dfs.datanode.http.address'])", "id": "2497"}, "2498": {"ground_truth": "0", "bug_report": "Cleanup setup https flow\nExpected flow:[root@localhost ~]# ambari-server setup-httpsUsing python /usr/bin/python2.6Setting up HTTPS properties...Do you want to configure HTTPS [y/n] (y)?SSL port (8443) ? Please enter path to Certificate: /some/path/on/my/host/server.crtPlease enter path to Private Key: /some/path/on/my/host/server.keyPlease enter password for Private Key:Importing and saving certificate...done.NOTE: Reset Ambari Server to apply changes ('ambari-server restart|stop|start')Ambari Server 'HTTPS setup' completed successfully. Exiting.", "id": "2498"}, "2515": {"ground_truth": "1", "bug_report": "Cannot add property mapred.task.tracker.task-controller\nCannot add property mapred.task.tracker.task-controller", "id": "2515"}, "2517": {"ground_truth": "0", "bug_report": "Decommission data node not working in secure mode\nDecommission datanode does not do kinit before refresh.", "id": "2517"}, "2519": {"ground_truth": "0", "bug_report": "Add download CSV action for security wizard\nWe need CSV content which shows which principals  keytabs etc. end up on the various hosts. This will be useful in scripts or other tools which can create appropriate environment.", "id": "2519"}, "2522": {"ground_truth": "0", "bug_report": "Zookeeper smoke test failing in secure cluster\nZookeeper smoke test failing in secure cluster", "id": "2522"}, "2525": {"ground_truth": "0", "bug_report": "Add helpful message when not able to download jdk with setup options for the user to be able to specify the jdk.\nAdd helpful message when not able to download jdk with setup options for the user to be able to specify the jdk.", "id": "2525"}, "2532": {"ground_truth": "1", "bug_report": "Incorrect permission on taskcontroller.cfg\n/etc/conf/hadoop/taskcontroller.cfgIn secure mode permissions are set to '400'.", "id": "2532"}, "2534": {"ground_truth": "1", "bug_report": "Some memory configs are set to -1 in Ambari\nSome memory configs are set to -1 in ambari-mapred.cluster.reduce.memory.mb-mapred.jobtracker.maxtasks.per.job-mapred.cluster.max.reduce.memory.mb-mapred.cluster.map.memory.mb-mapred.job.map.memory.mb-mapred.job.reduce.memory.mb-mapred.cluster.max.map.memory.mbModify the stack definition to put default values as appropriate.", "id": "2534"}, "2542": {"ground_truth": "0", "bug_report": "Custom Repo URL cannot be set when non-root\nThe first iteration for creating custom repo URL persisted the new URL to disk. This poses a problem when running Ambari as non-root because the file system is owned by root. Change the implementation to save the override in the metainfo table.", "id": "2542"}, "2545": {"ground_truth": "0", "bug_report": "Regression: Agent external hostname is not verified during bootstrap with no warnings\nWhen confirming hosts using external addresses bootstrapping should be failed immediately and a warning should be logged. Right now this functionality is broken  neither warning in log nor failing immediately present", "id": "2545"}, "2546": {"ground_truth": "0", "bug_report": "Simplify Local Repo setup in installer UI\nEnhance the UI to use stacks API to give user a choice of stacks and be able to customize repository locations.", "id": "2546"}, "2555": {"ground_truth": "0", "bug_report": "Security Wizard: Create separate page for principal/keytab\nAdd step 'Create Principals and Keytabs' to Security wizard.", "id": "2555"}, "2556": {"ground_truth": "0", "bug_report": "Ctrl+C during ambari-server setup prints out a python stack trace\nWe should be able to catch KeyBoardInterrupt in ambari-server main and print a useful message like:'Aborting ... Keyboard Interrupt.' and avoid the stack trace for the user.", "id": "2556"}, "2568": {"ground_truth": "1", "bug_report": "Setup LDAP does not validate true/false response\nGarbage responses to true/false questions just pass thru. Notice below  just put in garbage for Use SSL and that's what it would have written.Need validation to confirm they enter either 1) return to accept default or 2) the word true or 3) the word false. Else  inform the user'Property must be 'true' or 'false'.' and ask again.Secondary URL :Use SS &#91;true/false&#93; (false): asdUser name attribute* (uid):Base DN* :Property cannot be blank.Base DN* : asdBind anonymously* true/false (false):Manager DN* :asdEnter Manager Password*:Re-enter password:Passwords do not matchEnter Manager Password*:Re-enter password:====================Review Settings====================authentication.ldap.primaryUrl: my.url:849authentication.ldap.useSSL: asdauthentication.ldap.usernameAttribute: uid", "id": "2568"}, "2578": {"ground_truth": "0", "bug_report": "Using another user for ambari server user create a local group for the ambari server user with same name.\nUsing an ambari-qa user (that is a ldap user and he has hadoop set up as a primary ldap group) [root@va21 ldap]# id ambari-qauid=524(ambari-qa) gid=522(hadoop) groups=522(hadoop)causes ambari-server setup to create an ambari-qa local group (at /etc/group). ambari-qa:x:601:ambari-qa[root@va21 ldap]# id ambari-qauid=524(ambari-qa) gid=522(hadoop) groups=522(hadoop) 601(ambari-qa)Ldap users and groups are transparent for ambari-server  it starts well.The problem is that additional group is created.", "id": "2578"}, "2585": {"ground_truth": "0", "bug_report": "Host Check report show hosts without issues\nReport contains hosts  which don't have issues  but should show 'A space delimited list of hosts which have issues'.", "id": "2585"}, "2590": {"ground_truth": "0", "bug_report": "JS Error when deleting a widget after sorting it on remove/edit sign\nJS Error when deleting a widget after sorting it on remove/edit sign", "id": "2590"}, "2594": {"ground_truth": "0", "bug_report": "HDP installation fails due to puppet syntax error\nnamenode_host is not an hash or array when accessing it with 0 at /var/lib/ambari-agent/puppet/modules/hdp/manifests/params.pp:70 on node host1.", "id": "2594"}, "2595": {"ground_truth": "0", "bug_report": "Properties of the same name cannot be added to different custom site.xml's\nSteps (Installer Wizard):Go to 'Customize Services' page.Select 'HDFS' tab.Add custom property 'xxx' to 'Custom core-site.xml' panel.Try add custom property 'xxx' to 'Custom hdfs-site.xml' panel.Result:Custom property can not be added to 'Custom hdfs-site.xml' panel (see attachment).Steps (Ambari monitoring UI):Go to 'Customize Services' page.Select 'HDFS' tab.Add custom property to 'Custom core-site.xml' panel (for example  'install-test-core-site').Continue and end hadoop installation.Go to 'Services' page.Select 'MapReduce' tab.Try add custom property 'install-test-core-site' to 'Custom mapred-site.xml' panel.Result:Custom property can not be added to 'Custom mapred-site.xml' panel (see attachment).", "id": "2595"}, "2600": {"ground_truth": "0", "bug_report": "Add Quick Links (Web UI) for Oozie  Hue  Nagios  Ganglia\nThere are no quick links for Oozie. Similarly  some other services also are missing the quick links.", "id": "2600"}, "2605": {"ground_truth": "0", "bug_report": "'kdestroy' not required for zookeeper smoke test\nzookeeper smoke test passes without having to 'kdestroy' on the user running the smoke test.", "id": "2605"}, "2608": {"ground_truth": "0", "bug_report": "WebHCat and Oozie services does not start on RHEL5 with enabled security because of 'CRITICAL: Error doing kinit for nagios'\nFE only has support to provide single path for kinit. As Ambari supports mixed OS deployment it cannot be guaranteed that kinit exists at the same path on all nodes. FE should allow providing a set of look-up paths for kinit as well as the BE should support a set of default lookup paths.", "id": "2608"}, "2612": {"ground_truth": "0", "bug_report": "Rename agent.fqdn property in ambari.props to server.fqdn\nlets just rename agent -&gt; server", "id": "2612"}, "2613": {"ground_truth": "0", "bug_report": "Host Checks: truncation on checked processes makes it difficult to know the actual processes in conflict\nProcesses are truncated too short and can't really tell what's in conflict. Since there is a lot of space on the right (in fact  the hostname column is too far to the left compared to other sections)  we should display more characters (with hover tooltip showing full text).", "id": "2613"}, "2614": {"ground_truth": "0", "bug_report": "Popover with config name goes beyond the container\nSee attachecd screenshot.", "id": "2614"}, "2619": {"ground_truth": "0", "bug_report": "Wrong info on Services > Summary tab for DataNodes Live  TaskTrackers Live  RegionServers live\n1. Install cluster2. On the host with SNameNode  we also have a region server3. Stop snamenode component on that host4. Services &gt; hbase &gt; summary shows region server is not liveAlso after stopping DataNode or TaskTracker  component status changes are not reflected on Services &gt; summary tab", "id": "2619"}, "2631": {"ground_truth": "0", "bug_report": "Host cleanup left two packages(ambari-log4j  libconfuse)\nHost cleanup left two packages(ambari-log4j  libconfuse)", "id": "2631"}, "2632": {"ground_truth": "0", "bug_report": "Dashboard Widgets: 'hover to show details' experience is jarring\nDashboard Widgets: 'hover to show details' experience is jarring", "id": "2632"}, "2633": {"ground_truth": "0", "bug_report": "Reset the latest stack version for 1.2.5\nReset the latest stack version for 1.2.5", "id": "2633"}, "2635": {"ground_truth": "0", "bug_report": "Perf: Service summary view inefficiently binds to host components\nIn ambari-web/app/views/main/service/info/summary.js#hostComponentsUpd()  is called per each hostComponent's host and master property change. On a 150 node cluster  we get like 300 calls just for this method.Due to this  service_mapper  which usually maps in 600ms  takes now 5.8s.", "id": "2635"}, "2636": {"ground_truth": "0", "bug_report": "Dashboard Metrics legend size increased unexpectedly on mouseover from line space\nThis happened in a very specific situation.1. Put mouse on the line space around a metric widget.2. hover on the widget with mouse down.Result:The legend show up as a strange bigger size.", "id": "2636"}, "2637": {"ground_truth": "1", "bug_report": "Security CSV cleanup\nShould say 'Hive Metastore and HiveServer2'  not just HiveServer2. Even though they are co-located master components  let's make it clear this principal is for both include keytab file column. In addition to the keytab full path column (/etc/security/keytabs/jt.service.keytab)  include a column with just the filename (jt.service.keytab). Easier to copy/paste/parse if you want to use the CSV file.", "id": "2637"}, "2640": {"ground_truth": "0", "bug_report": "Going back to Customize Services page from the Install page resets certain directory values\nSteps to reproduce: Install using non-default directories Upon install failure  go back to Customize Services page from the left nav. Certain directories (NN dirs  SNN dir  DN dirs  Oozie Data Dir  ZK Dir  etc) are reverted back to the default. Other parameters are not reverted back.", "id": "2640"}, "2642": {"ground_truth": "0", "bug_report": "Update Ember-I18n\nUpdate Ember-I18n", "id": "2642"}, "2643": {"ground_truth": "0", "bug_report": "Read timeout issues in Oracle JDBC connections where read has a long timeout\nRead timeout issues in Oracle JDBC connections where read has a long timeout. This happens when the read timeout is too long. In order to set appropriate timeout  add specially prefixed values in ambari.properties", "id": "2643"}, "2644": {"ground_truth": "0", "bug_report": "Ambari-server can not find password for remote database with password encryption enabled\nPerformed cluster setup as proposed at E2E test scenario. ambari-server setupambari-server setup-ldapambari-server encrypt-passwordsambari-server setup-httpsambari-server startServer does not start. It complains about missing password file / db password alias19:03:36 249 INFO Configuration:300 - Generation of file with password19:03:37 320 INFO CredentialProvider:146 - action =&gt; PUT  alias =&gt; ambari.db.password19:03:37 885 INFO Configuration:313 - Reading password from existing file19:03:38 838 INFO CredentialProvider:146 - action =&gt; PUT  alias =&gt; ambari.ldap.manager.password19:12:02 925 INFO Configuration:313 - Reading password from existing file19:12:02 946 INFO Configuration:324 - API SSL Authentication is turned on.19:12:02 946 INFO Configuration:329 - Reading password from existing file19:12:02 948 INFO Configuration:481 - Hosts Mapping File null19:12:02 951 INFO HostsMap:60 - Using hostsmap file null19:12:04 467 INFO MasterKeyServiceImpl:209 - Loading from persistent master: #1.0# Fri  Jul 12 2013 19:03:34.71719:12:06 016 INFO AmbariServer:446 - Getting the controller19:12:11 146 INFO CertificateManager:68 - Initialization of root certificate19:12:11 147 INFO CertificateManager:70 - Certificate exists:false19:12:11 147 INFO CertificateManager:137 - Generation of server certificate19:12:16 383 INFO ShellCommandUtil:43 - Command openssl genrsa -des3 -passout pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -out /var/lib/ambari-server/keys/ca.key 4096 was finished with exit code: 0 - the operation was completely successfully.19:12:16 431 INFO ShellCommandUtil:43 - Command openssl req -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -new -key /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.crt -batch was finished with exit code: 0 - the operation was completely successfully.19:12:16 483 INFO ShellCommandUtil:43 - Command openssl x509 -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -req -days 365 -in /var/lib/ambari-server/keys/ca.crt -signkey /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.crt was finished with exit code: 0 - the operation was completely successfully.19:12:16 496 INFO ShellCommandUtil:43 - Command openssl pkcs12 -export -in /var/lib/ambari-server/keys/ca.crt -inkey /var/lib/ambari-server/keys/ca.key -certfile /var/lib/ambari-server/keys/ca.crt -out /var/lib/ambari-server/keys/keystore.p12 -password pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG was finished with exit code: 0 - the operation was completely successfully.19:12:16 883 INFO AmbariServer:123 - ********* Meta Info initialized **********19:12:16 896 INFO ClustersImpl:88 - Initializing the ClustersImpl19:12:17 115 ERROR Configuration:610 - Error reading from credential store.19:12:17 116 ERROR Configuration:616 - Cannot read password for alias = /etc/ambari-server/conf/password.dat19:12:17 117 ERROR AmbariServer:455 - Failed to run the Ambari Serverjava.lang.RuntimeException: Unable to read database password at org.apache.ambari.server.configuration.Configuration.readPasswordFromFile(Configuration.java:596) at org.apache.ambari.server.configuration.Configuration.getRcaDatabasePassword(Configuration.java:583) at org.apache.ambari.eventdb.webservice.WorkflowJsonService.setDBProperties(WorkflowJsonService.java:95) at org.apache.ambari.server.controller.AmbariServer.performStaticInjection(AmbariServer.java:437) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:125) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:452)Caused by: java.io.FileNotFoundException: File '/etc/ambari-server/conf/password.dat' does not exist at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:265) at org.apache.commons.io.FileUtils.readFileToString(FileUtils.java:1457) at org.apache.commons.io.FileUtils.readFileToString(FileUtils.java:1475) at org.apache.ambari.server.configuration.Configuration.readPasswordFromFile(Configuration.java:594) ... 5 more19:12:17 118 ERROR AmbariServer:420 - Error stopping the serverjava.lang.NullPointerException at org.apache.ambari.server.controller.AmbariServer.stop(AmbariServer.java:418) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:457)Content of ambari.properties:server.jdbc.rca.driver=oracle.jdbc.driver.OracleDriverauthentication.ldap.managerDn=uid=hdfs ou=people ou=dev dc=apache dc=orgauthentication.ldap.primaryUrl=localhost:389server.jdbc.rca.url=jdbc:oracle:thin:@ip-10-34-79-165.ec2.internal:1521/XEserver.connection.max.idle.millis=900000server.jdbc.port=1521server.version.file=/var/lib/ambari-server/resources/versionserver.jdbc.rca.user.passwd=/etc/ambari-server/conf/password.datapi.authenticate=truejce_policy.url=http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-6.zipserver.persistence.type=remoteclient.api.ssl.key_name=https.keyauthentication.ldap.useSSL=falseambari-server.user=ambar-serverclient.api.ssl.port=8443authentication.ldap.usernameAttribute=uidserver.jdbc.user.name=ambariserver.jdbc.schema=XEjava.home=/usr/jdk64/jdk1.6.0_31server.os_type=redhat6api.ssl=truebootstrap.script=/usr/lib/python2.6/site-packages/ambari_server/bootstrap.pyclient.api.ssl.cert_name=https.crtauthentication.ldap.bindAnonymously=falseclient.security=ldapserver.jdbc.hostname=ip-10-34-79-165.ec2.internalresources.dir=/var/lib/ambari-server/resourcessecurity.passwords.encryption.enabled=truebootstrap.setup_agent.script=/usr/lib/python2.6/site-packages/ambari_server/setupAgent.pyserver.jdbc.driver=oracle.jdbc.driver.OracleDriverjdk.url=http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-6u31-linux-x64.binsecurity.server.keys_dir=/var/lib/ambari-server/keysserver.jdbc.rca.user.name=ambariwebapp.dir=/usr/lib/ambari-server/webmetadata.path=/var/lib/ambari-server/resources/stacksserver.jdbc.url=jdbc:oracle:thin:@ip-10-34-79-165.ec2.internal:1521/XEserver.fqdn.service.url=http://169.254.169.254/latest/meta-data/public-hostnamebootstrap.dir=/var/run/ambari-server/bootstrapauthentication.ldap.baseDn=dc=apache dc=orgserver.jdbc.user.passwd=${alias=ambari.db.password}authentication.ldap.managerPassword=${alias=ambari.ldap.manager.password}server.jdbc.database=oraclesecurity.server.two_way_ssl=trueFile /etc/ambari-server/conf/password.dat is missingSetup flow:[root@ip-10-116-65-200 kerb]# ambari-server setupUsing python /usr/bin/python2.6Initializing...Setup ambari-serverChecking SELinux...SELinux status is 'enabled'SELinux mode is 'enforcing'Temporarily disabling SELinuxWARNING: SELinux is set to 'permissive' mode and temporarily disabled.OK to continue [y/n] (y)? yCustomize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):ambar-serverAdjusting ambari-server permissions and ownership...Checking iptables...iptables is disabled now. please reenable later.Checking JDK...Downloading JDK from http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-6u31-linux-x64.bin to /var/lib/ambari-server/resources/jdk-6u31-linux-x64.binJDK distribution size is 85581913 bytesjdk-6u31-linux-x64.bin... 100% (81.6 MB of 81.6 MB)Successfully downloaded JDK distribution to /var/lib/ambari-server/resources/jdk-6u31-linux-x64.binTo install the Oracle JDK you must accept the license terms found at http://www.oracle.com/technetwork/java/javase/downloads/jdk-6u21-license-159167.txt. Not accepting will cancel the Ambari Server setup.Do you accept the Oracle Binary Code License Agreement [y/n] (y)? Installing JDK to /usr/jdk64Successfully installed JDK to /usr/jdk64/jdk1.6.0_31Downloading JCE Policy archive from http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-6.zip to /var/lib/ambari-server/resources/jce_policy-6.zipSuccessfully downloaded JCE Policy archive to /var/lib/ambari-server/resources/jce_policy-6.zipCompleting setup...Configuring database...Enter advanced database configuration [y/n] (n)? ySelect database:1 - PostgreSQL (Embedded)2 - Oracle[1]:2Hostname [localhost]:ip-10-34-79-165.ec2.internalPort [1521]:Select Oracle identifier type:1 - Service Name2 - SID[1]:XEInvalid number.Select Oracle identifier type:1 - Service Name2 - SID[1]:1Service Name [ambari]:XEUsername [ambari]: Enter Database Password [bigdata]: WARNING: Before starting Ambari Server  you must copy the Oracle JDBC driver JAR file to /usr/share/java.Press &lt;enter&gt; to continue.Copying JDBC drivers to server resources...Configuring remote database connection properties...WARNING: Cannot find oracle sqlplus client in the path to load the Ambari Server schema. Before starting Ambari Server  you must run the following DDL against the database to create the schema sqlplus ambari/bigdata &lt; /var/lib/ambari-server/resources/Ambari-DDL-Oracle-CREATE.sql Press &lt;enter&gt; to continue.WARNING: The cli was not foundAmbari Server 'setup' completed with warnings.[root@ip-10-116-65-200 kerb]# less /etc/passwd", "id": "2644"}, "2646": {"ground_truth": "0", "bug_report": "Improve styles for HostCleanup code area\nImprove styles for HostCleanup code area", "id": "2646"}, "2653": {"ground_truth": "0", "bug_report": "Add umask checks for host checks - we should alert if umask is not 022.\nAdd umask checks for host checks - we should alert if umask is not 022.", "id": "2653"}, "2657": {"ground_truth": "0", "bug_report": "Add a re-type new password field when changing passwords for ambari users\nWe should add a third text box to re-type the new password while changing the password for an ambari user.", "id": "2657"}, "2660": {"ground_truth": "0", "bug_report": "Host checks say pass when all hosts failed to register\nWhen all hosts fail to register  there are no warnings  and hence we show OK for host checks.", "id": "2660"}, "2661": {"ground_truth": "0", "bug_report": "Security wizard: Relogin while on step3 without quitting the wizard throws JS error.\nSteps to reproduce: Go to step-3 (Generate principals and keytabs) of Enable security wizard. Restart Amabri server. Refresh on step-3. ui will take you to login page. Entering correct credentials  user will be navigated again to step-3 of security wizard. At this point JS error is encountered.", "id": "2661"}, "2670": {"ground_truth": "0", "bug_report": "Start button not available for various components within 10 sec after stop operation finishes\nSteps to reproduce: 1. Stop a component. 2. Wait for the corresponding BG operation to finish. Result: 'Start' button isn't available for the component within 10 seconds since stop operation finished in UI. It appears later.This happens cuz of update interval  it is set to 15 seconds (App.contentUpdateInterval)  that's why in some moments it can take up to 15 sec to update components status  after request is done.Solution: Create a seperate update interval specialy for updating host components. In config.js we even have App.componentsUpdateInterval = 6000; But this value was not used anywhere in code till now.", "id": "2670"}, "2681": {"ground_truth": "0", "bug_report": "setup ldap does not validate secondary url\nsetup ldap does not validate secondary url. It should validate the input (when entered) the same way as the primary.", "id": "2681"}, "2688": {"ground_truth": "0", "bug_report": "Error messages printed to log\nNotice in the ambari-server snippet below a lot of these messages:ERROR Configuration:616 - Cannot read password for alias = nullSteps to reproduce:Setup serverSetup encrypt passwords  don't persist the keySetup httpsStart the server  provided master keyDo cluster install", "id": "2688"}, "2689": {"ground_truth": "0", "bug_report": "Enable Security Wizard stops on step '2. Save Configurations' and doesn't let the user leave the wizard\nEnable Security Wizard stops on step '2. Save Configurations' and doesn't let the user leave the wizard", "id": "2689"}, "2690": {"ground_truth": "0", "bug_report": "Datanode Live widget displays 0 dead when no datanode is live on a cluster.\nDatanode Live widget displays 0 dead when no datanode is live on a cluster.", "id": "2690"}, "2692": {"ground_truth": "0", "bug_report": "Disable Show report for 0 issues\nReport on Host Checks for 0 isses looks like:####################################### Host Checks Report## Generated: Tue Jul 09 2013 13:11:14 GMT+0300 (FLE Daylight Time)############################################################################# Hosts## A space delimited list of hosts which have issues.# Provided so that administrators can easily copy hostnames into scripts  email etc.######################################HOSTSShow Report button should be disabled for 0 issues or at least do not show HOSTS section in report.", "id": "2692"}, "2697": {"ground_truth": "1", "bug_report": "Disable security not working in web-ui testMode.\nDisable security not working in web-ui testMode.", "id": "2697"}, "2698": {"ground_truth": "0", "bug_report": "Host specific progress bar for a task has some inconsistencies\nSee the attached images. The third image added is a summary of what was going on.", "id": "2698"}, "2701": {"ground_truth": "0", "bug_report": "Implement a cleanup thread that removes files in ambari-agent data directory that are older than a configurable amount of time\nImplement a cleanup thread that removes files in ambari-agent data directory that are older than a month or so(must be configurable).It's required  because the directory will grow unbounded if it's not cleaned up.", "id": "2701"}, "2707": {"ground_truth": "0", "bug_report": "Fix JS Unit tests after merge 1.4.0 to trunk\nFix JS Unit tests after merge 1.4.0 to trunk", "id": "2707"}, "2708": {"ground_truth": "0", "bug_report": "Make ambari web testMode work for installer wizard with HDP stack-2 selection.\nInstaller wizard with HDP stack-2 selection in test mode", "id": "2708"}, "2716": {"ground_truth": "0", "bug_report": "Disable autocomplete on form tag for Ambari UI.\nDisable autocomplete on form tag for Ambari UI.", "id": "2716"}, "2723": {"ground_truth": "0", "bug_report": "hbase super user cannot submit jobs since Ambari creates hbase super user with uid<1000\nCopytable jobs need to be submitted as the hbase super user however the uid for hbase super user created by Ambari has uid &lt; 1000", "id": "2723"}, "2727": {"ground_truth": "0", "bug_report": "Disallow actions upon host components on hosts that stopped heartbeating\nOn a host that stopped heartbeating  the UI shows actions to perform on host components. However  upon executing an action  the backend does not create any tasks and returns 200. The UI doesn't do anything in this case. Instead  the UI should disable action buttons in this case.", "id": "2727"}, "2729": {"ground_truth": "0", "bug_report": "While a host component is being installed (INSTALLING state)  it does not show up in the Host Detail page\nSay Add Hosts wizard fails to add a host and the host components are in INSTALL_FAILED state. In this case  the UI displays the host component with a red gear and shows the action menu with the current state 'Install Failed' and the action 'Re-Install'. Once you invoke 'Re-Install'  the host component disappears from the UI. Once the host component finishes installing  it magically appears again.", "id": "2729"}, "2733": {"ground_truth": "0", "bug_report": "Hosts and Host Details page UI tweaks\n1. Hosts &gt; Design around full hostname being displayed. Consider being able to show 40 characters and then truncate &gt; 40 chars 2. Hosts &gt; Show # control should persist when navigating around app  and after logout/login3. Hosts &gt; Make Components list an expand/collapse control instead of a list with abbreviations4. Host Details &gt; Move Components area above Summary area since the Component Controls are used very often (much more often than viewing the Summary area info).", "id": "2733"}, "2748": {"ground_truth": "0", "bug_report": "Misc logging changes\nAdditional logs:  When a task is timed-out/failed API requests to update component and component hosts   Remove logs  Do not log ganglia population time when its less than 5 second", "id": "2748"}, "2753": {"ground_truth": "0", "bug_report": "Security Wizard step 4: no hosts shown when clicking on the 'Start Services'/'Stop Services' link\nProceed to step 4 of security wizard  click on 'Start Services' or 'Stop Services' link.Result: popup window is shown with empty contentThis bug was happening due to js error 'Uncaught TypeError: Cannot call method 'filterProperty' of null' in setBackgroundOperationHeader: functionAfter background operation (host popup) popup was optimized for better peformace on large cluster this error showed up. New function on setting popup header  did not account that this popup (HostPopup) is used not only in BG operations  but also in security wizard.", "id": "2753"}, "2758": {"ground_truth": "0", "bug_report": "Jobs page: table is not striped and pagination is not disabled\nTo reproduce go to the Jobs page from top menu (do not open .../main/apps directly). Table is not striped and pagination buttons are enabled even if there is only one page.", "id": "2758"}, "2760": {"ground_truth": "0", "bug_report": "Stack 2.0.3  Hive Check execute fail\nWhen installing Hadoop 2 stack  Hive service check fails to run.", "id": "2760"}, "2761": {"ground_truth": "0", "bug_report": "Customize Services page - Misc tab: incorrect behavior of popup window for changing user names\nIn firefoxSteps:Go to 'Customize Services' page.Select 'Misc' tab.Change username for HDFS  HBase or Group (do not move focus to other elements).Click 'Next' button.Result:Browser was switched to 'Review' page.Was opened popup window for changing properties depended with user names. User must to refresh the page for popup menu disappearing.", "id": "2761"}, "2763": {"ground_truth": "0", "bug_report": "Ozzie does not work with local FS user\nRunning jobs as a local FS user does not work work 2.0.* stack because of permissions on /tmp/hadoop-yarn/staging  which is the default staging dir.", "id": "2763"}, "2768": {"ground_truth": "0", "bug_report": "Host Checks > Show Report is showing bogus information for FILES AND FOLDERS\nHost Checks popup is showing that /usr/lib/hadoop already exists.When I clicked on 'Show Reports'  it is showing '/usr/lib/hadoop/ /folder'. This doesn't make much sense. FILES AND FOLDERS/usr/lib/hadoop/ /folderThe API is showing: 'stackFoldersAndFiles' : [ { 'name' : '/usr/lib/hadoop'  'type' : 'directory' }", "id": "2768"}, "2769": {"ground_truth": "1", "bug_report": "FE should show the error-details when it encounters error while persisting web client state\nError which occurred while requesting cluster status is not informative.Change url in request for cluster state  go to admin page and try to enable security.", "id": "2769"}, "2777": {"ground_truth": "0", "bug_report": "Cannot save HDFS configs with SNN in MAINTENANCE mode\nCannot save HDFS configs with SNN in MAINTENANCE mode", "id": "2777"}, "2782": {"ground_truth": "0", "bug_report": "Hadoop2 stack install should merge YARN MR2 options\nYARN in current Hadoop 2 stack has only MR2 as application. Since it does not make sense to install YARN without a default application  and MR2 cannot be installed by itself  we should combine both into a single install option (see screenshot).", "id": "2782"}, "2786": {"ground_truth": "0", "bug_report": "YARN time series data needed for NodeManager statuses\nWe need API call for a graph which will show NodeManager status counts. NodeManagers can be in the following states: active  lost  unhealthy  rebooted  and decommissioned.", "id": "2786"}, "2799": {"ground_truth": "0", "bug_report": "YARN service summary additional information\nWe need to show below 2 additional information Across cluster memory - used/reserved/total Queues information if available", "id": "2799"}, "2808": {"ground_truth": "0", "bug_report": "Can't add from UI some queues in capacity-scheduler.xml\nProperties of capacity-scheduler.xml are truncated by ' '. It make impossible to create multiple queues  please see http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html", "id": "2808"}, "2816": {"ground_truth": "0", "bug_report": "Customize Services: directories are shown in comma-delimited format when revisiting\nIn the Install Wizard  Customize Services page shows multiple directories delimited by newlines.However  when revisiting the page (go back to Customize Services from the Review page  for example)  the directories are shown in comma-delimited format. We should always show the directories in newline-delimited format. Note that when we actually store the value  the directories are comma-delimited (which is correct).", "id": "2816"}, "2829": {"ground_truth": "0", "bug_report": "Dashboard refactor and Unit tests\nDashboard refactor and Unit tests", "id": "2829"}, "2834": {"ground_truth": "0", "bug_report": "Utility script to generate keytabs is broken\nkeytab Tar for each host is packaged including hostname. Untaring it on a host creates path starting with &lt;hostanme&gt;/&lt;actual path&gt;. Fix is to package the content inside the hostname directory excluding the hostname directory itself.", "id": "2834"}, "2836": {"ground_truth": "0", "bug_report": "HBase 0.95.2 - Logger doesn't work\nInstalling 0.95.2 from internal repo 2.0.5. hbase-daemon.sh defines a default logger of INFO RFA{{  when log4j.properties uses {{INFO DRFA. They should match  as startup generates an error and does not continue.We should stop over writing the log4j properties for hbase. This will allow for hbase log4j properties to be in sync with those that come with the rpms.", "id": "2836"}, "2838": {"ground_truth": "0", "bug_report": "Running Requests are not visibile on the UI since the API is not returning the running requests.\nThe issue is the following:getRequestsByTaskStatus behaves correctly and returns the latest N requests. Note that the N requests have M (M &gt;&gt; N) tasks.Then the call to findByRequestIds gets oldest N tasks where the requestId for tasks belong to the list returned by the first call. So instead of getting M tasks we only get N tasks that too N oldest tasks which are returned. As a result the call never returns that latest request/tasks. The fix is to drop the filter done by the calls findByRequestIds and findByRequestAndTaskIds. Filter should only be applied on the number of requests to be returned.", "id": "2838"}, "2840": {"ground_truth": "0", "bug_report": "YARN and ZK data directory names have ' ' at end\nInstalled the Hadoop2 stack and upon finishing the zk_data_dir in global  and yarn.nodemanager.local-dirs in yarn-site have the folder names suffixed with ' ' in API. Consequently  the folder names on system end up with a ' ' at end. Ex: /hadoop/yarn  and /hadoop/zookeeper .", "id": "2840"}, "2847": {"ground_truth": "0", "bug_report": "Restart service component fails if pid is reallocated\n1. Stop secondary namenode.2. Edit the pid file  default location = /var/run/hadoop/hdfs/hadoop-hdfs-secondarynamenode.pid3. Change the pid to any other process pid that is currently running.4. Start secondary namenode.Outcome:Secondary namenode start command succeeds but secondary namenode does not start. (indicated by live status of the component).", "id": "2847"}, "2858": {"ground_truth": "0", "bug_report": "YARN time series data needed for AllocatedContainers\nAPI call for a graph which will show time series for YARN Allocated containers.", "id": "2858"}, "2864": {"ground_truth": "0", "bug_report": "Host registration fails\nHost registration fails with:INFO 2013-08-10 01:50:49 923 Controller.py:99 - Unable to connect to: https://c6401.ambari.apache.org:8441/agent/v1/register/c6403.ambari.apache.orgTraceback (most recent call last): File '/usr/lib/python2.6/site-packages/ambari_agent/Controller.py'  line 80  in registerWithServer ret = json.loads(response) File '/usr/lib64/python2.6/json/__init__.py'  line 307  in loads return _default_decoder.decode(s) File '/usr/lib64/python2.6/json/decoder.py'  line 319  in decode obj  end = self.raw_decode(s  idx=_w(s  0).end()) File '/usr/lib64/python2.6/json/decoder.py'  line 338  in raw_decode raise ValueError('No JSON object could be decoded')ValueError: No JSON object could be decodedambari-server.log is showing:01:55:22 732 WARN [qtp967966535-50] ServletHandler:514 - /agent/v1/register/c6401.ambari.apache.orgcom.google.gson.JsonSyntaxException: java.lang.IllegalStateException: Expected a string but was BEGIN_OBJECT at line 1 column 3680 at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:176) at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:40) at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.Gson.fromJson(Gson.java:795) at com.google.gson.Gson.fromJson(Gson.java:761) at org.apache.ambari.server.api.GsonJsonProvider.readFrom(GsonJsonProvider.java:60) at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474) at com.sun.jersey.server.impl.model.method.dispatch.EntityParamDispatchProvider$EntityInjectable.getValue(EntityParamDispatchProvider.java:123) at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:46) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:183) at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)", "id": "2864"}, "2865": {"ground_truth": "0", "bug_report": "Nagios server fails to start with invalid configuration error\nNagios fails to start with invalid configuration error. (This is likely intermittent).Error: Configuration validation failed - when Nagios is started./usr/sbin/nagios -v /etc/nagios/nagios.cfgCopyright (c) 1999-2009 Ethan GalstadLast Modified: 03-15-2013License: GPLWebsite: http://www.nagios.orgReading configuration data... Read main config file okay...Processing object config file '/etc/nagios/objects/commands.cfg'...Processing object config file '/etc/nagios/objects/contacts.cfg'...Processing object config file '/etc/nagios/objects/timeperiods.cfg'...Processing object config file '/etc/nagios/objects/templates.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hosts.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hostgroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-servicegroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-services.cfg'...Processing object config file '/etc/nagios/objects/hadoop-commands.cfg'...Error: Could not find any hostgroup matching 'resourcemanager' (config file '/etc/nagios/objects/hadoop-services.cfg'  starting on line 292) Error processing object config files!***&gt; One or more problems was encountered while processing the config files... Check your configuration file(s) to ensure that they contain valid directives and data defintions. If you are upgrading from a previous version of Nagios  you should be aware that some variables/definitions may have been removed or modified in this version. Make sure to read the HTML documentation regarding the config files  as well as the 'Whats New' section to find out what has changed.", "id": "2865"}, "2866": {"ground_truth": "0", "bug_report": "API JMX mapping needs to be updated due to property name changes\nIn Ambari UI  we show properties like NameNode RPC Time.We used to get this metric by querying the NameNode component for 'RpcQueueTime_avg_time'.However  in Hadoop 2  it looks like this property name changed to 'RpcQueueTimeAvgTime'  so the Ambari API no longer contains these metrics. Other properties related to NameNode RPC may have changed. We need to update the mapping accordingly.", "id": "2866"}, "2871": {"ground_truth": "0", "bug_report": "Nagios start fails due to invalid configs\nSteps to reproduce:1) Deploy HDP-2.0.5 cluster with Nagios.2) Start of Nagios failed  puppet log:notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Enable_snmp/Exec[enable_snmp]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-hostgroups.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-hostgroups.cfg]/File[/etc/nagios/objects/hadoop-hostgroups.cfg]/content: content changed '{md5}873d2be7b9f78137e0740223944d93af' to '{md5}780117e3c2407e9d02b37eab93159149'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[nagios]/Hdp::Configfile[/etc/init.d//nagios]/File[/etc/init.d//nagios]/content: content changed '{md5}3990694abc37617c79e2ea5276d71089' to '{md5}c4c4454911c0c6c1ba29d9d0dc2aa28c'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-hosts.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-hosts.cfg]/File[/etc/nagios/objects/hadoop-hosts.cfg]/content: content changed '{md5}7979396ff0b495e40901acdd2ecc457c' to '{md5}fa5ec3a93a4827cc6a691e0b8e22b4f6'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-services.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-services.cfg]/File[/etc/nagios/objects/hadoop-services.cfg]/content: content changed '{md5}06c9d0bb0aa3b1e7b30b19fb1bb30b5a' to '{md5}934241156b5489483dab8018f9cecd22'notice: /Stage[2]/Hdp-nagios::Server::Web_permisssions/Hdp::Exec[htpasswd -c -b /etc/nagios/htpasswd.users nagiosadmin p]/Exec[htpasswd -c -b /etc/nagios/htpasswd.users nagiosadmin p]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Web_permisssions/Hdp::Exec[apache_permissions_htpasswd.users]/Exec[apache_permissions_htpasswd.users]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: nagios is stoppednotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: Configuration validation failed[FAILED]err: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: change from notrun to 0 failed: service nagios start returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-nagios/manifests/server.pp:284notice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: nagios is stoppednotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: Configuration validation failed[FAILED]err: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]: Failed to call refresh: service nagios start returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-nagios/manifests/server.pp:284notice: /Stage[2]/Hdp-nagios::Server::Services/Anchor[hdp-nagios::server::services::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-nagios::Server::Services/Anchor[hdp-nagios::server::services::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Package[httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Package[httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /tmp/HDP-artifacts/ ; curl -kf --retry 10 http://dev01.hortonworks.com:8080/resources//jdk-6u31-linux-x64.bin -o /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /tmp/HDP-artifacts/ ; curl -kf --retry 10 http://dev01.hortonworks.com:8080/resources//jdk-6u31-linux-x64.bin -o /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /usr/jdk ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin; cd /usr/jdk ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin -noregister &gt; /dev/null 2&gt;&amp;1 httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /usr/jdk ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin; cd /usr/jdk ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin -noregister &gt; /dev/null 2&gt;&amp;1 httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/File[/usr/jdk/jdk1.6.0_31/bin/java httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/File[/usr/jdk/jdk1.6.0_31/bin/java httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Exec[monitor webserver restart]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Exec[monitor webserver restart]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::end]: Skipping because of failed dependencies3) Run checkconfig:[root@dev02 ~]# /usr/sbin/nagios -v /etc/nagios/nagios.cfg Nagios Core 3.5.0Copyright (c) 2009-2011 Nagios Core Development Team and Community ContributorsCopyright (c) 1999-2009 Ethan GalstadLast Modified: 03-15-2013License: GPLWebsite: http://www.nagios.orgReading configuration data... Read main config file okay...Processing object config file '/etc/nagios/objects/commands.cfg'...Processing object config file '/etc/nagios/objects/contacts.cfg'...Processing object config file '/etc/nagios/objects/timeperiods.cfg'...Processing object config file '/etc/nagios/objects/templates.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hosts.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hostgroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-servicegroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-services.cfg'...Processing object config file '/etc/nagios/objects/hadoop-commands.cfg'...Error: Could not find any servicegroup matching 'MAPREDUCE' (config file '/etc/nagios/objects/hadoop-services.cfg'  starting on line 67) Error processing object config files!***&gt; One or more problems was encountered while processing the config files... Check your configuration file(s) to ensure that they contain valid directives and data defintions. If you are upgrading from a previous version of Nagios  you should be aware that some variables/definitions may have been removed or modified in this version. Make sure to read the HTML documentation regarding the config files  as well as the 'Whats New' section to find out what has changed.[root@dev02 ~]# It seems we have invalid condition for generation of MAPREDUCE Nagios checks.", "id": "2871"}, "2876": {"ground_truth": "0", "bug_report": "Puppet script syntax issues result in hive deployment with custom DB and oozie service check failures\nFew puppet variables are missing in jdbc-connector and oozie service-check puppet scripts.", "id": "2876"}, "2879": {"ground_truth": "0", "bug_report": "Oozie failed at smoke test in secured cluster\nstderr:None stdout:notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[1]/Hdp::Snmp/Hdp::Package[snmp]/Hdp::Package::Process_pkg[snmp]/Hdp::Java::Package[snmp]/Hdp::Java::Jce::Package[snmp]/Exec[jce-install snmp]/returns: executed successfullynotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Moved to trash: hdfs://domU-12-31-39-07-D5-91.compute-1.internal:8020/user/ambari-qa/examplesnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Moved to trash: hdfs://domU-12-31-39-07-D5-91.compute-1.internal:8020/user/ambari-qa/input-datanotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Error: AUTHENTICATION : Could not authenticate  GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Invalid sub-command: Missing argument for option: infonotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: use 'help [sub-command]' for help detailsnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Invalid sub-command: Missing argument for option: infonotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: use 'help [sub-command]' for help detailsnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: workflow_status=err: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: change from notrun to 0 failed: sh /tmp/oozieSmoke.sh /etc/oozie/conf /etc/hadoop/conf ambari-qa true /etc/security/keytabs/smokeuser.headless.keytab EXAMPLE.COM jt/domu-12-31-39-07-d5-91.compute-1.internal@EXAMPLE.COM nn/domu-12-31-39-07-d5-91.compute-1.internal@EXAMPLE.COM /usr/bin/kinit returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/oozie/service_check.pp:63notice: Finished catalog run in 51.05 seconds", "id": "2879"}, "2884": {"ground_truth": "0", "bug_report": "Oozie start fails - likely due to 'failed install'\nOozie start failed with following in the log: hadoop dfs -chmod -R 755 /user/oozie/share' returned 1 instead of one of 0On the node: chmod: '/user/oozie/share': No such file or directoryoozie@c6402 ~$ hadoop dfs -ls /user/ DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 3 itemsdrwxrwx--- - ambari-qa hdfs 0 2013-08-02 02:40 /user/ambari-qadrwx------ - hive hdfs 0 2013-08-02 02:41 /user/hivedrwxrwxr-x - hdfs hdfs 0 2013-08-02 02:42 /user/oozieoozie@c6402 ~$ hadoop dfs -ls /user/oozieDEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.oozie@c6402 ~$The reason  the oozie smoke is failing to run an oozie job with more than one node in the cluster  due to bad settings in /etc/hadoop/core-site.xml:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt; &lt;value&gt;host1&lt;/value&gt;&lt;/property&gt;host1  in my case is host on which oozie is not installed. After changing this to host2 (where oozie server is installed)  oozie smoke succeeded.", "id": "2884"}, "2886": {"ground_truth": "0", "bug_report": "HDFS Service Check failed (as designed) but took 10 mins to fail\nAfter I had a cluster up and running successfully  but manually turned HDFS Safe Mode ON by running:hdfs dfsadmin -safemode enter.HDFS Execute Check failed due to Puppet timeout after 10 minutes.It should have failed quicker since task timeout is 10 minutes on server.", "id": "2886"}, "2891": {"ground_truth": "0", "bug_report": "hadoop-env.sh and core-site are missing on hosts that have only yarn components deployed\nYarn component need hadoop-env.sh and core-site.xml. These files should be deployed on hosts on which only yarn components are deployed.", "id": "2891"}, "2903": {"ground_truth": "0", "bug_report": "Add HBase 96 metrics changes to jmx in a backwards compatible way.\nAdd HBase 96 metrics changes to jmx in a backwards compatible way.", "id": "2903"}, "2905": {"ground_truth": "0", "bug_report": "SNAMENODE should not start after transition to Maintenance mode\nCurrently  in HA NN cluster  starting HDFS service at services tab  tries to start SNAMENODE as well (through it is in Maintainance state because of executing curl -u admin:admin -i -X PUT -d '{'RequestInfo':{'context':'SNN maintenance'} 'Body':{'HostRoles':{'state':'MAINTENANCE'}}}' http://$SERVER:8080/api/v1/clusters/$CLUSTER/hosts/$SNN_HOST/host_components/SECONDARY_NAMENODE command )", "id": "2905"}, "2920": {"ground_truth": "0", "bug_report": "Rename alert titles and descriptions\nCurrently Nagios alerts are shown in Ambari UI like so: (green check) NameNode process down &lt;- means NameNode process is up (red X) NameNode process down &lt;- means NameNode process is down (green check) Percent DataNode down &lt;- means % of DataNodes that are up is above the threshold (red X) Percent DataNode down &lt;- means % of DataNodes that are up is below the threshold (green check) Nagios status log staleness &lt;- means Nagios status log is fresh (red X) Nagios status log staleness &lt;- means Nagios status log is staleWhen a user sees the word 'down' with a positive indication (green check) for it  it's confusing. It's like saying 'this is red' in green... is it green or red?The proposal here is to rename these alerts  like so: (green check) NameNode process &lt;- means NameNode process is up/healthy (red X) NameNode process &lt;- means NameNode process is down/unhealthy (green check) Percent DataNodes live &lt;- means % of DataNodes that are up/healthy is above the threshold (red X) Percent DataNodes live &lt;- means % of DataNodes that are up/healthy is below the threshold (green check) Nagios status log freshness &lt;- means Nagios status log is fresh (red X) Nagios status log freshness &lt;- means Nagios status log is staleAlso there are inconsistencies in the way we show component names in alert titles and descriptions (like 'templeton server status' to mean 'WebHCat Server status'  etc). These need to be fixed.", "id": "2920"}, "2927": {"ground_truth": "0", "bug_report": "ResourceManager's RPC and NodeManager counts missing from time-series\nResourceManager's rpc.rpc.RpcQueueTimeAvgTime and yarn.ClusterMetrics.NumActiveNMs are not being provided accurately due to changes introduced in AMBARI-2910 to YARN configuration to collect more metrics.We need to narrow down the scope of metric collection to keep getting these previous metrics.", "id": "2927"}, "2931": {"ground_truth": "0", "bug_report": "Popover stuck after routing to another page\nSteps to reporduce:1. Go to Installer-&gt;Welcome step2. Focus on cluster name textfield3. Hover on 'learn more' label4. Press Enter to route to the next pageResult:Popover remains visible on other pages", "id": "2931"}, "2938": {"ground_truth": "0", "bug_report": "Update stack definition for MAPREDUCE2\nUpdate stack definition for MAPREDUCE2 and the default site xml files.", "id": "2938"}, "2939": {"ground_truth": "0", "bug_report": "Update 1.3.2 stack definition for repo url\nUpdate 1.3.2 stack definition for repo url", "id": "2939"}, "2943": {"ground_truth": "0", "bug_report": "Oozie smoke tests fail on Ambari with NPE in Oozie Server.\nOozie smoke tests fail on Ambari with NPE in Oozie Server.", "id": "2943"}, "2947": {"ground_truth": "0", "bug_report": "Use a specific build number for the stck builds.\nUse a specific build number for the stck builds.", "id": "2947"}, "2948": {"ground_truth": "0", "bug_report": "Mapreduce pid directory cutomization fails\nChanging mapreduce log directory prefix results in Live status showing history server not started.", "id": "2948"}, "2972": {"ground_truth": "1", "bug_report": "Provide read-only view of security wizard entries\nAfter enabling security  when browsing back to Admin &gt; Security  it would be good to see the tabs with the values entered during wizard setup (read-only)  just so it's easy to see what the user specifically entered during the wizard setup.", "id": "2972"}, "2973": {"ground_truth": "0", "bug_report": "Ambari server and agent are not stopped during package uninstall\nWhen ambari-agent and ambari-server packages are uninstalled  uninstall scriplets don't stop running services. That's why processes remain running even after package removal. That may cause issues during upgrade (if administrator misses the 'stop services' step)[root@host01 ~]$ ambari-server statusUsing python /usr/bin/python2.6Ambari-server statusAmbari Server runningFound Ambari Server PID: '7850 at: /var/run/ambari-server/ambari-server.pid[root@host01]# ambari-agent statusFound ambari-agent PID: 3521ambari-agent running.Agent PID at: /var/run/ambari-agent/ambari-agent.pidAgent out at: /var/log/ambari-agent/ambari-agent.outAgent log at: /var/log/ambari-agent/ambari-agent.log[root@host01]# rpm -e ambari-server[root@host01]# rpm -e ambari-agent[root@host01]# ps aux | grep ambari | grep -v grep | grep -v postgresroot 7850 2.1 13.2 3031480 254212 ? Sl 20:51 0:36 /usr/jdk64/jdk1.6.0_31/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Xms512m -Xmx2048m -cp /etc/ambari-server/conf:/usr/lib/ambari-server/*:/sbin:/bin:/usr/sbin:/usr/bin:/usr/lib/ambari-server/* org.apache.ambari.server.controller.AmbariServerroot 3521 0.7 1.1 499760 22544 ? Sl 20:53 0:12 /usr/bin/python2.6 /usr/lib/python2.6/site-packages/ambari_agent/main.py start restart --expected-hostname=host01", "id": "2973"}, "2986": {"ground_truth": "0", "bug_report": "Should turn on predicate pushdown by default.\nShould turn on predicate pushdown by default.", "id": "2986"}, "3009": {"ground_truth": "0", "bug_report": "Trim and/or validate config parameter values to prevent failures due to extra spaces\nAutomatically trim whitespaces (both leading and trailing) for: Database Host in Oozie and Hive Database Name in Oozie and Hive Database URL in Oozie and Hive All directories  including log dir  pid dir data dir  etc (I believe we already trim  split  and join for the UI config type 'directories'. We should trim on the single-line directory values  if we are not doing so already).Automatically trim all trailing spaces (but not leading spaces) for all config values with the following exceptions: Password fields Values that consist of spaces only (such as ' ')", "id": "3009"}, "3019": {"ground_truth": "0", "bug_report": "Ambari should always point to latest repo\nAmbari should always point to latest repo.", "id": "3019"}, "3021": {"ground_truth": "0", "bug_report": "Customize Services page->Misc tab: Popup with related properties does not opened after 'Group User' value changing\nCustomize Services page->Misc tab: Popup with related properties does not opened after 'Group User' value changing", "id": "3021"}, "3024": {"ground_truth": "0", "bug_report": "Oozie oozie-site.xml misssing two xsd values causing shell and sla workflows to fail\nFollowing two values need to be added to oozie-site.xml in order to get sla and shell workflows to run successfully.shell-action-0.2.xsdoozie-sla-0.1.xsd oozie-sla-0.2.xsdFollowing is the property name&lt;property&gt;&lt;name&gt;oozie.service.SchemaService.wf.ext.schemas&lt;/name&gt;&lt;value&gt;shell-action-0.1.xsd email-action-0.1.xsd hive-action-0.2.xsd sqoop-action-0.2.xsd ssh-action-0.1.xsd distcp-action-0.1.xsd&lt;/value&gt;&lt;/property&gt;", "id": "3024"}, "3026": {"ground_truth": "0", "bug_report": "Ambari server setup with silent option prints error statement for the first time\ncommand: ambari-server setup -sCompleting setup...Configuring database...Enter advanced database configuration [y/n] &#40;n&#41;? ERROR: Connection properties not set in config file.Default properties detected. Using built-in database.Checking PostgreSQL...Running initdb: This may take upto a minute.About to start PostgreSQLConfiguring local database...Configuring PostgreSQL...Restarting PostgreSQLAmbari Server 'setup' completed successfully.Running ambari-server setup command again doesn't reproduce the error statement.", "id": "3026"}, "3047": {"ground_truth": "0", "bug_report": "Enhance host clean up to handle tmp files and folders\nHost cleanup should: remove files and folders in tmp folder based on what users are being cleaned remove default hadoop group", "id": "3047"}, "3056": {"ground_truth": "0", "bug_report": "Advanced config orders should be consistent in Install Wizard > Customize Services and Monitoring > Services > Config\nCompare the config parameter ordering in Install Wizard &gt; Customize Services service configs and Monitoring &gt; Services &gt; Config; they are different.We should use the same parameter ordering that we use in Install Wizard &gt; Customize Services for Monitoring &gt; Services &gt; Config.", "id": "3056"}, "3061": {"ground_truth": "0", "bug_report": "Do not use regex to determine folder name by full path for dfs_domain_socket_path\nThe point is to use custom puppet function instead of regex for line$dfs_domain_socket_path_dir = regsubst($hdp-hadoop::params::dfs_domain_socket_path  '/[^//]+$'  '').It could be implemented with ruby:File.split('/path/to/file') # =&gt; ['/path/to'  'file']", "id": "3061"}, "3068": {"ground_truth": "0", "bug_report": "Warning messages not cleared when task fails\nInstalled a cluster  namenode start fails  install exits with warnings.If i go an look at the specific task that fails  the puppet warnings are still present. Seems like those puppet warnings didn't get cleared.In a task failure case  having the warnings clear is when it's most important.", "id": "3068"}, "3069": {"ground_truth": "0", "bug_report": "Fix Unit tests\nFix Unit tests", "id": "3069"}, "3073": {"ground_truth": "0", "bug_report": "Amabri Client refactoring -2\n1) The last patch had an issue with the testcase because of which it did not compile. I might have accidently added a line while I was saving some of the files.(this is fixed in the new patch)The trunk currently fails for ambari-client.https://cwiki.apache.org/confluence/display/AMBARI/Ambari+python+Client has all the exposed methods", "id": "3073"}, "3074": {"ground_truth": "0", "bug_report": "Ambari wont start NodeManager because one of multiple folders not created\nyarn-site having:'yarn.nodemanager.local-dirs' : '/grid/0/hadoop/yarn /grid/1/hadoop/yarn /grid/2/hadoop/yarn /grid/3/hadoop/yarn /grid/4/hadoop/yarn /grid/5/hadoop/yarn' 'yarn.nodemanager.log-dirs' : '/grid/0/hadoop/yarn /grid/1/hadoop/yarn /grid/2/hadoop/yarn /grid/3/hadoop/yarn /grid/4/hadoop/yarn /grid/5/hadoop/yarn' Now /grid/3 was mounted as read-only due to some disk errors. Though other folders got successfully created  Ambari will not start the NodeManager process.notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Exec[mkdir -p /grid/3/hadoop/yarn]/returns: mkdir: cannot create directory '/grid/3/hadoop': Read-only file systemerr: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Exec[mkdir -p /grid/3/hadoop/yarn]/returns: change from notrun to 0 failed: mkdir -p /grid/3/hadoop/yarn returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:479notice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Anchor[hdp::exec::mkdir -p /grid/3/hadoop/yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Anchor[hdp::exec::mkdir -p /grid/3/hadoop/yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Directory[/grid/3/hadoop/yarn]/File[/grid/3/hadoop/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Directory[/grid/3/hadoop/yarn]/File[/grid/3/hadoop/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[capacity-scheduler]/File[/etc/hadoop/conf/capacity-scheduler.xml]/content: content changed '{md5}e5d17c21c7a5e1db9f3af35cba71df0a' to '{md5}2ca1d267a46f1aecac726caabaa16774'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[capacity-scheduler]/File[/etc/hadoop/conf/capacity-scheduler.xml]/owner: owner changed 'hdfs' to 'yarn'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[core-site]/File[/etc/hadoop/conf/core-site.xml]/content: content changed '{md5}86d742a780d59a957ea0a283dec03784' to '{md5}8506e4402ba8140ea4f9fed97b6f94e2'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[yarn-site]/File[/etc/hadoop/conf/yarn-site.xml]/content: content changed '{md5}d84a967ce47a6b77734ed8f53d817c6e' to '{md5}42940cca6e8f64ae5de50524fb131274'notice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Exec[mkdir -p /var/log/hadoop-yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Exec[mkdir -p /var/log/hadoop-yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Directory[/var/log/hadoop-yarn]/File[/var/log/hadoop-yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Directory[/var/log/hadoop-yarn]/File[/var/log/hadoop-yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Exec[mkdir -p /var/run/hadoop-yarn/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Exec[mkdir -p /var/run/hadoop-yarn/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Directory[/var/run/hadoop-yarn/yarn]/File[/var/run/hadoop-yarn/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Directory[/var/run/hadoop-yarn/yarn]/File[/var/run/hadoop-yarn/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Anchor[hdp-yarn::nodemanager::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Anchor[hdp-yarn::nodemanager::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/content: content changed '{md5}093cb1899b3c3b9dc4a7c1c93729c18b' to '{md5}4c462999cc47e6f6ba0e6381d71d81ba'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/owner: owner changed 'mapred' to 'yarn'notice: Finished catalog run in 2.39 seconds", "id": "3074"}, "3076": {"ground_truth": "0", "bug_report": "Jobs run date and duration not sorting correctly\nInput and output bytes columns have been removed from the UI  but the web service doesn't know about that yet so it doesn't map the column index to a sort field correctly.", "id": "3076"}, "3077": {"ground_truth": "0", "bug_report": "Jobs summary has 'oldest' and 'youngest' run dates swapped\nThe web service is labeling these incorrectly when they are retrieved from the db.", "id": "3077"}, "3093": {"ground_truth": "0", "bug_report": "Incorrect units of measure for 'HBase Master Heap' widget on Dashboard\n'HBase Master Heap' widget on Dashboard displays parameters in TB (param1.png)  but real values are in MB (param0.png).", "id": "3093"}, "3095": {"ground_truth": "0", "bug_report": "Incorrect color of rack indicators on 'Heatmaps' page\nPrecondition: hadoop is installed. Total disk space for both machines is 100 GB. Used disk space is about 5-15%.Steps: Go to 'Heatmaps' page. Select 'Host Disk Space Used %' metric ('Maximum' input field has '100' value). Both indicators has green color (disk space usage is in 0-20% category). Choose 'Maximum' input field value to '99'.Result:One indicator has red color (79.2% - 99% category)  but real disk space usage is 8.3% and indicator should be green.", "id": "3095"}, "3099": {"ground_truth": "0", "bug_report": "Cannot change JMX ports using Ambari configuration API\nAmbari 1.2.5  attached the configs to the cluster  in order to provide ability for override behavior at the service and host component levels.The JMX ports read from the service configs can no longer be modified from their default values since the existing code reads the service configurations which do not exist.", "id": "3099"}, "3105": {"ground_truth": "0", "bug_report": "Random change of blocks for Summary and Alerts and health checks on some Service pages\nThe height of blocks 'Alerts and health checks' is changed sometimes after Service page refresh.It can change to normal sizes after another tries.Produced only in Firefox.", "id": "3105"}, "3112": {"ground_truth": "1", "bug_report": "Security wizard: disabling security does not return to initial condition after enabling security fails.\nSteps to reproduce:1. enable security WITHOUT pre-configuring kerberos on cluster and see failures on '3. Start Services';2. disable security.In the end DataNode fails on ALL hosts without a possibility to get started.When you try to start DataNode manually it also ends with error:err: /Stage[2]/Hdp-hadoop::Datanode/Hdp-hadoop::Service[datanode]/Hdp::Exec[su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/Exec[su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/returns: change from notrun to 0 failed: su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:479", "id": "3112"}, "3115": {"ground_truth": "0", "bug_report": "HBase Read/Write request metrics seem to have changed for 2.x stack\nSee the screenshot for HBase Cumulative Requests graph.The graph does not seem like it's displaying cumulative read/write requests  but rather showing the rate... so the label no longer makes sense.This is on the 2.0.5 stack. On the 1.x  I believe this graph would show cumulative read/write requests (which were less useful).So for 2.x stack  we should probably change the label for this graph to say Reads/Writes per Second (or whatever we are showing - need to confirm).", "id": "3115"}, "3119": {"ground_truth": "1", "bug_report": "NullPointerException thrown while retrieving ganglia properties\nAPI Call:curl -u admin:admin http://localhost:8080/api/v1/clusters/c1/services?fields=components/ServiceComponentInfo components/host_components components/host_components/HostRoles components/host_components/metrics/jvm/memHeapUsedM components/host_components/metrics/jvm/memHeapCommittedM components/host_components/metrics/mapred/jobtracker/trackers_decommissioned components/host_components/metrics/cpu/cpu_wio components/host_components/metrics/rpc/RpcQueueTime_avg_time components/host_components/metrics/flume/flume components/host_components/metrics/yarn/QueueAmbari log:20:54:57 044 WARN [qtp912472968-20] ServletHandler:514 - /api/v1/clusters/c1/servicesjava.lang.NullPointerException at org.apache.ambari.server.controller.ganglia.GangliaPropertyProvider.getRRDRequests(GangliaPropertyProvider.java:225) at org.apache.ambari.server.controller.ganglia.GangliaPropertyProvider.populateResources(GangliaPropertyProvider.java:110) at org.apache.ambari.server.controller.internal.VersioningPropertyProvider.populateResources(VersioningPropertyProvider.java:98)", "id": "3119"}, "3124": {"ground_truth": "0", "bug_report": "Incorrect behavior after entering wrong current password while editing user;\n1. Go to Admin page -&gt; Users tab2. Click Edit for some user3. Try to enter wrong value for Current Password field.The message does not tell me that my entered current password is wrong  so it can be confusing for user.Another scenario:1. Go to Admin page -&gt; Users tab2. Click Edit for some user3. Try to enter wrong value for Current Password field and don't fill fields for a new passwordIt don't really change the password but it still allows to enter wrong Current Password without any message", "id": "3124"}}}