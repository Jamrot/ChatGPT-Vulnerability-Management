{
    "Ambari": {
        "3135": {
            "ground_truth": "1",
            "bug_report": "Out of memory issues with Request API on large cluster\nNumber of ExecutionCommandEntity objects keep growing and result in Out of memory on large cluster (100 nodes).Script to re-create the issue:&#91;root@domain user&#93;# cat test1.shfor i in{0..100}doecho 'doing $i'curl -u admin:admin 'http://domain.net:8080/api/v1/clusters/c1/requests?to=end&amp;page_size=10&amp;fields= tasks/Tasks/*' &gt; /dev/nullsleep 5done",
            "id": "3135"
        },
        "3136": {
            "ground_truth": "0",
            "bug_report": "Reduce the size of ExecutionCommand entity\nCurrently  each ExecutionCommandEntity stores the whole config blob. This is a severe duplication of data as tagged configuration information is already available as immutable entry.We need to reduce the footprint of ExecutionCommandEntity by storing only configuration tags. The tags can be replaced with actual configuration value when the command is handed off to the agent.We need to ensure that when Ambari is upgraded and there is a mix of ExecutionCommandEntity instances with and without embedded config - it works.",
            "id": "3136"
        },
        "3145": {
            "ground_truth": "0",
            "bug_report": "ambari-agent service script should return non-zero when the agent is not running\nThe ambari-agent service script should return non-zero when the agent is not running. For example  if a customer wants to have puppet ensure the service is always running  it will not start a killed service because it thinks it's already running when it returns 0.&#91;root@host-123-123-123 init.d&#93;# service ambari-agent statusambari-agent currently not runningUsage: /usr/sbin/ambari-agent {start|stop|restart|status}&#91;root@host-123-123-123 init.d&#93;# echo $?0For comparison...&#91;root@host-123-123-123 init.d&#93;# service winbind statuswinbindd is stopped&#91;root@host-123-123-123 init.d&#93;# echo $?3Possible fix:AMBARI_AGENT_PID_PATH='/var/run/ambari-agent/ambari-agent.pid';RES='3';if [ -f $AMBARI_AGENT_PID_PATH ]then RES='cat $AMBARI_AGENT_PID_PATH | xargs ps -f -p | wc -l'; AMBARI_AGENT_PID='cat $AMBARI_AGENT_PID_PATH';else RES=-1;fiif [ $RES -eq '2' ]then echo 'OK: Ambari agent is running &#91;PID:$AMBARI_AGENT_PID&#93;'; exit 0;else echo 'CRITICAL: Ambari agent is not running &#91;$AMBARI_AGENT_PID_PATH not found&#93;'; exit 2;fi",
            "id": "3145"
        },
        "3147": {
            "ground_truth": "0",
            "bug_report": "Modify ganglia config to match the data resolution of the older version\nNew ganglia version retains almost 50 times more data for higher resolution metrics collection. Ambari needs to modify the configuration to match the older resolution as the new default config has a very high disk space requirement.Looks like the default config for ganglia changed fromRRAs 'RRA:AVERAGE:0.5:1:244' 'RRA:AVERAGE:0.5:24:244' 'RRA:AVERAGE:0.5:168:244' 'RRA:AVERAGE:0.5:672:244' 'RRA:AVERAGE:0.5:5760:374'toRRAs 'RRA:AVERAGE:0.5:1:5856' 'RRA:AVERAGE:0.5:4:20160' 'RRA:AVERAGE:0.5:40:52704'Its an increase from 1350 data points to 78720. After reverting to older configuration the file size for a single metrics and its summary info are-rw-rw-rw- 1 nobody nobody 12224 Sep 7 18:29 ./HDPNameNode/c6402.ambari.apache.org/disk_free.rrd-rw-rw-rw- 1 nobody nobody 23656 Sep 7 18:29 ./HDPNameNode/__SummaryInfo__/disk_free.rrdIn contrast it was-rw-r--r-- 1 root root 630768 Sep 7 17:48 /tmp/rrds/HDPNameNode/c6402.ambari.apache.org/disk_free.rrd-rw-r--r-- 1 root root 1261000 Sep 7 17:47 /tmp/rrds/HDPNameNode/__SummaryInfo__/disk_free.rrdThe recommendation is to revert back to the old config (i.e. do not use the new default config). Confirming that with an older installation of Ambari.",
            "id": "3147"
        },
        "3148": {
            "ground_truth": "0",
            "bug_report": "Oozie install fails with 'could not connect to database' when choosing 'use existing mysql database ' and choosing 'new mysql database' for Hive within Ambari\nPROBLEM: When installing with Ambari and selecting an existing MySQL database for oozie and a new MySQL database for Hive  then the Ambariinstall fails when installing the oozie database. The error thrown is 'could not connect to database' [it appears to drop the 'existing' databaseand you are required to start MySQL on the machine and create the database]BUSINESS IMPACT: This will affect all customers who choose an existing MySQL database for oozie and a new MySQL DB for hive when installing a cluster using AmbariSTEPS TO REPRODUCE: Choose an exisiting oozie MySQL database and point Ambari at this  and select a new Hive MySQL database on the installation optionsACTUAL BEHAVIOR: It seems that it drops the exisiting oozie database and then fails with an errot of could not connect (This is due to MySQL being down  but also the Oozie database is not there anymore)After starting MySQL and creating the database then the installer can continue from where it left off.EXPECTED BEHAVIOR: The installer should not stop MySQL and drop the oozie database if you select create a new Hive database and exisiting MySQL database for oozie.SUPPORT ANALYSIS: Reproduced in the lab in HDP 1.3.2 by following the steps above.",
            "id": "3148"
        },
        "3151": {
            "ground_truth": "0",
            "bug_report": "ambari-server command output should point to Apache Ambari documentation\nambari-server command outputs should point to a generic link for current ambari documentation.",
            "id": "3151"
        },
        "3153": {
            "ground_truth": "0",
            "bug_report": "Secure cluster: Yarn service check fails after configuring yarn for spnego authentication.\nYarn smoke test uses REST api exposed by ResourceManager to get its status. After configuring web authentication yarn client that is assigned yarn service check needs to negotiate 401 HTTP authentication response received while using REST api.",
            "id": "3153"
        },
        "3154": {
            "ground_truth": "0",
            "bug_report": "ZKFailoverController should be shown as a component that can be started/stopped in Host Details page\nZKFailoverController should be shown as a component that can be started/stopped in Host Details page",
            "id": "3154"
        },
        "3160": {
            "ground_truth": "0",
            "bug_report": "WebHCat alert does not nave any description\nSteps to reproduce1. Go to Services page2. click on different services. They all have a status and a message for status in 'Alerts and Health Checks' list (as example hive.png)3. WebHCat service has a status but does not have a status message",
            "id": "3160"
        },
        "3191": {
            "ground_truth": "0",
            "bug_report": "Cannot delete a stopped host_component in INSTALLED state\nI have a host with 4 stopped host_components. When I issue a DELETE on say http://c6401:8080/api/v1/clusters/vmc/hosts/c6404.ambari.apache.org/host_components/DATANODEThe response is:{ 'status' : 500  'message' : 'org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: To remove master or slave components they must be in MAINTENANCE/INIT/INSTALL_FAILED/UNKNOWN state. Current=INSTALLED.'}",
            "id": "3191"
        },
        "3198": {
            "ground_truth": "0",
            "bug_report": "ambari-server reset is broken on centos5.8\nambari-server reset fails due to syntax error in centos 5.8 (postgres 8.1).psql:/var/lib/ambari-server/resources/Ambari-DDL-Postgres-DROP.sql:18: LINE 1: DROP DATABASE IF EXISTS ambari;The IF EXISTS clause was only added to DROP command in PotgreSQL8.2.+Show warnings if any SQL commands failed during server reset or upgradestack",
            "id": "3198"
        },
        "3221": {
            "ground_truth": "0",
            "bug_report": "NameNode Uptime does not appear\nDue to backend change of the position of NameNode startTime property  this will not show up.",
            "id": "3221"
        },
        "3229": {
            "ground_truth": "0",
            "bug_report": "Ambari does not set the correct value for 'templeton.storage.class' in webhcat-site.xml\nAmbari does not set the correct value for 'templeton.storage.class' in webhcat-site.xmlIn an Ambari deployed cluster currently the following value in /etc/hcatalog/conf/webhcat-site.xml is set incorrectly: &lt;property&gt; &lt;name&gt;templeton.storage.class&lt;/name&gt; &lt;value&gt;org.apache.hcatalog.templeton.tool.ZooKeeperStorage&lt;/value&gt; &lt;/property&gt;With the change from HIVE-4895 this should be: &lt;property&gt; &lt;name&gt;templeton.storage.class&lt;/name&gt; &lt;value&gt;org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage&lt;/value&gt; &lt;/property&gt;All jobs involving MapReduce fail because of this issue.",
            "id": "3229"
        },
        "3240": {
            "ground_truth": "0",
            "bug_report": "URLStreamProvider reads are flooding the log\nThese logs do not need to be at INFO level. When user is using the UI then theer is roughly one log per second.06:56:34 232 INFO [pool-1-thread-12] URLStreamProvider:81 - readFrom spec:http://c6402.ambari.apache.org:50075/jmx06:56:34 235 INFO [pool-1-thread-5] URLStreamProvider:81 - readFrom spec:http://c6401.ambari.apache.org:50075/jmx06:56:34 248 INFO [qtp1620999494-18] URLStreamProvider:81 - readFrom spec:http://c6401.ambari.apache.org/cgi-bin/rrd.py?c=HDPSlaves&amp;h=c6401.ambari.apache.org c6402.ambari.apache.org&amp;m=cpu_wio jvm.JvmMetrics.MemHeapUsedM rpc.rpc.RpcQueueTimeAvgTime jvm.JvmMetrics.MemHeapCommittedM&amp;e=now&amp;pt=true...",
            "id": "3240"
        },
        "3251": {
            "ground_truth": "0",
            "bug_report": "When Bind DN credentials are incorrect - we should log it\nWhen integrating Ambari with LDAP if you specify the Bind DN  or Bind credentials that are invalid there is no logging to identify that the authentication fails  so the following search for the logging in user DN will fail. I had to use wireshark to figure out why the integration wasn't working.",
            "id": "3251"
        },
        "3252": {
            "ground_truth": "1",
            "bug_report": "Setup the krb5 Jaas configuration using 'ambari-server setup-security'\n1. Add 'ambari-server setup-security' to replace all of the following operations:'setup-https|setup-ganglia-https|setup-nagios-https|encrypt-passwords'2. Add new operation 'setup-kerberos-auth' to ask the user for: ambari.keytab ambari.principalRelated to  https://issues.apache.org/jira/browse/AMBARI-2941",
            "id": "3252"
        },
        "3255": {
            "ground_truth": "1",
            "bug_report": "Read-only views of security admin tab became editable after visiting other tabs on admin page\nSTR: Go to Admin page -&gt; Security tab Switch to another tab on admin page (Misc tab  HA tab  etc.) Go back to Security tab--------------------Verify that all input fields on this page are read-onlyExpected Result: Everything should be read-only.Actual Result: Some fields become editable .",
            "id": "3255"
        },
        "3258": {
            "ground_truth": "0",
            "bug_report": "Provide UI page to enable/disable experimental functionality\nWe have various experimental functionality provided in UI. We need an easy UI page http://server:8080/#/experimental to enable/disable these functionalities.This will make it easy for users to test these functionalities and give feedback.Refresh of Ambari UI will clear the changes.",
            "id": "3258"
        },
        "3260": {
            "ground_truth": "0",
            "bug_report": "Fix text for custom JCE policy setup\nCurrent text:-c JCE_POLICY  --jce-policy=JCE_POLICY Use specified jce_policy. Must be valid on all hostsThis is required only on ambari-server  the agents will download from the server.",
            "id": "3260"
        },
        "3261": {
            "ground_truth": "0",
            "bug_report": "Cleanup UX for advanced database in 'ambari-server setup'\nCleanup UX...Enter advanced database configuration [y/n] (n)? y==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle==============================================================================Enter choice (1):",
            "id": "3261"
        },
        "3276": {
            "ground_truth": "0",
            "bug_report": "Dashboard page: buttons are shifted if screen width is more than 1200px\nDashboard page: buttons are shifted if screen width is more than 1200px",
            "id": "3276"
        },
        "3279": {
            "ground_truth": "0",
            "bug_report": "Strange behavior of 'JobTracker CPU WIO' dashboard widget\nThis happened because when fixing an old issue  jobTrackerCpu got ignored about that fix.",
            "id": "3279"
        },
        "3285": {
            "ground_truth": "0",
            "bug_report": "Help Text for NameService ID when enabling HA is random in responding to mouse movement and clicks.\nHelp Text for NameService ID when enabling HA is random in responding to mouse movement and clicks.",
            "id": "3285"
        },
        "3292": {
            "ground_truth": "0",
            "bug_report": "Security wizard: On NameNode HA mode  General category should have spnego principal and keytab field\nEarlier dfs.web.authentication.kerberos.keytab field was being used for NameNode and SNameNode component. So we planned to pull this key to NameNode category when HA is enabled as it's the only component then using the key.After HDFS-5091 fix  journalNode also uses this key.So instead of pulling this config key in NameNode section  it should be kept in General category and the description of this principal and keytab location field should be changed accordingly.",
            "id": "3292"
        },
        "3296": {
            "ground_truth": "0",
            "bug_report": "SSH key in logs?\nNotice after agent registration (automatic with SSH key)  I see the SSH key in the popup. Also see it in the ambari-agent.log.Not sure we want to capture this in the log and show in the UI?",
            "id": "3296"
        },
        "3301": {
            "ground_truth": "0",
            "bug_report": "Unavailable stacks should be hidden.\nUnavailable stacks should be hidden.",
            "id": "3301"
        },
        "3302": {
            "ground_truth": "0",
            "bug_report": "Parameterize the repo url for latest stack\nAllow the latest stack repo url to be parameterized for the build. This allows the build to cater to the stack repo as it goes through dev/private/public builds each with different URLs.",
            "id": "3302"
        },
        "3304": {
            "ground_truth": "0",
            "bug_report": "Nagios alert text for NodeManagers should say 'live'\nServices &gt; YARNSee screen shot.Should say 'Percent NodeManagers live'",
            "id": "3304"
        },
        "3307": {
            "ground_truth": "0",
            "bug_report": "Fix Unit tests and create new test for step3 installer\nFix Unit tests and create new test for step3 installer",
            "id": "3307"
        },
        "3310": {
            "ground_truth": "0",
            "bug_report": "HDFS service check should not be disabled when NN HA is enabled and one NN is down\nHDFS service check should not be disabled when NN HA is enabled and one NN is down. In fact  service check passing is an indication that NN is available with one NN down.",
            "id": "3310"
        },
        "3315": {
            "ground_truth": "1",
            "bug_report": "Security wizard: 'Create Principals and Keytabs' step doesn't save state after page refresh\nSecurity wizard: 'Create Principals and Keytabs' step doesn't save state after page refresh",
            "id": "3315"
        },
        "3318": {
            "ground_truth": "0",
            "bug_report": "Use correct case for YARN\nUse correct case for YARN",
            "id": "3318"
        },
        "3324": {
            "ground_truth": "0",
            "bug_report": "UI optimization: constrain hostComponents model loading\nConstrain loading hostComponents into model  so then it loads only on initial loading or when added new hostComponents.",
            "id": "3324"
        },
        "3328": {
            "ground_truth": "0",
            "bug_report": "Unit test for agents fail/hang at TestActionQueue and TestStackUpgrade.\nRunning mvn test on agent fails/hangs under Mac Os",
            "id": "3328"
        },
        "3332": {
            "ground_truth": "0",
            "bug_report": "switching to Configs tab causes Quick Links to disappear\nIn Ambari Web  browse to HDFS  YARN  MapReduce2  etc. Click on the Configs tab  the Quick Links option disappears.See the same regardless of 1.3.2 or 2.0.6  and tried on Firefox and Chrome",
            "id": "3332"
        },
        "3333": {
            "ground_truth": "0",
            "bug_report": "'Services'  'Dashboard' 'Navigation errors\nNavigation from Hosts to Services page or from Hosts to Dashboard pagesometimes fails (nothing happens besides highlighting 'Services' tab) and sometimes navigates to an empty page.Uncaught Error: assertion failed: calling set on destroyed object ember-latest.js:43Ember.assert ember-latest.js:43set ember-latest.js:1386Ember.Observable.Ember.Mixin.create.set ember-latest.js:7769App.MainServiceMenuView.Em.CollectionView.extend.renderOnRoute menu.js:52invokeAction ember-latest.js:3174iterateSet ember-latest.js:3156sendEvent ember-latest.js:3273notifyObservers ember-latest.js:1865Ember.notifyObservers ember-latest.js:1980propertyDidChange ember-latest.js:2613set ember-latest.js:1419(anonymous function) ember-latest.js:10459f.event.dispatch jquery-1.7.2.min.js:3h.handle.i jquery-1.7.2.min.js:3",
            "id": "3333"
        },
        "3336": {
            "ground_truth": "0",
            "bug_report": "HDFS health status when HA config'd\nWhen NameNode HA is config'd:1) HDFS status green if and only if there is Active NameNode; red otherwise2) green -&gt; HDFS Service Stop enabled  HDFS Service Start disabled. red -&gt; HDFS Service Start enabled  HDFS Service Stop disabled",
            "id": "3336"
        },
        "3337": {
            "ground_truth": "0",
            "bug_report": "When invalid jce policy file path is specified  ambari-server setup silently switches over to downloading the file from public repo\nWhen an non-existent file is provided as jce-policy parameter we should get ambari-server setup process failed instead of downloading the file from public repo.Also  if the path is a folder then we should gracefully error out instead of allowing shutil.copy to fail with an error stack.",
            "id": "3337"
        },
        "3350": {
            "ground_truth": "0",
            "bug_report": "ambari-agent RPM claims ownership of /usr/sbin\n*This also affects trunk*The ambari-agent.spec (generated from rpm-maven-plugin) claims ownership of /usr/sbin $ grep sbin target/rpm/ambari-agent/SPECS/ambari-agent.spec | grep attr%attr(755 root root) /usr/sbinThis is a problem because the filesystem RPM owns /usr/sbin.According to rpm-maven-plugin documentation&#91;0&#93;  this is because the only file under /usr/sbin is ambari-agent and'directoryIncludedIf the value is true then the attribute string will be written for the directory if the sources identify all of the files in the directory (that is  no other mapping contributed files to the directory). This is the default behavior.'The 'no other mapping contributed files to the directory' bit is important.The solution is to add directoryInclude=false to the mapping.&#91;0&#93; http://mojo.codehaus.org/rpm-maven-plugin/map-params.html",
            "id": "3350"
        },
        "3356": {
            "ground_truth": "0",
            "bug_report": "wrong property name for https address of NN in hdfs-site.xml\nThe property which defines the https address of namenode is dfs.namenode.https-address. In ambari   the property name is mentioned as 'dfs.https.namenode.https-address'",
            "id": "3356"
        },
        "3362": {
            "ground_truth": "0",
            "bug_report": "Modify the config mappings in the upgrade script to reflect the latest\nModify the upgrade mappings for hdfs-site  core-site  mapred-site  and global to reflect the latest.",
            "id": "3362"
        },
        "3363": {
            "ground_truth": "0",
            "bug_report": "wrong path being set for JSVC_HOME on suse OS in hadoop-env.sh.\nJSVC_HOME is being set to /usr/lib/hadoop/sbin/Linux-amd64-64/ instead of /usr/lib/bigtop-utils.",
            "id": "3363"
        },
        "3397": {
            "ground_truth": "0",
            "bug_report": "Deploy progress bar is not correct\nAll ajax-requests are completed  but progress bar doesn't filled to 100%.",
            "id": "3397"
        },
        "3398": {
            "ground_truth": "0",
            "bug_report": "The number of alerts in MapReduce item in menu gets out on the next line.\nSTD:Make the browser window's width less than actual width of the page.Go to Services page.Stop all services.Result:The number of alerts in MapReduce item in menu gets out on the next line.",
            "id": "3398"
        },
        "3402": {
            "ground_truth": "0",
            "bug_report": "ambari-server setup silently fails when it cannot connect to the remote oracle host\nThe oracle db was installed on a host where port 1521 was not accessible to the ambari-server host. However  'ambari-server setup' did not report failure.Enter advanced database configuration [y/n] (n)? y==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle==============================================================================Enter choice (1): 2Hostname (localhost): test-sm1.iad1Port (1521):Select Oracle identifier type:1 - Service Name2 - SID(1):Service Name (ambari): XEUsername (ambari):Enter Database Password (bigdata):Copying JDBC drivers to server resources...Configuring remote database connection properties...Copying JDBC drivers to server resources...Ambari Server 'setup' completed successfully.",
            "id": "3402"
        },
        "3433": {
            "ground_truth": "0",
            "bug_report": "Add hcat.bin to pig.properties for hcat integration\nAdd hcat.bin to pig.properties for hcat integration.hcat.bin=/usr/bin/hcat",
            "id": "3433"
        },
        "3434": {
            "ground_truth": "0",
            "bug_report": "On 2.x stack  dfs.block.local-path-access.user should not be set in hdfs-site\nOn 2.x stack  dfs.block.local-path-access.user should not be set in hdfs-site",
            "id": "3434"
        },
        "3435": {
            "ground_truth": "1",
            "bug_report": "YARN cluster should not have shared directories between yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs\nYARN cluster should not have shared directories between yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs",
            "id": "3435"
        },
        "3437": {
            "ground_truth": "0",
            "bug_report": "Incorrect alert for NodeManager\nError with RM nagios alert for rpc latency.[1380746845] SERVICE NOTIFICATION: nagiosadmin;&lt;hostaname&gt;;RESOURCEMANAGER::ResourceManager RPC latency;CRITICAL;notify-service-by-email;CRITICAL: Data inaccessible  Status code = 200",
            "id": "3437"
        },
        "3443": {
            "ground_truth": "0",
            "bug_report": "'Assign Slaves and Clients' step. 'all | none' click error\nClicking on 'all|none' affects disabled checkboxes.",
            "id": "3443"
        },
        "3446": {
            "ground_truth": "0",
            "bug_report": "When SSL is enabled on Hadoop JMX endpoints ResourceManager quick links become unavailable\nWhen hadoop.ssl.enabled=true  ResourceManager port is 8090. When it is false  the port is still 8088.",
            "id": "3446"
        },
        "3456": {
            "ground_truth": "0",
            "bug_report": "Text of installation stage doesn't correspond to reality\nSTD:On the latter stages of installing cluster  refresh page 'Install  Start and Test'..Result:Appeared 'Next' button and progress of installation is setted to 100%  but message of installation on the second host says that the Nagios Server is not installed yet. After clicking 'Next' all seems good and Nagios Server is installed normally.",
            "id": "3456"
        },
        "3457": {
            "ground_truth": "0",
            "bug_report": "When multiple MR2 Clients are installed  the label is a bit off\nWhen multiple MR2 Clients are installed  the MR2 summary panel has a label like: '3 MapReduce2 Client s Installed' (with an unnecessary space)",
            "id": "3457"
        },
        "3477": {
            "ground_truth": "0",
            "bug_report": "JavaScript errors during service tab changing\nSteps:Open browser console.Run start or stop operation for any service.Switch between some services repeatedly.Result:'Calling set on destroyed view' was appeared in console.",
            "id": "3477"
        },
        "3488": {
            "ground_truth": "0",
            "bug_report": "Status does not show up for newly added hosts\nThe problem is that host can have actual status only when service mapper recieve response(could be long latency  about 10 - 12 seconds) with new hostComponents and then status mapper compute them and set status to host.",
            "id": "3488"
        },
        "3490": {
            "ground_truth": "0",
            "bug_report": "Remove RCO management logic at ambari-agent\nIn 1.5.0 release  ambari-agent will not need to process RCO to re-order/parallelizing tasks. Let's remove the code/unit-test and keep them aside in a JIRA targeted for release after 1.5.0. If possible  let's remove upgrade related code as well as there is no plan for automatic stack upgrade for Baikal. We can keep the python executor as there is a requirement for python executor.",
            "id": "3490"
        },
        "3491": {
            "ground_truth": "0",
            "bug_report": "HBase Master/RegionServer can no longer be started after reconfiguring HBase or HDFS with NameNode HA enabled\nHBase Master/RegionServer can no longer be started after reconfiguring HBase or HDFS with NameNode HA enabled",
            "id": "3491"
        },
        "3498": {
            "ground_truth": "0",
            "bug_report": "Hbase secure config properties in HDP-2.x stack revert back to non-secure values on reconfiguration\nHbase secure config properties in HDP-2.x stack revert back to non-secure values on reconfiguration",
            "id": "3498"
        },
        "3507": {
            "ground_truth": "0",
            "bug_report": "'Assign Slaves' step. Error with installed NodeManagers\nInstalled NodeManagers don't appears as selected checkboxes on the 'Assign Slaves' step.",
            "id": "3507"
        },
        "3512": {
            "ground_truth": "0",
            "bug_report": "Nagios doesn't start after upgrade [SLES11  1.3.2->2.0.6]\ncheck_cpu needs to be disabled for Suse for JobHistory server and ResourceManager",
            "id": "3512"
        },
        "3521": {
            "ground_truth": "0",
            "bug_report": "Incorrect status counters on cluster deploy\nIn host stauts filter label shows incorrect number of hosts after deploy failed.Wrong progress bar color  on fail color should be red or yellow  instead of blue.",
            "id": "3521"
        },
        "3528": {
            "ground_truth": "0",
            "bug_report": "DB url isn't calculated automatically\nSelect Hive or Oozie and go to step 'Customize services'.'Open' Oozie tab.Database Url is 'jdbc'.Click 'Existing MySQL Database'.Click 'New Derby Database'.Database Url became jdbc:derby:${oozie.data.dir}/${oozie.db.schema.name}-db;create=true.Expect:proper value should be right after step is loaded.",
            "id": "3528"
        },
        "3534": {
            "ground_truth": "0",
            "bug_report": "Hadoop Core Health Check script needs to be included in Ambari HDP installations\nHadoop Core Health Check script needs to be included in Ambari HDP installations",
            "id": "3534"
        },
        "3535": {
            "ground_truth": "0",
            "bug_report": "skip 'Customize Services' step for services that can't be customized\nServices like PIG  Sqoop can't be customized.Wizard should check services-list (that user want to add) and if no one service can't be customized  should skip 'Customize' step ('Back' click on the next step should also be verified).",
            "id": "3535"
        },
        "3537": {
            "ground_truth": "0",
            "bug_report": "Allow log4j properties to be applied via the API in Ambari for hadoop/oozie/hbase/hive/zookeeper/pig\nAllow log4j properties to be applied via the API in Ambari for hadoop/oozie/hbase/hive/zookeeper/pig.",
            "id": "3537"
        },
        "3553": {
            "ground_truth": "0",
            "bug_report": "NameNode HA wizard: Refreshing the wizard displays incorrect manual commands.\nInstall HDFS with customized hostname hdfs1. Start NameNode HA wizard and Refresh on step-2 (select host). Proceed ahead. Create checkpoint step asks to run command with incorrect user name:sudo su -l hdfs -c 'hdfs dfsadmin -safemode enter' Above command returns safemode: Access denied for user hdfs. Superuser privilege is required Actual command should be:sudo su -l hdfs1 -c 'hdfs dfsadmin -safemode enter'",
            "id": "3553"
        },
        "3569": {
            "ground_truth": "0",
            "bug_report": "'Config' step refresh\nGo to Config Step on the addServiceWizard.Refresh page.Got JS error  because selected services where not saved.Expect: get page with config list for selected services.",
            "id": "3569"
        },
        "3582": {
            "ground_truth": "0",
            "bug_report": "Cleanup UI restart calculations using actual_configs\nAs documented in AMBARI-3531  the restart flags will be provided in host_components itself and services will have an API to get restart host_components easily. Due to this  there is no need for actual_configs on the client  and the code to calculate diffs with global properties.",
            "id": "3582"
        },
        "3584": {
            "ground_truth": "0",
            "bug_report": "Reassign Master: Misc UI display fixes\nThis ticket mainly covers UI label and message changes in reassign master wizard.",
            "id": "3584"
        },
        "3589": {
            "ground_truth": "0",
            "bug_report": "Common storage for different wizards\nSome wizards (installer  addHosts  addServices) use common local storage objects.But each should has separated object (for example  based on controllerName).",
            "id": "3589"
        },
        "3594": {
            "ground_truth": "0",
            "bug_report": "Service reconfiguration fails for multiple services\nService reconfiguration fails for HDFS  MapReduce and Hive service with js error. For other services it fails silently without any error (no API call is triggered).",
            "id": "3594"
        },
        "3609": {
            "ground_truth": "0",
            "bug_report": "os_type_check.sh for RHEL is too restrictive (Server vs Workstation)\nThe /etc/redhat-release on my RHEL6.4 dev box containsRed Hat Enterprise Linux Workstation release 6.4 (Santiago)os_type_check.sh is checking for 'Server' on rhel boxes. It should simply check for Red Hat Enterprise Linux and ignore text up to the version number.",
            "id": "3609"
        },
        "3611": {
            "ground_truth": "0",
            "bug_report": "Hosts: clarify which filter is in effect\n1. Use a shadow with high contrast to make the current filter more visible.2. Show how many hosts total / how many are in the current view  and a 'Clear all filters' link.",
            "id": "3611"
        },
        "3613": {
            "ground_truth": "0",
            "bug_report": "Enable HA wizard loads after sign in\nSteps: Go to 'Admin' page -&gt; 'High Availability' tab and run 'Enable NameNode HA' wizard. Close wizard. Sign out (or reopen browser). Sign in.Result:After sign in was opened first page of 'Enable NameNode HA' wizard instead 'Dashboard' page.",
            "id": "3613"
        },
        "3615": {
            "ground_truth": "0",
            "bug_report": "Ambari agent creates empty folder /var/ambari-agent\nAmbari agent creates an empty directory /var/ambari-agent during installation.This directory isn't needed  /var/run/ambari-agent is used instead.",
            "id": "3615"
        },
        "3618": {
            "ground_truth": "0",
            "bug_report": "host actions UI changes based on new stop/start all and delete func\n1. Do the right-float on the action menus on the Components section.2. Rename buttons: on SERVICE PAGES: Maintenance --&gt;Service Actions...on HOST PAGES: Maintenance --&gt; Host Actions...on HOST PAGES / COMPONENT SECTION: Actions --&gt; Actions...",
            "id": "3618"
        },
        "3621": {
            "ground_truth": "0",
            "bug_report": "cleanup dialog for unable to delete host\nmake dialog according to left mockup",
            "id": "3621"
        },
        "3623": {
            "ground_truth": "0",
            "bug_report": "LiveStatus of the component is not updated when username is changed\nSteps to reproduce: On installer wizard  make install phase fail by killing any install task of master component. Go back and change hdfs username to hdfs1. Proceed ahead and installer wizard completes successfully. HDFS service is red. Nagios shows no alerts  but API returns INSTALLED status for all hdfs host components. UI impact: On starting HDFS  all tasks completes successfully with 100% green progress bar but service status always remains red. Restarting agent resolves the issue.Looks like AmbariConfig.servicesToPidNames is not getting updated when username is changed.",
            "id": "3623"
        },
        "3631": {
            "ground_truth": "0",
            "bug_report": "traceback when attempting to stop ambari-agent as non-root\nI attempted to stop the ambari-agent without going to root first. Prints a pretty bad traceback.&#91;vagrant@c6403 ~&#93;$ ambari-agent stop/usr/sbin/ambari-agent: line 66: /var/lib/ambari-agent/ambari-env.sh: Permission deniedVerifying Python version compatibility...Using python /usr/bin/python2.6Found ambari-agent PID: 2996Stopping ambari-agentTraceback (most recent call last):File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 235  in &lt;module&gt;main()File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 190  in mainsetup_logging(options.verbose)File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 73  in setup_loggingrotateLog = logging.handlers.RotatingFileHandler(logfile  'a'  10000000  25)File '/usr/lib64/python2.6/logging/handlers.py'  line 112  in initBaseRotatingHandler.init(self  filename  mode  encoding  delay)File '/usr/lib64/python2.6/logging/handlers.py'  line 64  in initlogging.FileHandler.init(self  filename  mode  encoding  delay)File '/usr/lib64/python2.6/logging/init.py'  line 827  in __initStreamHandler.init(self  self._open())File '/usr/lib64/python2.6/logging/init.py'  line 846  in _openstream = open(self.baseFilename  self.mode)IOError: &#91;Errno 13&#93; Permission denied: '/var/log/ambari-agent/ambari-agent.log'Removing PID file at /var/run/ambari-agent/ambari-agent.pidrm: cannot remove '/var/run/ambari-agent/ambari-agent.pid': Permission deniedambari-agent successfully stopped",
            "id": "3631"
        },
        "3645": {
            "ground_truth": "0",
            "bug_report": "HA cluster: some dashboard's widgets contain 'Null'  'NaN' values after services stop\nSteps:Stop YARN service.Go to 'Dashboard'.Result:'NodeManagers Live' widget contains 'null' values.Similar problem is present for 'HBase Ave Load' widget - 'NaN' value.",
            "id": "3645"
        },
        "3648": {
            "ground_truth": "0",
            "bug_report": "Failed to start Hive Metastore (centos5.8  Stack 2.0)\nOccurred during install as warning (could not start the service). Clicked next to continue  when into Ambari  then tried to start Hive there as well  same issue.",
            "id": "3648"
        },
        "3650": {
            "ground_truth": "0",
            "bug_report": "Poll for host_components which have stale_configs\nUI needs to know which host_components need restart due to stale_configs (saved but not picked up). Server API provide stale_configs flag per host-component. We need this polled and maintained on client model.",
            "id": "3650"
        },
        "3674": {
            "ground_truth": "0",
            "bug_report": "UI does not update active hbase master in display\nWhen we have 3 HBase masters we show in UI that one of them is active. When the master is stopped  the other HBase master is not marked as active in UI. In API it does become active.",
            "id": "3674"
        },
        "3675": {
            "ground_truth": "1",
            "bug_report": "Default value of 'Default virtual memory for a job's map-task' is not valid\nThe default value of 'Default virtual memory for a job's map-task' (in Customize Services page -&gt; MapReduce2 tab -&gt; General) was '619.5'.Warning hint says 'Must contain digits only'Value depends on quantity of installed components.",
            "id": "3675"
        },
        "3679": {
            "ground_truth": "0",
            "bug_report": "Better error message needed when incompatible ambari-agents installed\nOn a few days old cluster I attempted to add a host. The add host failed due to ambari-server trying to install ambari-agent-1.4.1.17 and the repo having ambari-agent-1.4.1.23-1.The message in /var/run/ambari-server/bootstrap/11/hostname.log was:STDERRscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py done for host srimanth1-5.c.pramod-thangali.internal  exitcode=0Copying files finishedRunning setup agent...STDOUTError: Nothing to do{'exitstatus': 1  'log': ('Loaded plugins: downloadonly  fastestmirror  security/nDetermining fastest mirrors/n * base: www.gtlib.gatech.edu/n * extras: centos.mirror.netriplex.com/n * updates: mirror.cogentco.com/nSetting up Install Process/nNo package ambari-agent-1.4.1.17 available./n'  None)}",
            "id": "3679"
        },
        "3686": {
            "ground_truth": "0",
            "bug_report": "NameNode HA wizard (Configure Components step): Task 'Reconfigure HDFS' always fail  and user cannot proceed to next step\nThis task fails due to bad request:/api/v1/clusters/c1/hosts/HDFS_CLIENT/host_components/dev01.hortonworks.comShould be: api/v1/clusters/c1/hosts/dev01.hortonworks.com/host_components/HDFS_CLIENT",
            "id": "3686"
        },
        "3690": {
            "ground_truth": "0",
            "bug_report": "Typo in text label on Hosts page\nOn the Hosts page  we have a typo in the label 'filterd'. Should be 'filtered'.",
            "id": "3690"
        },
        "3695": {
            "ground_truth": "0",
            "bug_report": "'Confirm hosts' shows 'ntpd not running' warning  but it's running on host\nSTR: Install  setup and start Ambari server by default. Reach 'Choose services' phase of installer.Actual result:'Confirm hosts' shows warning that ntpd service isn't running on hosts  but it's running in console by command service ntpd status",
            "id": "3695"
        },
        "3698": {
            "ground_truth": "0",
            "bug_report": "Modify UI text for host cleanup\npython /usr/lib/python2.6/site-packages/ambari_agent/HostCleanup.py -s -k 'users'To cleanup in interactive mode  remove *-s* option. To cleanup all resources  including _users_  remove *-k users* option. Use *--help* for a list of available options. The motivation is to provide the conservative option but minimal detail to allow for full clean up.",
            "id": "3698"
        },
        "3701": {
            "ground_truth": "0",
            "bug_report": "Reduce logs emitted to report heartbeats from agents\nFor a 657 node cluster:~10 minute for 10 MB and 20 log files store about 200 minutes (~3 hours) of log. This is not ideal if an error overnight needs to be investigated. We should try for the log to last 24 hours - ideally 72 hours to account for weekends.-rw-r--r-- 1 root root 10485854 Oct 17 17:35 ambari-server.log.6-rw-r--r-- 1 root root 10485836 Oct 17 17:44 ambari-server.log.5-rw-r--r-- 1 root root 10485811 Oct 17 17:52 ambari-server.log.4-rw-r--r-- 1 root root 10485793 Oct 17 18:01 ambari-server.log.3-rw-r--r-- 1 root root 10485793 Oct 17 18:10 ambari-server.log.2-rw-r--r-- 1 root root 10485854 Oct 17 18:18 ambari-server.log.1",
            "id": "3701"
        },
        "3708": {
            "ground_truth": "0",
            "bug_report": "Reconfigure of dynamic configs not showing modified values\nModifications of dynamic properties are being persisted on server  but default values are being shown in UI. Also  we should not validate dynamic configs which are of type string. mapreduce.map.java.opts mapreduce.reduce.java.opts yarn.app.mapreduce.am.command-opts",
            "id": "3708"
        },
        "3713": {
            "ground_truth": "0",
            "bug_report": "When filtering on hosts  the table column sizes shift  should stay fixed.\nFor the 'defaultsProvider' and 'serviceValidator' functionalities  we need unit tests",
            "id": "3713"
        },
        "3715": {
            "ground_truth": "0",
            "bug_report": "Reassign Master Wizard does not display folder and hosts on 'Manual commands' page after browser reopening\nSteps: Open 'Reassign Master Wizard' for NameNode or SNameNode. Go to 'Manual commands' page. Close browser and open it again.Result: Was opened 'Manual commands' page  but hostnames and foldername were replaced with '{1}'  '{2}' etc.Attached picture for other page  but behavior is similar.",
            "id": "3715"
        },
        "3720": {
            "ground_truth": "0",
            "bug_report": "Provide read-only view of repo options in Ambari Web\nIf a user is using local repos  and customizes Advanced Repository Options during install  the user might need this info to debug post install (since it is used in Add Hosts)  for example.Note: We should show this information regardless if the user customizes repos or not during install.",
            "id": "3720"
        },
        "3724": {
            "ground_truth": "0",
            "bug_report": "Incorrect host status when slave down\nWhen host has slave down status 'No Heartbeat' status is shown instead of 'Slave Down'.",
            "id": "3724"
        },
        "3726": {
            "ground_truth": "0",
            "bug_report": "Restart indicators for services and hosts disappear after some time.\nGoto Service (that have stale configs) -&gt; configs   no restart indicators are shown. Refresh page  for 10-15 seconds you see indicators then they disappear. The same for host detail page",
            "id": "3726"
        },
        "3729": {
            "ground_truth": "0",
            "bug_report": "Ganglia monitor started with second or third attempt on secure cluster\nGanglia monitor started with second or third attempt on secure cluster",
            "id": "3729"
        },
        "3738": {
            "ground_truth": "0",
            "bug_report": "Background ops dialog checkbox UI cleanup\nThe OK and the text should be on the same centerline row.",
            "id": "3738"
        },
        "3739": {
            "ground_truth": "0",
            "bug_report": "Remove Exception message printed to log for successful starts\nFollowing error messages are printed to log with default log level and are misleading.04:19:52 438 ERROR [main] MasterKeyServiceImpl:109 - Master key is not provided as a System property or an environment varialble.04:19:52 439 INFO [main] Configuration:415 - Credential provider creation failed.Master key initialization failed.",
            "id": "3739"
        },
        "3742": {
            "ground_truth": "0",
            "bug_report": "HBase Links widget has 'more' button out of bounds and looks broken when there are multiple masters\nWhen there are multiple HBase Masters  the HBase Links widget looks broken. Let's get rid of the 'and X Standby Masters' static text as it is not a link and not very useful.So the widget would look like:'HBase Master''X RegionServers''Master Web UI'",
            "id": "3742"
        },
        "3752": {
            "ground_truth": "0",
            "bug_report": "MR jobs are hanging on a 2-node cluster with default configuration\nThis is a 2-node cluster with 2GB of RAM each. Cluster deployment goes fine but MR jobs do not complete resulting in service check failures for MR  OOZIE  Pig  etc.",
            "id": "3752"
        },
        "3759": {
            "ground_truth": "0",
            "bug_report": "Add host wizard: After successfully bootstrapping host  'next' button is disabled\nSee screenshot",
            "id": "3759"
        },
        "3760": {
            "ground_truth": "0",
            "bug_report": "Provide config-group support in add-host wizard\nWhen adding hosts  a user should be able to select which config-groups this host belongs to. Configurations of that group (Default or config-group) will be applied on that host.",
            "id": "3760"
        },
        "3761": {
            "ground_truth": "0",
            "bug_report": "'Uncaught exception' in JS while navigating through services on Services page\nThis one was discovered while quick navigating through services on Services page.To reproduce just try to click on services links fast.After that service content is not displayed.",
            "id": "3761"
        },
        "3770": {
            "ground_truth": "0",
            "bug_report": "Need better error log message when agent unable to reach server\nhttp://hortonworks.com/community/forums/topic/installing-hdp2-0-6-on-centos6-4/The current ERROR in the agent log can be cryptic.",
            "id": "3770"
        },
        "3771": {
            "ground_truth": "0",
            "bug_report": "Ambari should allow changing Ganglia cache location\nAmbari allows changing Ganglia directory during installation  but not after the cluster is installed. We should allow changing this directory after cluster installed",
            "id": "3771"
        },
        "3779": {
            "ground_truth": "0",
            "bug_report": "During cluster install cannot go past Step0\nUI makes a call to http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions?fields=Versions operatingSystems/repositories/Repositories.The API is missing the operatingSystems info  except for the suse11 one:{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions?fields=Versions operatingSystems/repositories/Repositories'  'items' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.2.0'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.2.0' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.2.1'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.2.1' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.0'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.0' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.2'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.2' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.3'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.3' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.5'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '2.0.5' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'operatingSystems' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/centos5'  'OperatingSystems' : { 'os_type' : 'centos5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/centos6'  'OperatingSystems' : { 'os_type' : 'centos6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/oraclelinux5'  'OperatingSystems' : { 'os_type' : 'oraclelinux5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/oraclelinux6'  'OperatingSystems' : { 'os_type' : 'oraclelinux6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/redhat5'  'OperatingSystems' : { 'os_type' : 'redhat5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/redhat6'  'OperatingSystems' : { 'os_type' : 'redhat6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/sles11'  'OperatingSystems' : { 'os_type' : 'sles11'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/suse11'  'OperatingSystems' : { 'os_type' : 'suse11'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/suse11/repositories/HDP-2.0.6'  'Repositories' : { 'base_url' : 'http://public-repo-1.hortonworks.com/HDP/suse11/2.x/updates/2.0.6.0'  'default_base_url' : 'http://public-repo-1.hortonworks.com/HDP/suse11/2.x/updates/2.0.6.0'  'mirrors_list' : null  'os_type' : 'suse11'  'repo_id' : 'HDP-2.0.6'  'repo_name' : 'HDP'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' } } ] } ] } ]}",
            "id": "3779"
        },
        "3791": {
            "ground_truth": "0",
            "bug_report": "Provide add/remove/rename/duplicate actions in manage-config-groups dialog\nVarious config-group actions (add/remove/rename/duplicate) should be provided in the manage-config-groups dialog. Any of these should not rely on the Save button  but are immediately persisted via API.",
            "id": "3791"
        },
        "3793": {
            "ground_truth": "0",
            "bug_report": "Do not store disks_info in DB. Store it as dynamic info in memory that can be used to show on the UI.\nCurrently  we store disks info in DB and this information is never updated after registration. As disks details can change (space availability changes  disks get mounted/unmounted  etc.) the persisted information is not useful.We should instead hold the details in memory and refresh it at certain intervals (e.g. once every 10 minutes) and then alert if space availability hits some lower limit.",
            "id": "3793"
        },
        "3805": {
            "ground_truth": "0",
            "bug_report": "'Add service' if nothing to add\nDisable and gray out the Add Services button if there aren't any more services to be added.Upon hover  show a tooltip saying 'No more services to be added'.Although we hide the 'Add Component' button when no more components are to be added  we don't actually like that and want to move towards 'disabling/graying out with hover tooltip' pattern.",
            "id": "3805"
        },
        "3815": {
            "ground_truth": "0",
            "bug_report": "Remove  Rename actions enabled for 'Default' config group\nIn the Manager Configuration Groups dialog I selected the Default config-group  and the actions to remove and rename config-group are enabled. These ops are not allowed for Default config-group.",
            "id": "3815"
        },
        "3824": {
            "ground_truth": "0",
            "bug_report": "Provide change config-group action on host configs\nOn a host's configs page  provide a Change action beside config-group to switch from one group to another  or to Default.",
            "id": "3824"
        },
        "3836": {
            "ground_truth": "0",
            "bug_report": "Unable to close manage-config-groups dialog when only Default group present\nI had only the Default config-group when I launched the manage config-groups dialog. When I clicked on Cancel or X  it would not close dialog with the following error:Uncaught TypeError: Cannot read property 'id' of null This was in method updateConfigGroupOnServicePage() at the first line belowselectedConfigGroup = managedConfigGroups.findProperty('id'  selectedConfigGroup.id); if(selectedConfigGroup){ mainServiceInfoConfigsController.set('selectedConfigGroup'  selectedConfigGroup); }else{ mainServiceInfoConfigsController.set('selectedConfigGroup'  managedConfigGroups.findProperty('isDefault'  true)); }",
            "id": "3836"
        },
        "3838": {
            "ground_truth": "0",
            "bug_report": "Services sidebar for host configs needs vertical gap\nWhen you go to host configs page  there is no gap between the services sidebar and the tabs. This should be changed so that the config-group bar and services sidebar have the same gap from the tabs at top.",
            "id": "3838"
        },
        "3842": {
            "ground_truth": "0",
            "bug_report": "Host configs page should properly order services\nWhen App.supports.hostOverridesHost is enabled  and you visit the configs page for a host  the services are in some random order. They should be in the same order as the services page.Also  there is a small empty entry in the menu  which gives like a 5px extra space between some services. You can even hover on this empty entry and it will highlight.",
            "id": "3842"
        },
        "3844": {
            "ground_truth": "0",
            "bug_report": "Error in saving host for newly created config group\nAfter creating new Config Group in Manage Configuration Groups dialog try to add host to this group and save. Also sometimes '+' button to add hosts is disabled for newly created group.",
            "id": "3844"
        },
        "3857": {
            "ground_truth": "0",
            "bug_report": "Clicking on Settings link navigates to login page for a non-admin user.\nThis happens because non-admin users are not authorized to make POST/PUT calls to any resource  including 'persist'.For now  let's hide 'Settings' if the user is a non-admin user.",
            "id": "3857"
        },
        "3864": {
            "ground_truth": "0",
            "bug_report": "JS Error in 'Add Host Wizard' if we proceed with failed registered hosts\n1. Add two hosts in Add Host Wizard.2. In Conform Hosts step  one registered successfully  the other failed.3. Proceed to next  JS error happened when deploy  wizard UI hang up.",
            "id": "3864"
        },
        "3868": {
            "ground_truth": "0",
            "bug_report": "Add host fails after configuring NN HA with JavaScript error\nAdd host fails after configuring NN HA with JavaScript error",
            "id": "3868"
        },
        "3873": {
            "ground_truth": "0",
            "bug_report": "Unittests for User resource an all it's attributes\nTest resource User of resource management . Test all actions and all the attributes. Please make sure we mock to check both cases when user already exists and when it's not.Note here we should not call directly provider methods like action_create  but make resource management library do that for us  by calling env.run(). In other case test won't cover resources definitions  and other import logic",
            "id": "3873"
        },
        "3877": {
            "ground_truth": "0",
            "bug_report": "Duplicate config-group action not duplicate configs\nOverride like 3 configs in a config group and save. Now go to the Manage Config Groups dialog and select this group and click on Duplicate action. A popup with name and description pops up and hitting OK creates the duplicated config group.Though this duplicated config group has the correct name/description  it does not have the duplicated configs (3 that we overrode). When doing the POST call  we should populate the desired_configs to be the exact same as the source config-group.",
            "id": "3877"
        },
        "3878": {
            "ground_truth": "0",
            "bug_report": "ResourceManager Heap metrics is not correct on Ambari console\nPROBLEM: The ResourceManager Heap metrics on Ambari web console doesn't show the correct value  not only the current heap usage doesn't reflect the correct usage  but also the total heap size doesn't match what we configure for the ResourceManager Heap size  even the total heap size number is changing constantly.",
            "id": "3878"
        },
        "3881": {
            "ground_truth": "0",
            "bug_report": "UI incorrect behavior during upgrade\nThis issue was discovered while upgrading the cluster to hash 87adc8c2d29b20a30f01e54c12f67dcbbe34b32e of 1.4.2 branch. The logic inside configuration_controller.js function getConfigsByTags(tagObject) has a reference to undefined variable and js error was encountered. This happened when any of the service page was rendered and program flow from quick_view_link_view.js function didInsertElement() -&gt; setQuickLinks() -&gt; loadTags() -&gt; loadTagsSuccess(data) -&gt; getSecurityProperties() -&gt; configurationController.getConfigsByTags(tag)",
            "id": "3881"
        },
        "3882": {
            "ground_truth": "0",
            "bug_report": "Background operations popup window minimum size should be fixed when narrowing down the browser\nBackground operations popup window minimum size should be fixed when narrowing down the browser",
            "id": "3882"
        },
        "3888": {
            "ground_truth": "0",
            "bug_report": "Incorrect restart required tooltip view\nWhen components and hosts required to restart we show refresh icon near the service name. On icon hover event we show tooltip with count of components and hosts.",
            "id": "3888"
        },
        "3890": {
            "ground_truth": "0",
            "bug_report": "Background operations: two scrollbars  if width is lower then 1450px\nScreenshot attached.",
            "id": "3890"
        },
        "3897": {
            "ground_truth": "0",
            "bug_report": "Restart indicator flags not showing up for HDP 1.3.x stack services\nWhen config-groups are used for HDP 1.3.x stack services  the stale_configs are always false.",
            "id": "3897"
        },
        "3899": {
            "ground_truth": "0",
            "bug_report": "'HDFS Short-circuit read' config property is repeated\n'HDFS Short-circuit read' config property is repeated",
            "id": "3899"
        },
        "3900": {
            "ground_truth": "0",
            "bug_report": "Modify messages from 'reassign master' to 'move master'\nModify messages from 'reassign master' to 'move master'",
            "id": "3900"
        },
        "3911": {
            "ground_truth": "1",
            "bug_report": "Security Wizard: Service Configuration page is broken\nTrying to enable security  the configurations page is blank.",
            "id": "3911"
        },
        "3914": {
            "ground_truth": "0",
            "bug_report": "Add Host wizard stuck on configuration step\nCluster should have service(HDFS PIG SQOOP) which doesn't have any slave or client host-components.1. Run Add Host wizard2. Proceed to configuration stepResult: Wizard popup stuck in loading processJS error:Uncaught TypeError: Cannot call method 'get' of undefined app.js:10245(anonymous function) app.js:10245App.AddHostController.App.WizardController.extend.loadServiceConfigGroups app.js:10242(anonymous function) app.js:43659f.Callbacks.o vendor.js:95f.Callbacks.p.add vendor.js:95(anonymous function) app.js:43657f.Callbacks.o vendor.js:95f.Callbacks.p.fireWith vendor.js:95f.Callbacks.p.fire vendor.js:95(anonymous function)",
            "id": "3914"
        },
        "3921": {
            "ground_truth": "0",
            "bug_report": "Hovers stay after manage-config-groups dialog is closed\nI opened the manage-config-groups dialog and hovered on one of the actions (remove host from config-group). Then I saved or cancelled to close the dialog. The hover still remains in the middle of page.I have seen this in other usages as well. We need to make sure that all hovers are closed when the focus is lost.",
            "id": "3921"
        },
        "3925": {
            "ground_truth": "0",
            "bug_report": "Adding host to multiple groups at the same time fails\nSteps:1. Create 2 config groups2. Rename 1 config group3. Add hosts to renamed config group4. Add host to other config group.Result:Save stops working. Error in the JS console.Error in JS console:Uncaught TypeError: Cannot call method 'sort' of undefined manage_config_groups_controller.js:460(anonymous function) manage_config_groups_controller.js:460(anonymous function) manage_config_groups_controller.js:458ComputedPropertyPrototype.get ember-latest.js:2949get ember-latest.js:1355getPath ember-latest.js:1477get ember-latest.js:1348Ember.Observable.Ember.Mixin.create.get ember-latest.js:7695App.ModalPopup.show.onPrimary item.js:251newFunc ember-latest.js:949ActionHelper.registeredActions.(anonymous function).handler ember-latest.js:19458(anonymous function) ember-latest.js:11250f.event.dispatch jquery-1.7.2.min.js:3h.handle.i",
            "id": "3925"
        },
        "3930": {
            "ground_truth": "0",
            "bug_report": "Missing host message on cluster deploy\nWhen execute any service check  host message become empty. Server return  unsupported on UI  command of task - 'SERVICE_CHECK'.",
            "id": "3930"
        },
        "3938": {
            "ground_truth": "0",
            "bug_report": "JS error when switching config groups in Hive / Oozie service config pages\nJS error when switching config groups in Hive / Oozie service config pages",
            "id": "3938"
        },
        "3953": {
            "ground_truth": "0",
            "bug_report": "HBase Master alerts are confusing in multi-master environment\nWhen multiple HBase Masters are set up  HBase service-level alert section shows multiples of the following: HBase Master process HBase Master Web UI HBase Master CPU utilizationThe label is exactly the same for all masters  so you can't distinguish which alert is for which master. Ambari should mirror what ambari does for NameNode alerts so that the user can tell them apart (append hostname in the alert label).",
            "id": "3953"
        },
        "3954": {
            "ground_truth": "0",
            "bug_report": "hbase.zookeeper.quorum changing inconsistently on hosts after adding ZookeeperServer\nIssue 1:Steps followed: 1. Install a 3-node cluster with Hbase and Zookeeper and 3 zookeeper servers. 2. After installation on each host the property hbase.zookeeper.quorum in /etc/hbase/conf/hbase-site.xml has:&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org c6402.ambari.apache.org c6403.ambari.apache.org&lt;/value&gt;3. After adding a host with Hbase Region Server  and after that a ZookeeperServer to it  the property on the added c6404.ambari.apache.org host has the same value  as in 2. 4. After restarting HBase master on c6401 (which is proposed by UI)  the property value on c6401 becomes:  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org&lt;/value&gt;. On other hosts it remains unchanged. 5. After restarting HbaseRegionServers on the rest of the hosts (not proposed by ui)  the property changes too  to the same value  as in 4. __________________________________________________________Issue 2:Property templeton.zookeeper.hosts in /etc/hcatalog/conf/webhcat-site.xmlPrior to adding a zookeeperServer on host c6404  config on WebHCat server lookes like:[root@c6402 vagrant]# cat /etc/hcatalog/conf/webhcat-site.xml |grep templeton.zookeeper.hosts -C 2 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;templeton.zookeeper.hosts&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org:2181 c6402.ambari.apache.org:2181 c6403.ambari.apache.org:2181&lt;/value&gt; &lt;/property&gt;After adding a ZookeeperServer on c6404 and restarting WebHCatServer on c6402: [root@c6402 vagrant]# cat /etc/hcatalog/conf/webhcat-site.xml |grep templeton.zookeeper.hosts -C 2 &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.zookeeper.hosts&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org&lt;/value&gt; &lt;/property&gt;__________________________________________________________Issue 3. On a 3-node cluster with HA-enbaled  property ha.zookeeper.quorum in /etc/hadoop/conf/core-site.xml  has the same value on all hosts:  &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org:2181 c6402.ambari.apache.org:2181 c6403.ambari.apache.org:2181&lt;/value&gt;It doesn't change on either of the hosts after adding c6404 to the cluster  installing ZookeeperServer on it and restarting HDFS.",
            "id": "3954"
        },
        "3981": {
            "ground_truth": "0",
            "bug_report": "Services mysteriously disappear after Stack upgrade\nServices mysteriously disappear after Stack upgrade",
            "id": "3981"
        },
        "3982": {
            "ground_truth": "0",
            "bug_report": "Background operations window  called from wizard doesn't react to 'Do not show this dialog...' flag\nSTR:Go through the Reassign NameNode wizard.On the last step click on the Start All Services link.Change the state of flag Do not show this dialog again when starting a background operation.Click OK.Click Start All Services link again.Result: State of flag was not changed.",
            "id": "3982"
        },
        "3984": {
            "ground_truth": "0",
            "bug_report": "Config Groups: Background popup show up needs to be integrated when restarting components\n1. Change config group for a service.2. Click 'Stop components' on service config page.3. Click 'Start components' on the same page.Actual results:Background Operations popup will always show up. (as attached)Expected results:Load the 'do not show this dialog..' flag first  then determine if show this popup.",
            "id": "3984"
        },
        "3986": {
            "ground_truth": "0",
            "bug_report": "YARN and MapReduce2 configs is not displayed\nIn Services -&gt; Configs YARN and MapReduce2 configs is not displayed.",
            "id": "3986"
        },
        "3987": {
            "ground_truth": "0",
            "bug_report": "Resource providers are set with wrong stack version.\nAbstractProviderModule.updateClusterVersion sets the cluster version for the resource providers with the following ... PropertyHelper.MetricsVersion version = clusterVersion.startsWith('HDP-1') ? PropertyHelper.MetricsVersion.HDP1 : PropertyHelper.MetricsVersion.HDP2;So  the Cluster/version property set to 'HDPLocal-1.3.2' will incorrectly be detected as HDP2. This causes the property providers to use the wrong metric mapping files which causes many JMX properties not to be set properly.",
            "id": "3987"
        },
        "3991": {
            "ground_truth": "0",
            "bug_report": "Manage config group links needed in save config-group confirmation\nWhen any service config-group is saved  we have a confirmation popup saying save was successful. We should enhance that popup to have a button to Manage Config Groups dialog  along with appropriate message. When button is clicked  the popup should go away and the Manage Config Groups dialog should show.",
            "id": "3991"
        },
        "3992": {
            "ground_truth": "0",
            "bug_report": "After making config changes w/o saving  prompt user if they try to navigate away\n1) Browse to Services &gt; HDFS &gt; Configs2) Change some props3) Do not click save4) Browse away  to Summary or to another serviceUser would have lost config changes. We should prompt before allowing user to navigate away from Configs.'You have unsaved changes. Save changes or discard?'&#91;Discard&#93; &#91;Save&#93;",
            "id": "3992"
        },
        "3997": {
            "ground_truth": "0",
            "bug_report": "Config-Group POST call should tolerate name reuse\nCluster wide we prohibit reuse of config-group name. However names can be reused across services. The API should be updated to tolerate POST/PUT of similar named config-groups.",
            "id": "3997"
        },
        "3999": {
            "ground_truth": "0",
            "bug_report": "Long host names are inconvenient for viewing in background operations popup\nSteps: Open background operations window and select any operation.Result: If the host name is too long  they are not placed on designated place.Solution:If the host name is too long  we show part of the string with '...' at the end.Also the string should keep in a single line all the time",
            "id": "3999"
        },
        "4003": {
            "ground_truth": "0",
            "bug_report": "Add Service Wizard: Customize Services configs are not displayed.\nIn 'Customize services' step in 'Add Service Wizard' config group and configs are not displayed. See screenshot.",
            "id": "4003"
        },
        "4004": {
            "ground_truth": "0",
            "bug_report": "Duplicate hosts after closing addServiceWizard\nInstall cluster with 2 hosts (with HDFS  ZooKeeper).Go to Add Service Wizard.Select some configurable service.Go to Step 4 (Customize services).Close wizard.Go to Hosts page.Result:each host appears two times.",
            "id": "4004"
        },
        "4012": {
            "ground_truth": "0",
            "bug_report": "NameNode max heap is not showing in HDP 1.3.2 stack\nReason:We use in-consistent value to show Heap size for different components.And some of them are missing.Solution:Make sure all Heap size percentage value (including NN  RM  JT and HBase Master) use the same property.",
            "id": "4012"
        },
        "4024": {
            "ground_truth": "0",
            "bug_report": "HiveSchema file for Hive should be hive-schema-0.12.0.oracle.sql\nHiveSchema file for Hive should be hive-schema-0.12.0.oracle.sql",
            "id": "4024"
        },
        "4028": {
            "ground_truth": "0",
            "bug_report": "Dashboard quick links polls desired_configs every 6s\nOn the dashboard  calls to /clusters/{clusterName}?fields=Clusters/desired_configs are made every 6s. We need to verify if this really is necessary. Initial investigation revealed calls from quick-links  where links were set based on security being enabled. But there is no need to poll every 6s for this.",
            "id": "4028"
        },
        "4040": {
            "ground_truth": "0",
            "bug_report": "In installer  behavior of actions in manage config-groups dialog different from reconfigure\nDuring reconfigure  in the Manage Config Groups dialog  the actions below the config-groups table (left table - Add/Remove/Duplicate/Rename) are immediate - you do not need to hit Save. Save is only for host membership changes. If host membership changes  the actions under left-table are disabled till Save. During install however  all actions are allowed till Save is hit. If you rename/duplicate and hit Cancel  all changes are lost - something which does not happen during reconfigure.The installer dialog should have similar behavior to reconfigure dialog.",
            "id": "4040"
        },
        "4044": {
            "ground_truth": "0",
            "bug_report": "Refactor templates and popups\nRefactor templates and popups",
            "id": "4044"
        },
        "4048": {
            "ground_truth": "0",
            "bug_report": "Minor manage-config-group dialog UI changes\nLabel on top of left hand config-group table should be removed Left hand table and right hand table should occupy 1/3 and 2/3 of the dialog.",
            "id": "4048"
        },
        "4052": {
            "ground_truth": "0",
            "bug_report": "Rename config-group dialog should allow only description change also\nWe wanted to change just the config-group description  but were unable to change without changing the name. The rename config-group dialog should allow changing description only also.",
            "id": "4052"
        },
        "4054": {
            "ground_truth": "0",
            "bug_report": "Need to show stale-config indicator on hosts page\nWe show the restart indicator for stale-configs on services and individual host. Since we already have that information on the client  we need to show that on the Hosts page table. We need a filter to select stale-config hosts  and also show beside each host the stale-config indicator.",
            "id": "4054"
        },
        "4058": {
            "ground_truth": "0",
            "bug_report": "ambari-agent/server should start automatically upon reboot\nambari-agent and server are not set to start automatically  so if a machine reboots it becomes inaccessible.",
            "id": "4058"
        },
        "4069": {
            "ground_truth": "0",
            "bug_report": "Add hosts  if using Local Repository  UI incorrectly says 'no'\nOn review page of install wizard/add host wizard -List the repos with their OS-Say 'Repositories'  not 'Local Repository'-Not a yes or no",
            "id": "4069"
        },
        "4076": {
            "ground_truth": "0",
            "bug_report": "Installer's host list doesn't show masters at top\nInstaller's host list should show masters at top",
            "id": "4076"
        },
        "4089": {
            "ground_truth": "0",
            "bug_report": "HDFS/ZKFC relations in EmberData\nAfter enabling HA go to hosts page.Open host's page for host that has ZKFC.ZKFC doesn't have service.Expect:ZKFC should have HDFS as service.",
            "id": "4089"
        },
        "4092": {
            "ground_truth": "0",
            "bug_report": "Need tooltip showing error why local repo is bad\nI selected HDP 2.0.8 stack and Centos5 base-url was bad. The reason was given in the response - but it is not shown anywhere in UI. On the red exclamation mark we need a tooltip showing the error message from server along with HTTP error code.",
            "id": "4092"
        },
        "4104": {
            "ground_truth": "0",
            "bug_report": "TestHardware.test_fqdnDomainHostname() fails\nThe unit test failsFAIL: test_fqdnDomainHostname (TestHardware.TestHardware)----------------------------------------------------------------------Traceback (most recent call last): File '/home/dmitry/incubator-ambari/ambari-common/src/test/python/mock/mock.py'  line 1199  in patched return func(*args  **keywargs) File '/home/dmitry/incubator-ambari/ambari-agent/src/test/python/ambari_agent/TestHardware.py'  line 83  in test_fqdnDomainHostname self.assertEquals(result['hostname']  'ambari')AssertionError: 'dmitry-pc' != 'ambari'",
            "id": "4104"
        },
        "4106": {
            "ground_truth": "0",
            "bug_report": "Should not allow blank config group name\n1) Create config group2) just enter a &lt;space&gt; for the name3) That enables the OK button to save4) You can save that config group w/o a name.is reproducible on installer (works fine after installation)",
            "id": "4106"
        },
        "4118": {
            "ground_truth": "0",
            "bug_report": "Manage Config Group link appears on host detail page (and cannot dismiss it once opened)\nManage Config Group link appears on host detail page (and cannot dismiss it once opened)",
            "id": "4118"
        },
        "4129": {
            "ground_truth": "0",
            "bug_report": "HA wizard not accessible after upgrade\nHA wizard not accessible after upgrade",
            "id": "4129"
        },
        "4132": {
            "ground_truth": "0",
            "bug_report": "Stale config indicator not shown when reconfiguring WebHCat  Ganglia  Nagios  ZooKeeper\nSteps: Create a config group for any of the mentioned services and add a host to the config group. Save the config group.Result:Restart indicators do not appear.",
            "id": "4132"
        },
        "4135": {
            "ground_truth": "0",
            "bug_report": "Review page doesn't have info about Repositories\nLoad Repositories info in the same way as for add Host Wizard.",
            "id": "4135"
        },
        "4145": {
            "ground_truth": "0",
            "bug_report": "Ability to restart a component\nAdd option to Actions menu for 'restart'. That should queue the stop and start tasks.We should assume 'RESTART' as a default command. The base/default implementation is to call STOP and then START.",
            "id": "4145"
        },
        "4152": {
            "ground_truth": "0",
            "bug_report": "Service Config page shows a blank page after enabling security (JS error)\nUI don't respond after enabling security",
            "id": "4152"
        },
        "4171": {
            "ground_truth": "0",
            "bug_report": "Fix UI Unit tests\nFix UI Unit tests",
            "id": "4171"
        },
        "4177": {
            "ground_truth": "0",
            "bug_report": "YARN check execute fails on install\nYARN check execute fails on install. See attached logs.IP Address on Hosts Page is shown as OS NOT SUPPORTED (w/o js errors  cluster installed using regular Nano-CentOS5.9 image type)",
            "id": "4177"
        },
        "4184": {
            "ground_truth": "0",
            "bug_report": "Fix versions of rpms in the stack to match the installed ones.\nFix versions of rpms in the stack to match the installed ones.",
            "id": "4184"
        },
        "4186": {
            "ground_truth": "0",
            "bug_report": "Global configs are not sent to server\nOn the Deploy step Global Configs are not sent to server.So  Nagios and Ganglia (if installed via add service wizard) don't have any configs in service config page.",
            "id": "4186"
        },
        "4195": {
            "ground_truth": "0",
            "bug_report": "Misalignment of 'Filter' combobox on installer when browser window is narrow\nMisalignment of 'Filter' combobox on installer when browser window is narrow",
            "id": "4195"
        },
        "4200": {
            "ground_truth": "0",
            "bug_report": "Minor UI cleanup - remove double-borders  reduce text\nRemove double borders on alerts and host component page sections Remove some text to not wrap in 'delete host' dialog Remove 'Hosts' table header to clean-up config group dialog",
            "id": "4200"
        },
        "4206": {
            "ground_truth": "0",
            "bug_report": "Significant lag between host status update and slave/master component start/stop\nSteps: Go to any host with slave component (DataNode  for example). Click 'Stop' options for slave component.Result: status marker for slave component start blinking red  but status marker for host behaves strangely: sometimes it also start blinking red immediately/ sometimes changes status to not-blinking red only after slave component stopping will be ended.The same for slave component starting.",
            "id": "4206"
        },
        "4207": {
            "ground_truth": "0",
            "bug_report": "Frontend: History server should be managed as separate component\nFor stack 1.x history server will be as separate master component. Add ability to choose host on which this component will be installed. Set value for 'mapreduce.history.server.http.address'  i.e. &lt;historyserverHost&gt;:&lt;historyServerPort&gt;.",
            "id": "4207"
        },
        "4211": {
            "ground_truth": "0",
            "bug_report": "Empty content of review step in Add Host wizard\nJS error: Uncaught TypeError: Cannot call method 'forEach' of undefined",
            "id": "4211"
        },
        "4217": {
            "ground_truth": "0",
            "bug_report": "Mirroring page redesign\nRedesign existing Mirroring page with datasets table according to new attached mockups.",
            "id": "4217"
        },
        "4220": {
            "ground_truth": "0",
            "bug_report": "Padding for config override properties\nPadding for config override properties",
            "id": "4220"
        },
        "4224": {
            "ground_truth": "0",
            "bug_report": "When issuing Start/Stop of host components then predicate stale_config=true does not work\nSee the series of curl calls below.There are no components with stale configs but 4 components are being stopped. There are only 4 because Ambari only allows INSTALL call on already installed clients[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true'  'items' : [ ]}[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari'-d '{'HostRoles': { 'state': 'INSTALLED'}}' -X PUT http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14'  'Requests' : { 'id' : 14  'status' : 'InProgress' }}[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14'  'Requests' : { 'aborted_task_count' : 0  'cluster_name' : 'c1'  'completed_task_count' : 0  'failed_task_count' : 0  'id' : 14  'progress_percent' : 9.0  'queued_task_count' : 4  'request_context' : ''  'request_status' : 'PENDING'  'task_count' : 4  'timed_out_task_count' : 0 }  'tasks' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/178'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 178  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/179'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 179  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/180'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 180  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/181'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 181  'request_id' : 14 } } ]}",
            "id": "4224"
        },
        "4225": {
            "ground_truth": "0",
            "bug_report": "Starting/Stopping components based on restart indicator floods request history and execution queue\nWhen starting/stopping components based on the Start/Stop Components buttons that show up after service reconfiguration  it creates one request per component (as shown in the Background Operations popup). This floods the request history and the user cannot see what has been done prior. From the user's standpoint  Start Components and Stop Components actions should each show up as one request.Also  this has major performance implications  since multiple requests cannot be processed in parallel by the server (unlike tasks within a single request).",
            "id": "4225"
        },
        "4231": {
            "ground_truth": "0",
            "bug_report": "Storm: Update Dashboard / Services to support Storm\nDefine mock data and make this functional in App.testMode.E2E integration will be a separate task.",
            "id": "4231"
        },
        "4247": {
            "ground_truth": "0",
            "bug_report": "Restart marker does not show up sometimes in the Hosts page\nAfter reconfiguring  restart indicators do not show up on the Host pages sometimes.Clicking on an individual host shows the restart indicator in the Host Details page.",
            "id": "4247"
        },
        "4265": {
            "ground_truth": "0",
            "bug_report": "Add env AMBARI_JVM_ARGS to ambari-server start\nUseful if you want to set AMBARI_JVM_ARGS in the environment. For example: Setting http proxy that Ambari Serverexport AMBARI_JVM_ARGS='-Dhttp.proxyHost=the.proxy.host -Dhttp.proxyPort=1234'ambari-server start",
            "id": "4265"
        },
        "4279": {
            "ground_truth": "0",
            "bug_report": "Status commands are not executed for new services\nFor new services (those  that are not hardcoded at LiveStatus.py)  status commands are not executed.",
            "id": "4279"
        },
        "4284": {
            "ground_truth": "0",
            "bug_report": "Alerts  Restart  Maintenance elements in the Hosts filters\nMove this 3 elements to the second line (under All  Healthy...).Also refactor their code-realization.",
            "id": "4284"
        },
        "4299": {
            "ground_truth": "0",
            "bug_report": "Ambari server unit test failure\nResults :Failed tests: testDoWork(org.apache.ambari.server.state.scheduler.BatchRequestJobTest): (..)Tests run: 1265  Failures: 1  Errors: 0  Skipped: 8",
            "id": "4299"
        },
        "4300": {
            "ground_truth": "0",
            "bug_report": "Service tab: growing number of calls to update alerts\nAfter routing by services on Service page number of calls to server(to get Alerts) grows.Also mock json with alerts need to be added.",
            "id": "4300"
        },
        "4306": {
            "ground_truth": "0",
            "bug_report": "Request Schedule status not updated for Point in time execution request\nAPI call:curl -u admin:admin -H 'X-Requested-By:ambari' -i -X POST -d '[{'RequestSchedule':{'batch':[{'requests':[{'order_id' : '1' 'type':'POST' 'uri':'/api/v1/clusters/c1/requests' 'RequestBodyInfo':{'RequestInfo':{'context':'Restart Nagios' 'command':'RESTART' 'service_name':'NAGIOS' 'component_name':'NAGIOS_SERVER' 'hosts':'c6401.ambari.apache.org'}}}]} {'batch_settings':{'batch_separation_in_seconds':120 'task_failure_tolerance':1}}]}}]' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/request_schedulesRequest Schedule:{href: 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/request_schedules/23' RequestSchedule: {batch: {batch_requests: [{order_id: 1 request_type: 'POST' request_uri: '/api/v1/clusters/c1/requests' request_body: '{'RequestInfo':{'context':'Restart Nagios' 'command':'RESTART' 'service_name':'NAGIOS' 'component_name':'NAGIOS_SERVER' 'hosts':'c6401.ambari.apache.org'}}' request_status: 'InProgress' return_code: 202}] batch_settings: {batch_separation_in_seconds: 120 task_failure_tolerance_limit: 1}} cluster_name: 'c1' description: null id: 23 last_execution_status: 'InProgress' schedule: null status: 'SCHEDULED'}}",
            "id": "4306"
        },
        "4312": {
            "ground_truth": "0",
            "bug_report": "Storm: Install wizard. Add master components Storm UI Server  DRPC Server  LogViewer Server\nAdd new master components to Install Wizard -&gt; Assign Masters page.",
            "id": "4312"
        },
        "4316": {
            "ground_truth": "0",
            "bug_report": "Storm: Add Storm UI Server  DRPC Server  LogViewer Server components config categories.\nAdd master components Storm UI Server  DRPC Server  Log Viewer Server. Create config categories for components. Replace exist configs according to categories.",
            "id": "4316"
        },
        "4318": {
            "ground_truth": "0",
            "bug_report": "Service Restart All action cleanup\n1. There is no need to send request to restart clients.2. When user clicks on Restart All actions  components start restarting immediately. But all other actions shows confirmation popup like 'Are you sure?' (Start  Stop  Run Service Check etc).",
            "id": "4318"
        },
        "4319": {
            "ground_truth": "0",
            "bug_report": "Task timeout should be a configurable knob at the ambari-server\nTask timeout is a configurable knob at the ambari-agent.timeout_seconds = 600This opens up the possibility of different timeout value at different agent instances as well as different value between the server and the agent. This can lead to state where server may have timed out the tasks but agent may not have. The other way is benign.This config can be moved to the server and server can hand it off to the agent when agent registers. This also makes it easier to manage the timeout value.",
            "id": "4319"
        },
        "4333": {
            "ground_truth": "0",
            "bug_report": "Incorrect behavior of 'Save' button in 'Manage Configuration Groups' window\nSteps: Go to 'Customize Services' page. Open 'Manage Configuration Groups' window for any service. Add new custom group and assign host to it. Save changes. Open 'Manage Configuration Groups' window again. Select custom group and remove assigned host.Result: 'Save' button is disabled  'Add hosts' also is disabled.Note: after selecting default group 'Save' button become active.Also if after all steps try to add hosts to custom group  'Save' button will not be active.",
            "id": "4333"
        },
        "4336": {
            "ground_truth": "0",
            "bug_report": "Move 1.3.4 stack to 1.3.3 using the python libraries.\nMove 1.3.4 stack to 1.3.3 using the pythin libraries.",
            "id": "4336"
        },
        "4341": {
            "ground_truth": "0",
            "bug_report": "Rename 2.0.8 to 2.1.1 in the stack definition.\nRename 2.0.8 to 2.1.1 in the stack definition.",
            "id": "4341"
        },
        "4349": {
            "ground_truth": "0",
            "bug_report": "Slaves API-calls\nFor Slaves (DataNodes  NodeManagers  RegionServers  or TaskTrackers): Start Stop Restart Decommission Recommission",
            "id": "4349"
        },
        "4354": {
            "ground_truth": "0",
            "bug_report": "HostCleanup should also clean /tmp/hadoop-*\nHostCleanup should also check the following directories (and clean):/tmp/hadoop-*For example  if the /tmp/hadoop-nagios directory is present  but it doesn't have the right ownership/perms for nagios user  the Hive Metastore Nagios alert will occur. I saw this after doing an install on an un-clean machine.Hive Metastore statusCRIT for about a minuteCRITICAL: Error accessing Hive Metastore status [Error creating temp dir in hadoop.tmp.dir /tmp/hadoop-nagios due to Permission denied]An easy way to reproduce this:1) Perform install2) Go to Nagios server machine3) Change perms on /tmp/hadoop-nagios so that the nagios user does not have access4) The Nagios alert will fire",
            "id": "4354"
        },
        "4355": {
            "ground_truth": "0",
            "bug_report": "Add relocate resources scripts to the pom file\nThe relocate resources python script is not a part of the rpm package.",
            "id": "4355"
        },
        "4361": {
            "ground_truth": "0",
            "bug_report": "Rolling restart failure tolerance should be percentage values\nCurrently when we make rolling restart API calls  the task_failure_tolerance value is given as count of hosts. Rather it should be percentage of total hosts.",
            "id": "4361"
        },
        "4365": {
            "ground_truth": "0",
            "bug_report": "Action definitions should be provided as declarative resources - read from XML files\nCurrently  action definition are stored in database. This is not in-line with the declarative definition of stack and custom commands (as part of stack). The custom actions are essentially same as custom commands except they are defined at the level of clusters. So we should move the custom actions to XML formatted files that ambari-server can read when starting up. This means that when one needs to make any edit they will have to restart ambari-server. This requirement is OK as adding/modifying custom actions is not a frequently done operation.",
            "id": "4365"
        },
        "4367": {
            "ground_truth": "0",
            "bug_report": "Alerts block shows spinner if Nagios not installed\nWhen Nagios not installed Alerts block should show message that it's not installed instead of spinner.",
            "id": "4367"
        },
        "4380": {
            "ground_truth": "0",
            "bug_report": "Hosts API-calls\nFor each host:P0: Start All Components Stop All ComponentsP1: Restart All Components",
            "id": "4380"
        },
        "4383": {
            "ground_truth": "0",
            "bug_report": "Datanode data directory is not created correctly\ndfs.datanode.data.dir must be handled as comma separated directories.",
            "id": "4383"
        },
        "4395": {
            "ground_truth": "0",
            "bug_report": "ambari-server should use repo when downloading jdk 7\nambari-server should use repo when downloading jdk 7",
            "id": "4395"
        },
        "4396": {
            "ground_truth": "0",
            "bug_report": "Misc code cleanup\nMisc code cleanup",
            "id": "4396"
        },
        "4402": {
            "ground_truth": "0",
            "bug_report": "Delete Config Group Host mapping broken due to error introduced by perf patch\nUnit test: org.apache.ambari.server.state.ConfigGroupTest#testRemoveHostThis unit test is not a part of 1.4.3 branch  it was added later. (trunk)Exception thrown during ConfigGroupImpl.removeHost()2014-01-06 17:46:35 989 ERROR [main] configgroup.ConfigGroupImpl (ConfigGroupImpl.java:removeHost(274)) - Failed to delete config group host mapping  clusterName = foo  id = 1  hostname = h1java.lang.IllegalArgumentException: Object: org.apache.ambari.server.orm.cache.ConfigGroupHostMappingImpl@cc34948d is not a known entity type. at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.performRemove(UnitOfWorkImpl.java:3538) at org.eclipse.persistence.internal.jpa.EntityManagerImpl.remove(EntityManagerImpl.java:518) at org.apache.ambari.server.orm.dao.ConfigGroupHostMappingDAO.removeByPK(ConfigGroupHostMappingDAO.java:250) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:58) at org.apache.ambari.server.state.configgroup.ConfigGroupImpl.removeHost(ConfigGroupImpl.java:272) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:66) at org.apache.ambari.server.state.cluster.ClustersImpl.deleteConfigGroupHostMapping(ClustersImpl.java:640) at org.apache.ambari.server.state.cluster.ClustersImpl.unmapHostFromCluster(ClustersImpl.java:615) at org.apache.ambari.server.state.ConfigGroupTest.testRemoveHost(ConfigGroupTest.java:203)",
            "id": "4402"
        },
        "4404": {
            "ground_truth": "0",
            "bug_report": "mapreduce.task.io.sort.mb max value should not exceed 1024mb\nhttps://issues.apache.org/jira/browse/MAPREDUCE-5028https://issues.apache.org/jira/browse/MAPREDUCE-2308",
            "id": "4404"
        },
        "4405": {
            "ground_truth": "0",
            "bug_report": "Remove property fs.checkpoint.size during upgrade\nProperty fs.checkpoint.size is deprecated. The upgrade script should remove it. Users can add the replacement themselves - dfs.namenode.checkpoint.txns.",
            "id": "4405"
        },
        "4408": {
            "ground_truth": "0",
            "bug_report": "Background operations dialog in weird state after exception\nBackground operations dialog goes into a weird state after hitting an exception with request_schedule information. It always expects request_schedule information to be present.",
            "id": "4408"
        },
        "4416": {
            "ground_truth": "0",
            "bug_report": "HDFS start failed on 2.1.1 stack\nSTR: Deployed minimal cluster with HDFS and ZK. HDFS start failed. Added YARN+MR2  Nagios and Ganglia. Picture with HDFS was the same.Output:Fail: Execution of 'ulimit -c unlimited &amp;&amp; if [ 'ulimit -c' != 'unlimited' ]; then exit 77; fi &amp;&amp; export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start secondarynamenode' returned 1. -bash: line 0: ulimit: core file size: cannot modify limit: Operation not permittedFull folders with logs are attached.",
            "id": "4416"
        },
        "4420": {
            "ground_truth": "0",
            "bug_report": "ORA-01795: maximum number of expressions in a list is 1000 for Oracle DB\nPROBLEM:ORA-01795: maximum number of expressions in a list is 1000 in Ambari Server log. Customer recently upgraded to Ambari 1.4.2Error is:08:54:51 320 ERROR [qtp1280560314-2070] ReadHandler:84 - Caught a runtime exception executing a queryLocal Exception Stack: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000Error Code: 1795Call: SELECT task_id  attempt_count  event  exitcode  host_name  last_attempt_time  request_id  role  role_command  stage_id  start_time  status  std_error  std_out FROM host_role_command WHERE (task_id IN (? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?)) ORDER BY task_id bind =&gt; [2551 parameters bound]Query: ReadAllQuery(referenceClass=HostRoleCommandEntity sql='SELECT task_id  attempt_count  event  exitcode  host_name  last_attempt_time  request_id  role  role_command  stage_id  start_time  status  std_error  std_out FROM host_role_command WHERE (task_id IN ?) ORDER BY task_id') at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:333) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:646) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:537) at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:1800)STEPS TO REPRODUCE: Over 1000 entries in the host_role_command and execution_command tables when oracle is used for Ambari backend databseACTUAL BEHAVIOR: Oracle throws the errorEXPECTED BEHAVIOR: There should be a limit to prevent this (possibly modify the syntax of the oracle query)",
            "id": "4420"
        },
        "4425": {
            "ground_truth": "0",
            "bug_report": "Add upgradestack support for MySQL\nSimilar to Oracle/Postgres we need to make sure that ambari-server upgradestack works for MySQL",
            "id": "4425"
        },
        "4433": {
            "ground_truth": "0",
            "bug_report": "Ambari version info should be visible in UI\nThere is currently no easy way to find the version of Ambari that is in use from the Ambari Web interface. This info should be accessible via an 'about' link in Ambari Web",
            "id": "4433"
        },
        "4501": {
            "ground_truth": "0",
            "bug_report": "health status yellow (lost heartbeat) not showing icon\nSee attached. Using 1.5.0.335On Hosts page. Other pages seem fine.",
            "id": "4501"
        },
        "4521": {
            "ground_truth": "0",
            "bug_report": "Bulk Ops: Restart on Slaves should popup rolling restart dialog\nWhen performing Restart on Slaves via Bulk Ops menu on the Hosts page  Restart should popup the rolling restart dialog  just like it does when Restarting Slaves under Service Actions.",
            "id": "4521"
        },
        "4523": {
            "ground_truth": "0",
            "bug_report": "Host registering failure from primary/agent os checking on centos6\nI am using Ambari (1.4.3.38) for hadoop cluster installation and management. All the cluster nodes are built on centos 6.0.During the ambari server installation  ambari-server recognized the primary/cluster os as redhat6 (see ambari.properties). During the ambari agent bootstrap/host register  ambari-agent regonized the agent os as centos linux6 (see log). From log files (ambari-server.log  ambari-agent.log)  I found the inconsistence caused the warning of ambari-agent bootstrapping and failure of host registering.I'm still not sure why this happen  but I guess it's caused by the differene of os checking methods among ambari server side code  ambari-agent bootstrap script (os_type_check.sh based on os release file) and registering script (Controller.py/Register.py based on os hardware profile) .I just share to see if anyone can fix the issue.BTW  for me  to solve the problem  I manually edited the script files to make it work temporarily:To avoid warning of agent bootstrapping  in os_type_check.sh  add current_os=$RH6 above the echo line or add res=0 after case statement;To make the node register work  in Controller.py  add data=data.replace('centos linux' 'redhat') before sending registering request;Thanks.",
            "id": "4523"
        },
        "4526": {
            "ground_truth": "0",
            "bug_report": "Oozie Server installation fails when Falcon is selected\nOozie Server installation fails when Falcon is selected",
            "id": "4526"
        },
        "4534": {
            "ground_truth": "0",
            "bug_report": "Add ability to delete individual DataNode  TaskTracker  NodeManager  and RegionServer from Host Details page\nNote: We let the user delete Storm Supervisor already.",
            "id": "4534"
        },
        "4551": {
            "ground_truth": "0",
            "bug_report": "Alert count badge and restart indicator issues\nThe alert badge shown in the left nav shows up a bit strange (too little padding on the right). Also  when the restart indicator appears  the padding for the alert badge fixes itself  but the restart indicator appears too close. See attached.",
            "id": "4551"
        },
        "4552": {
            "ground_truth": "0",
            "bug_report": "OOS status for component on host detail page makes button too big\nOOS status for component on host detail page makes button too big",
            "id": "4552"
        },
        "4556": {
            "ground_truth": "0",
            "bug_report": "Refactor and Unit tests for host summary\nRefactor and Unit tests for host summary",
            "id": "4556"
        },
        "4560": {
            "ground_truth": "0",
            "bug_report": "BG operation pop-up: JS error encountered on clicking on host in progress state.\nBG operation pop-up: JS error encountered on clicking on host in progress state.",
            "id": "4560"
        },
        "4561": {
            "ground_truth": "0",
            "bug_report": "Falcon Client install task shows up as just 'install' rather than 'Falcon Client install'\nSee attached.",
            "id": "4561"
        },
        "4566": {
            "ground_truth": "0",
            "bug_report": "Jobs: implement the TEZ DAG page/panel\nNeed a graph implemented to show the Tez DAG for Hive queries",
            "id": "4566"
        },
        "4567": {
            "ground_truth": "0",
            "bug_report": "Ambari should check that there is enough disk space during installation\nAdd a new category in 'Hosts Check' popup after registering hosts.So for '/' mountpoint  we check if the disk free space is larger than 2.0GB.And for '/usr/lib' or '/usr'  check the free space is larger than 1.0GB. Then show related warning message if any of them got space shortage.",
            "id": "4567"
        },
        "4570": {
            "ground_truth": "0",
            "bug_report": "Various issues related to decommission support\nTracking few issues related to decommission support Even with HBase HA only one master should be used for decommission Alternatively  for HDFS/MR/YARN use all master host components Do not use 'default' method while using python based system resources Add support for command details and custom command names",
            "id": "4570"
        },
        "4574": {
            "ground_truth": "0",
            "bug_report": "Upon restart of ambari-server  the service status on Dashboard page remain unchanged\nAfter ambari-server restart  if FE remains at the Dashboard page  the service status (Yellow buttons) never gets updated even if the API indicates that the status are all Green/Red. In my case  never is 6 minutes.Upon refresh or moving to other tabs the view is promptly updated.",
            "id": "4574"
        },
        "4575": {
            "ground_truth": "0",
            "bug_report": "Host Details > Actions pulldown likes to hide\nActions pull down on the Host Detail page likes to hide itself when it's open. This is a bit annoying.",
            "id": "4575"
        },
        "4600": {
            "ground_truth": "0",
            "bug_report": "Remove --jce-policy from warning statement\nWe have removed option -c or --jce-policy. So the warning message should not use that option in the help statement.jce_download_fail_msg = ' Failed to download JCE Policy archive : {0}. ' / 'Please check that JCE Policy archive is available ' / 'at {1} . Also you may install JCE Policy archive manually using ' / '--jce-policy command line argument.'.format('{0}'  jce_url)",
            "id": "4600"
        },
        "4601": {
            "ground_truth": "0",
            "bug_report": "adding more master components styling is missing\nadding more master components styling is missing",
            "id": "4601"
        },
        "4603": {
            "ground_truth": "0",
            "bug_report": "Host names goes under text  icons\nsee attached",
            "id": "4603"
        },
        "4605": {
            "ground_truth": "0",
            "bug_report": "Host details: clients list disappears\nGo to host page with some clients installedRefresh itGot: clients are not displayed on the pageExpected: list of installed clients",
            "id": "4605"
        },
        "4608": {
            "ground_truth": "0",
            "bug_report": "medkit-icons are shifted out of their places  in hosts table in Safari.\nmedkit-icons are shifted out of their places  in hosts table in Safari.",
            "id": "4608"
        },
        "4612": {
            "ground_truth": "0",
            "bug_report": "Exception on deploing step: java.sql.BatchUpdateException: ORA-00942\nException trace:18:27:19 191 WARN [qtp1643608425-22] ServletHandler:514 - /api/v1/clusters/c1/servicesjavax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.BatchUpdateException: ORA-00942: table or view does not existError Code: 942 at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commitInternal(EntityTransactionImpl.java:102) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:63) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:87)",
            "id": "4612"
        },
        "4635": {
            "ground_truth": "0",
            "bug_report": "'SNN Process' alert displays after HA enabled successfully\nSTR:1. Install  setup and start Ambari server by default.2. Deploy Hadoop by default (stack 2.0.6  choose all services to install).3. Enable HA.4. Wait for at least 150 seconds.4. Go to HDFS Service/Hosts page page.Actual result:'Secondary NameNode Process' alert displayed.Expected result:There should not be 'Secondary NameNode' alert on page.",
            "id": "4635"
        },
        "4637": {
            "ground_truth": "0",
            "bug_report": "The user is not always redirected to the login page when unauthenticated\n1. Try to enable HA with fail on 9th step for Start All services2. Use link http://&lt;host&gt;:8080/#/main/admin/highAvailability/enable/step13. HA Wizard is shown without previously loaded Login Page (see url and js errors in attached screenshot)Expexted result:Login Page should be loaded firts.",
            "id": "4637"
        },
        "4643": {
            "ground_truth": "0",
            "bug_report": "Restart All fails for Client only components\nRestart for a service calls stop and start on all components. Currently there is no implementation for stopping a client component. This leads to error message: 'Stop not implemented for component'",
            "id": "4643"
        },
        "4658": {
            "ground_truth": "0",
            "bug_report": "Routes are incorrect after launching wizard\nSteps to reproduce:1. Launch Add Service wizard2. Close wizard3. Go by link #/main/services/add/step1 (type it in address bar)The router goes to nonexistent page '#/main/services/add/summary'.",
            "id": "4658"
        },
        "4662": {
            "ground_truth": "0",
            "bug_report": "Install Wizard: Assign Masters: some selects disabled\nStep to reproduce.Go untill to Customize Services Page.Click to Choose Services menu link.Go to Assign Masters page.",
            "id": "4662"
        },
        "4673": {
            "ground_truth": "0",
            "bug_report": "Bulk Ops: add Supervisor to Bulk Ops on Hosts page\nAdd Supervisor to Hosts page's Bulk Ops menu if Storm is installed.Decommission and Recommission should be disabled  as Supervisors do not support these operations.",
            "id": "4673"
        },
        "4682": {
            "ground_truth": "0",
            "bug_report": "Customize Services page of Add Service Wizard offers to customize already installed Oozie\nSteps to reproduce: Deploy cluster without Ooozie and some other customizable service. Go to the Add Service wizard. Add Oozie service. Fail starting Oozie server. Close add service wizard. Ensure that Oozie was added as service. Go again to Add Service Wizard and choose some customizable service. Go to Customize Services page.Result: Customize Services page proposes to customize Oozie  but it was already added to the cluster during installation.",
            "id": "4682"
        },
        "4687": {
            "ground_truth": "0",
            "bug_report": "Write unnitests for HDFS install script on HDP1 and HDP2\nWrite unnitests for HDFS install script on HDP1 and HDP2",
            "id": "4687"
        },
        "4693": {
            "ground_truth": "0",
            "bug_report": "Hosts table: sort order arrows should be close to the respective column label\nThe sort order arrows are right-justified within the column.This makes it look like the arrows apply to the column next to it.Instead  we should show the arrows right next to the respective column label.Like:Name (arrows) IP Address (arrows)Not:Name (arrows) IP Address",
            "id": "4693"
        },
        "4700": {
            "ground_truth": "0",
            "bug_report": "Oozie tests fails\nOozie tests fail",
            "id": "4700"
        },
        "4708": {
            "ground_truth": "0",
            "bug_report": "Actual configs not updated after restart of host component\nUpdated HDFS configs aren't applied after RESTART.Steps: On a 3 node cluster  create a ConfigGroup for datanode. Override heap size to 1025m instead of 1024m Restart DN.Result: The /var/lib/ambari-agent/data/config.json  has the correct values for the config type:'global': {'2': 'version1392171779044'  'tag': 'version1'} The API call still shows global as default version:http://hostname1:8080/api/v1/clusters/c1/hosts/hostname1/host_components/DATANODE Note:It works after agent is restarted.{global: {overrides: {2: 'version1392171779044'} default: 'version1'}The cause is that hooks aren't executed for the custom_command like RESTART. Since configs for HDFS are generated in hook.py  we must execute hook.py before custom commands or move config generation from hook.py.",
            "id": "4708"
        },
        "4710": {
            "ground_truth": "1",
            "bug_report": "Add unittets for hooks in secured mode.\nAdd unittets for hooks in secured mode.",
            "id": "4710"
        },
        "4737": {
            "ground_truth": "0",
            "bug_report": "Falcon Server can not be restarted\nFalcon Server can not be restarted",
            "id": "4737"
        },
        "4741": {
            "ground_truth": "0",
            "bug_report": "Alerts for ATS Component\nImpl alert for ATS server ATS process (running / not running)Note: this alert should be disabled/removed when ATS gets deleted (when Kerb is enabled).",
            "id": "4741"
        },
        "4745": {
            "ground_truth": "0",
            "bug_report": "Value 'storm.zookeeper.servers' not changing after adding new ZK server\nAfter adding new ZK server needs to change value of storm.zookeeper.servers property.",
            "id": "4745"
        },
        "4750": {
            "ground_truth": "0",
            "bug_report": "Tez DAG UI not showing due to changed ATS responses\nATS has changed structure of http://server:8188:8188/ws/v1/apptimeline/HIVE_QUERY_ID/&lt;id&gt; where the entire query JSON structure is now represented as a string under 'otherinfo/query'. UI will need to deserialize this string back into JSON and continue.",
            "id": "4750"
        },
        "4764": {
            "ground_truth": "0",
            "bug_report": "AmbariManagementControllerTest Test fails with unable to delete the last user.\nAmbariManagementControllerTest Test fails with unable to delete the last user.Results :Tests in error: testDeleteUsers(org.apache.ambari.server.controller.AmbariManagementControllerTest): Could not remove user user1. System should have at least one user with administrator role. Tests run: 1404  Failures: 0  Errors: 1  Skipped: 7",
            "id": "4764"
        },
        "4772": {
            "ground_truth": "0",
            "bug_report": "Security Wizard: History Server should be a different section for MR service.\nEarlier History Server was not a different service component and was always co-hosted with JobTracker for HDP-1.x. So we had a same section for Job Tracker and History server in security wizard config page.After AMBARI-2617 and AMBARI-4207 fix  it's possible to have JobTracker and History Server on different host via Ambari web-ui. With this capability it's important to show Job History Server as a different section in Security Wizard MR service config page.",
            "id": "4772"
        },
        "4777": {
            "ground_truth": "0",
            "bug_report": "Restart indicators work incorrectly after adding component\nSTR: Deploy cluster with DataNodes on 2 from 3 hosts. Change DataNode maximum Java heap size from 1024 to 1025. Check that Restart indicators appeared and 2 DataNodes require restart. Add DataNode on missing host. Change property DataNode maximum Java heap size back to 1024.Result: Those two DataNodes still require restart and added DataNode not.Gluster properties shouldn't be added unless GLUSTERFS is installed.",
            "id": "4777"
        },
        "4779": {
            "ground_truth": "0",
            "bug_report": "Add services wizard throw JS exception\nSee attached. During Customize Services.I installed a cluster w/o Storm  and went to add Storm.",
            "id": "4779"
        },
        "4787": {
            "ground_truth": "0",
            "bug_report": "/var/lib/hadoop-hdfs/ location does not has +x permission for others\nThe file defined by dfs.domain.socket.path must give +x permission for other user. &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt; &lt;/property&gt;Currently  In ambari installed cluster  /var/lib/hadoop-hdfs does not give +x permission to other user[root@ambari-sec-1392876050-hdfs-re-8 ~]# stat /var/lib/hadoop-hdfs/ File: '/var/lib/hadoop-hdfs/' Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: 803h/2051d Inode: 1182008 Links: 3Access: (0750/drwxr-x---) Uid: ( 1005/ hdfs) Gid: ( 500/ hadoop)Access: 2014-02-18 18:10:35.000000000 -0800Modify: 2014-02-20 07:50:55.274766162 -0800Change: 2014-02-20 07:50:55.274766162 -0800Due to this Issue  hadoop commands are seeing below WARN messages. 2014-02-18 05:54:32 734|beaver.machine|INFO|RUNNING: /usr/bin/hdfs dfs -tail /user/hrt_qa/hdfsRegressionData/smallFiles/smallRDFile7552014-02-18 05:54:35 528|beaver.machine|INFO|14/02/18 05:54:35 WARN hdfs.BlockReaderLocal: error creating DomainSocket2014-02-18 05:54:35 528|beaver.machine|INFO|java.net.ConnectException: connect(2) error: Permission denied when trying to connect to '/var/lib/hadoop-hdfs/dn_socket'2014-02-18 05:54:35 528|beaver.machine|INFO|at org.apache.hadoop.net.unix.DomainSocket.connect0(Native Method)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.net.unix.DomainSocket.connect(DomainSocket.java:250)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.DomainSocketFactory.createSocket(DomainSocketFactory.java:158)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.BlockReaderFactory.nextDomainPeer(BlockReaderFactory.java:691)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:439)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.client.ShortCircuitCache.create(ShortCircuitCache.java:669)The expected Permissions on this location is as below.[root@ambari-sec-1392876050-yarn-10 ~]# stat /var/lib/hadoop-hdfs/ File: '/var/lib/hadoop-hdfs/' Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: 803h/2051d Inode: 1181767 Links: 3Access: (0751/drwxr-x--x) Uid: ( 1005/ hdfs) Gid: ( 500/ hadoop)Access: 2014-02-20 18:00:06.586040913 -0800Modify: 2014-02-20 07:06:28.267889888 -0800Change: 2014-02-20 17:59:56.629052410 -0800",
            "id": "4787"
        },
        "4788": {
            "ground_truth": "0",
            "bug_report": "NameNode fails to start due to 'fs.defaultFS' being null\nNameNode fails to start due to 'fs.defaultFS' being null",
            "id": "4788"
        },
        "4790": {
            "ground_truth": "0",
            "bug_report": "Skip Failing tests for now.\nSkip Failing tests for now.",
            "id": "4790"
        },
        "4794": {
            "ground_truth": "0",
            "bug_report": "Reconfiguring memory related properties of a service suffixes 'm' to memory related properties of other service.\nReconfiguring memory related properties of a service suffixes 'm' to memory related properties of other service.",
            "id": "4794"
        },
        "4796": {
            "ground_truth": "0",
            "bug_report": "Do not automatically put host component in Maintenance Mode upon decommissioning (and out of Maintenance Mode when recommissioning)\nWe originally wanted to couple decom/recom with putting the host component in / out of maintenance mode.After experimenting  we decided to undo that. This is the JIRA for the Ambari BE changes.",
            "id": "4796"
        },
        "4800": {
            "ground_truth": "0",
            "bug_report": "Hosts table UI cleanup\n1) has too much padding (See attached.)2) the sorting carets should have more padding-left so there is a bit more space between the column label3) the checkboxes should have more left padding. They are not balanced.4) The checkboxes and status icons are not vert centered with the hostname text.5) The input field for searching the hostname column should take up more horizontal space. with all that blank  makes it look like there is a missing column.",
            "id": "4800"
        },
        "4809": {
            "ground_truth": "0",
            "bug_report": "Allow Falcon to be configured with keytab/security and custom params\nAllow Falcon to be configured with keytab/security and custom params",
            "id": "4809"
        },
        "4818": {
            "ground_truth": "0",
            "bug_report": "Do not show 'restart' op on non-admin user\n1) As admin  change a config but do not perform the restarts2) Create a test user (non-admin)3) login as test user4) in host details page  operation to restart is shown. Should not be shown",
            "id": "4818"
        },
        "4821": {
            "ground_truth": "0",
            "bug_report": "Navigation from Hosts page to HostDetailsPage breaks occasionally\nSTR: 1. Navigate to hosts page  wait for page loaded. 2. Click specific hosts link. 3. Wait for HostDetails page loaded. 4. Click Back. 5. Iterate. Actual result: sometimes the Hosts page hangs not resulting in presenting a HostDetails page. Speed of attached videos is 5 times faster than real.",
            "id": "4821"
        },
        "4841": {
            "ground_truth": "0",
            "bug_report": "Incorrect behavior of HA wizard on second step\nSTR:1) Deploy cluster by default;2) Go to HA wizard3) Second wizard stepExpected result:1) In 'Additional NameNode' combobox should NOT be ability to choose host where NameNode component already installed.2) In 'JournalNode' comboboxes should NOT be ability to choose more than one JournalNode on one hostActual results:1) In 'Additional NameNode' combobox can be chosen host with already installed NameNode (see first screenshot)2) JournalNode components might be chosen to install for any host  including case when three JournalNode's might be installed on one host (see second screenshot).",
            "id": "4841"
        },
        "4842": {
            "ground_truth": "0",
            "bug_report": "Update falcon install scripts to recent changes\nFollowing features must be implemented: /apps/falcon directory on hdfs and owned by falcon user make falcon able to be runned from custom user change alerts text",
            "id": "4842"
        },
        "4843": {
            "ground_truth": "0",
            "bug_report": "Ambari DDL for MySQL should not create ambarirca database\nAmbari-DDL-MySQL-CREATE.sql includes 'create ambarirca' database.CREATE DATABASE ambarirca;USE ambarirca;1) At minimum  this DDL should not be creating an ambarirca database and instead mix in the RCA tables with core Ambari tables (that's how Oracle DDL does it &#8211; I think). Not ideal but better than creating an ambarirca database in this script (a user would be surprised to see this).2) Alternatively  we need to split out RCA DDL from the core Ambari. We can doc that if you plan to use HDP 1.3.x stack that you need to to setup an RCA database (since RCA is only applicable to those with HDP 1.3.x stack). If we go this route  for consistency  we would need to do the same for the oracle DDL.Note: Right now  looks like the oracle DDL just puts the RCA tables in the same database as ambari core tables (basically #1 above).",
            "id": "4843"
        },
        "4872": {
            "ground_truth": "0",
            "bug_report": "Nagios alerts are not shown on SUSE\nSTR:1. Deploy cluster by default scenario w/o Storm and Falcon.2. Go to HDFS service page.3. Go to Nagios service page.4. Navigate to Nagios Web UI.5. Enter credentials nagiosadmin/passwordActual results: There are no alerts on service page. Web UI is unavailable due to ERROR 403.Screenshots attached.",
            "id": "4872"
        },
        "4893": {
            "ground_truth": "0",
            "bug_report": "Avoid printing stacktrace for state machine exceptions\nLog gets filled on install failure.13:28:08 856 WARN [qtp615964260-72] HeartBeatHandler:361 - State machine exceptionorg.apache.ambari.server.state.fsm.InvalidStateTransitionException: Invalid event: HOST_SVCCOMP_OP_SUCCEEDED at INSTALL_FAILED at org.apache.ambari.server.state.fsm.StateMachineFactory.doTransition(StateMachineFactory.java:297) at org.apache.ambari.server.state.fsm.StateMachineFactory.access$300(StateMachineFactory.java:39) at org.apache.ambari.server.state.fsm.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:440) at org.apache.ambari.server.state.svccomphost.ServiceComponentHostImpl.handleEvent(ServiceComponentHostImpl.java:730) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:66)",
            "id": "4893"
        },
        "4895": {
            "ground_truth": "0",
            "bug_report": "License header is repeated in oozie-log4j.properties\n# Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License  Version 2.0 (the 'License'); you may not use this file except in compliance with the License. You may obtain a copy of the License at##http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an 'AS IS' BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied. See the License for the specific language governing permissions and limitations under the License.# http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an 'AS IS' BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.#",
            "id": "4895"
        },
        "4902": {
            "ground_truth": "0",
            "bug_report": "Service Check does not work\nService Check does not work",
            "id": "4902"
        },
        "4909": {
            "ground_truth": "0",
            "bug_report": "Slave component should include 'restart' command on host details page 'Actions'\nSlave components (such as DataNode  NodeManager  RegionServer  Supervisor  Ganglia Monitor) should include a 'Restart' command in their 'Actions' menu on the host details page.The Restart option should be shown and enabled when the component is started.",
            "id": "4909"
        },
        "4939": {
            "ground_truth": "0",
            "bug_report": "Ganglia alerts after adding YARN+MR2\nSTR: Deploy cluster with HDFS+ZK  Nagios  Ganglia. Add YARN+MR2  Tez services.Result: Alerts Ganglia Monitor process for HistoryServer and Ganglia Monitor process for ResourceManager don't dissappear after restarting all services",
            "id": "4939"
        },
        "4947": {
            "ground_truth": "0",
            "bug_report": "Change Hive alerts to move away from Hive metadata queries to port checks\n1) remove 'Hive Metastore status' alert2) add 'Hive Metastore process' alert3) add 'HiveServer2 process' alert",
            "id": "4947"
        },
        "4954": {
            "ground_truth": "0",
            "bug_report": "After configuring NNHA  nn process alerts don't work\n1) Configure NN HA2) stack 2.0.6 (but check stack 2.1 as well)3) Two alerts show' check_tcp: Port must be a positive integer'NameNode process on c6401.ambari.apache.orgNameNode process on c6402.ambari.apache.org4) Looked at /etc/nagios/objects/hadoop-services.cfg5) Saw:define service { host_name c6401.ambari.apache.org use hadoop-service service_description NAMENODE::NameNode process on c6401.ambari.apache.org servicegroups HDFS check_command check_tcp_wrapper!//test!-w 1 -c 1 normal_check_interval 0.5 retry_check_interval 0.25 max_check_attempts 3}define service { host_name c6402.ambari.apache.org use hadoop-service service_description NAMENODE::NameNode process on c6402.ambari.apache.org servicegroups HDFS check_command check_tcp_wrapper!//test!-w 1 -c 1 normal_check_interval 0.5 retry_check_interval 0.25 max_check_attempts 3}Notice in the above //test is the name of my nameservice.Attaching screen shot of my config. So looks like it's grabbing port from the wrong prop.",
            "id": "4954"
        },
        "4970": {
            "ground_truth": "0",
            "bug_report": "Jobs popup message clicking continues to stay on the same page\n1. Click on an app on ATS/jobs page which is running 2. It pops up a window with message 'Tez DAG has no ID associated with name hrt_qa_20140228161212_a5713292-8213-43a3-b61e-06685799a5b3'3. Press OK and the page refreshes and pops up the same window again.4. This goes on forever until the user closes the browser or enters a different URLInstead  the page should be redirected back to &lt;ambari server URL&gt;/#/main/jobs",
            "id": "4970"
        },
        "4983": {
            "ground_truth": "0",
            "bug_report": "During upgrade. migrate decommissioned DN hosts list to the new format\nThe details of how the notion of a decommissioned DN is stored has changed for 1.5.0. Add support to modify persisted data when Ambari is upgraded to 1.5.0.",
            "id": "4983"
        },
        "4986": {
            "ground_truth": "0",
            "bug_report": "Reduce loading time of Host Warnings popup\nWizard-&gt;Confirm Hosts step:1. Refactor parsing of json response with hosts warnings.2. Reduce latency on opening Host Warnings popup.",
            "id": "4986"
        },
        "5020": {
            "ground_truth": "0",
            "bug_report": "Lost heartbeat on host but ganglia shows a heartbeat lost\n3 hosts  third-host has only Ganglia monitor and Datanode.I kill the third machine agent. Hosts page correctly shows heartbeat lost.On the services page  ganglia shows 'yellow'  even though the ganglia server host is fine.Ganglia service should considered started if Ganglia Server is started.",
            "id": "5020"
        },
        "5028": {
            "ground_truth": "0",
            "bug_report": "Hive Service Check Failed during Install Wizard\nHive Service Check Failed during Install Wizard",
            "id": "5028"
        },
        "5036": {
            "ground_truth": "0",
            "bug_report": "Secured: Start All Services task got stuck forever\nDeployed 2-node cluster. Added 3rd node. Enabled security.After steps above on all 3 hosts tasks jammed and don't want to perform or fail for a very long time.VMs are alive  ambari-server and all ambari-agents are running.Finally got a reproduce using 2 commandscurl 'http://vm-0.vm:8080/api/v1/clusters/cc/services?params/run_smoke_test=false' -X PUT -H 'X-Requested-By: X-Requested-By' -u admin:admin --data '{'RequestInfo': {'context': 'Start All Services'}  'Body': {'ServiceInfo': {'state': 'STARTED'}}}' ; sleep 3; curl 'http://vm-0.vm:8080/api/v1/clusters/cc/hosts/vm-0.vm/host_components/APP_TIMELINE_SERVER' -X DELETE -H 'X-Requested-By: X-Requested-By' -u admin:adminThe way to reproduce is a bit different compared to an original description (I issue a DELETE request in 3 seconds after START_ALL_SERVICES request has been issued)  but the symptoms are the same: ServiceComponentHostNotFoundException exception is posted to log and operation is stuck on stage that contains 'App Timeline Server Start' command.",
            "id": "5036"
        },
        "5040": {
            "ground_truth": "0",
            "bug_report": "2-way auth fails when using jdk7\nSteps to reproduce:On the Ambari Server host  open /etc/ambari-server/conf/ambari.properties with a text editor.Add the following property:security.server.two_way_ssl = trueError messageINFO 2014-03-07 13:57:17 184 security.py:184 - Agent certificate not exists  sending sign requestINFO 2014-03-07 13:57:17 335 security.py:89 - SSL Connect being called.. connecting to the serverERROR 2014-03-07 13:57:17 414 security.py:76 - Two-way SSL authentication failed. Ensure that server and agent certificates were signed by the same CA and restart the agent. In order to receive a new agent certificate  remove existing certificate file from keys directory. As a workaround you can turn off two-way SSL authentication in server configuration(ambari.properties) Exiting..",
            "id": "5040"
        },
        "5043": {
            "ground_truth": "0",
            "bug_report": "oozie-site.xml defaults need to be updated for 2.1 stack\noozie-site.xml needs the following two services added to the list of services:org.apache.oozie.service.XLogStreamingService (needed to display logs)org.apache.oozie.service.JobsConcurrencyService (needed to run Recovery Service - for example  this would handle jobs that are dangling and stuck in RUNNING state)",
            "id": "5043"
        },
        "5051": {
            "ground_truth": "0",
            "bug_report": "Start all services silently fails when a service is not startable\nI clicked on Start All services button and nothing happened. Turns out that on the API call  the server throws a 500 exception that is silently lost. We should show in a dialog the error response from server. Similarly for Stop All action.PUT http://c6401:8080/api/v1/clusters/c1/services?params/run_smoke_test{'RequestInfo': {'context' :'_PARSE_.START.ALL_SERVICES'}  'Body': {'ServiceInfo': {'state': 'START{ 'status' : 500  'message' : 'org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Invalid transition for servicecomponenthost  clusterName=c1  clusterId=3  serviceName=OOZIE  componentName=OOZIE_SERVER  hostname=c6402.ambari.apache.org  currentState=INSTALL_FAILED  newDesiredState=STARTED'}",
            "id": "5051"
        },
        "5060": {
            "ground_truth": "0",
            "bug_report": "Security Wizard: enable Kerberos setup for Storm\n1) Following jaas.conf file needs to be on all storm component hosts:Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab='$keytab' storeKey=true useTicketCache=false serviceName='zookeeper' principal='$principal';};2) In YAML  following java configurations should have jaas.conf options:nimbus.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'ui.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'supervisor.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'",
            "id": "5060"
        },
        "5097": {
            "ground_truth": "0",
            "bug_report": "Retry failure after installation failure triggers start all services request\nSteps to reproduce: Make Install all services request fail. Hit on Retry button. Make retry attempt to install all services fail.This will trigger start all services call and error pop-up will be displayed.Expected behavior: Start all services call should not be called.",
            "id": "5097"
        },
        "5103": {
            "ground_truth": "0",
            "bug_report": "Maintenance Mode: maintenance icon changes on Host Details page\nFor host components whose service is in maintenance mode  the maintenance mode icon should be displayed to the right of the service name and we should display the host component health (green / red  etc) on the far left.When the host itself is in maintenance mode  we should not show any maintenance mode icon on the host component (unless the service is in maintenance mode). This allows the UI to distinguish which host components are in service-derived maintenance mode vs host-derived. Also  on this page  host-level operations apply to all but the host components in service-derived maintenance mode (regardless of the host maintenance mode)  so this display is more natural and easier to understand for the end user.",
            "id": "5103"
        },
        "5112": {
            "ground_truth": "0",
            "bug_report": "hadoop-mapreduce.jobsummary.log is empty when specified custom YARN Log Dir\nReproduced with such preconditions:On Customize Services page specify 'YARN Log Dir Prefix' to some custom dir. After deploying  run MapReduce2 Service check and check that:hadoop-mapreduce.jobsummary.log is empty  but /var/log/hadoop-yarn/yarn/hadoop-mapreduce.jobsummary.log contains jobs records.",
            "id": "5112"
        },
        "5123": {
            "ground_truth": "0",
            "bug_report": "Background Operations window does not appear after triggering Rolling Restart\nSTR: Deploy cluster. Check that flag Do not show the Background Operations dialog when starting an operation is set to false. Click Restart DataNodes in Actions menu of HDFS. Click 'Trigger Restart' in appeared modal window.Result: Background Operations was not appeared.",
            "id": "5123"
        },
        "5128": {
            "ground_truth": "0",
            "bug_report": "Rolling Restart dialog shows incorrect message that slaves won't be restarted when service is in maintenance mode\nThe fix for this issue would be to display that the slaves whose host is in 'host' maintenance mode will be skipped.For example  we have 3 hosts (host1  host2  and host3) with NodeManager installed on each. Say host3 is in maintenance mode.Rolling Restart dialog should say '1 NodeManager in maintenance mode will not be restarted'.",
            "id": "5128"
        },
        "5146": {
            "ground_truth": "0",
            "bug_report": "After Ambari is upgraded to 1.5.0  previous JAVA_HOME is overwritten to /usr/jdk64/jdk1.6.0_3\nAfter upgrading Ambari from 1.2.5 to 1.5.0 and Stack from 1.3.2 to 2.0.10When starting HDFS service  Datanode and SNameNode on the agent host failed to start due to JAVA_HOME=cbin/java is missing. In ambari.properties  java.home=/usr/jdk64/jdk1.6.0_3 after upgrade.",
            "id": "5146"
        },
        "5156": {
            "ground_truth": "1",
            "bug_report": "Hive CLI using Tez runtime does not start by throwing HDFS exception\nIn a cluster node we had set the below in /etc/hive/conf/hive-site.xml&lt;property&gt; &lt;name&gt;hive.jar.directory&lt;/name&gt; &lt;value&gt;hdfs:///apps/hive/install&lt;/value&gt;&lt;/property&gt;The HDFS folder has the following contents# hadoop fs -ls -R /apps/hive/drwxr-xr-x - hive hdfs 0 2014-03-19 17:10 /apps/hive/install-rwxr-xr-x 3 hive hdfs 14719276 2014-03-19 17:10 /apps/hive/install/hive-exec.jardrwxrwxrwx - hive hdfs 0 2014-03-19 17:08 /apps/hive/warehouseAs user ambari-qa I run the hive command to hit this exception$ hiveLogging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.propertiesException in thread 'main' java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=ambari-qa  access=WRITE  inode='/apps/hive/install':hive:hdfs:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5481) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5463) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5437) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2265) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2218) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2003) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1999) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1997) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:345) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:682) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=ambari-qa  access=WRITE  inode='/apps/hive/install':hive:hdfs:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5481) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5463) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5437) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2265) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2218) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2003) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1999) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1997) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1602) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1461) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1386) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390) at org.apache.had",
            "id": "5156"
        },
        "5173": {
            "ground_truth": "0",
            "bug_report": "The status of App Timeline Server affect the health status of YARN service.\nWhen stopping ATS  YARN service indicator blinks red giving the idea that YARN is going down and become solid red after that. YARN service should not be indicated as STOPPED if ATS is down.",
            "id": "5173"
        },
        "5223": {
            "ground_truth": "0",
            "bug_report": "hive-env.sh overwrites user value of HIVE_AUX_JARS_PATH\nThis line at the bottom of hive-env.sh disregards any user-provided values  masking them from the launch script.# Folder containing extra ibraries required for hive compilation/execution can be controlled by:export HIVE_AUX_JARS_PATH=/usr/lib/hcatalog/share/hcatalog/hcatalog-core.jarThis breaks this feature for users of both the environment export HIVE_AUX_JARS_PATH='my custom value' and the command line hive --auxpath 'my custom value' .",
            "id": "5223"
        },
        "5235": {
            "ground_truth": "0",
            "bug_report": "Add component of clients in Ambari doesn't work in a secured cluster\nThe reason of bug is wrong query formation while triggering API to create clients on host.The data sent with the POST call to create client components on the host is:{'RequestInfo':{'context':'Install Clients'} 'Body':{'host_components':[{'HostRoles':{'component_name':'CLIENTS'}}]}}This is incorrect. There is no component with name 'CLIENTS'.",
            "id": "5235"
        },
        "5244": {
            "ground_truth": "0",
            "bug_report": "Wizard Step7 JS error on load page (add service wizard)\nAdd Nagios  Ganglia via Add Service WizardProceed to step 'Customize Services'JS-error appears about null-object:/app/controllers/wizard/step7_controller.js : loadServiceTagsSuccess()if (serviceConfigsDef.sites.indexOf(site) &gt; -1)Also  HDFS by default is selected as active tab  but one of the 'new' services (Nagios for example) should be selected.",
            "id": "5244"
        },
        "5246": {
            "ground_truth": "0",
            "bug_report": "Mistake in title of operation 'Restart APP_TIMELINE_SERVER on ...\nWe should show displayName  not componentName",
            "id": "5246"
        },
        "5258": {
            "ground_truth": "0",
            "bug_report": "Installer: 'Undo' button for repo BaseURL does not work\nSTR: during installer phase go to &lt;cluster&gt;:8080 /#/installer/step1 ; Change/delete any of BaseURL of repos ; Click 'Undo' buttonActual Results:Undo button does not work. Moreover  if we click Undo after any update of text in BaseURL  it leads to cleanup of it value atall.",
            "id": "5258"
        },
        "5264": {
            "ground_truth": "0",
            "bug_report": "Ganglia Server goes to 'installed' state after double 'Ganglia rrdcached base directory' config changing\nSTR:1) Deploy cluster by default2) Go to Ganglia service page -&gt; Config tab 3) Change value 'Ganglia rrdcached base directory' (e.g. /var/lib/ganglia/rrd8)4) Restart Ganglia5) Change 'Ganglia rrdcached base directory' back to old value (by default - /var/lib/ganglia/rrds)6) Restart GangliaExpected result: Ganglia should be restarted successfullyCurrent result: Ganglia Server goes to 'installed' state and stuck thereError message from gmetad file:=============================Starting hdp-gmetad...=============================Base directory (-b) resolved via file system links!Please consult rrdcached '-b' documentation!Consider specifying the real directory (/var/lib/ganglia/rrd8)chgrp: cannot access '/var/run/ganglia/hdp/rrdcached.sock': No such file or directorychgrp: cannot access '/var/run/ganglia/hdp/rrdcached.limited.sock': No such file or directoryFailed to start /usr/bin/rrdcachedNot starting /usr/sbin/gmetad because starting /usr/bin/rrdcached failed.root 16990 0.0 0.0 108164 1560 ? S 03:19 0:00 /bin/bash --login -c service hdp-gmetad start &gt;&gt; /tmp/gmetad.log 2&gt;&amp;1 ; /bin/ps auwx | /bin/grep [g]metad &gt;&gt; /tmp/gmetad.log 2&gt;&amp;1",
            "id": "5264"
        },
        "5274": {
            "ground_truth": "0",
            "bug_report": "Supervisor under supervision fails w/o ganglia server\nJMXetricAgent instrumented JVM  see https://github.com/ganglia/jmxetricMar 28  2014 6:40:13 PM info.ganglia.jmxetric.JMXetricAgent premainSEVERE: Exception starting JMXetricAgentjava.net.UnknownHostException: {0}: Name or service not known at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293) at java.net.InetAddress.getAllByName0(InetAddress.java:1246) at java.net.InetAddress.getAllByName(InetAddress.java:1162) at java.net.InetAddress.getAllByName(InetAddress.java:1098) at java.net.InetAddress.getByName(InetAddress.java:1048) at info.ganglia.gmetric4j.gmetric.AbstractProtocol.&lt;init&gt;(AbstractProtocol.java:29) at info.ganglia.gmetric4j.gmetric.Protocolv31x.&lt;init&gt;(Protocolv31x.java:34) at info.ganglia.gmetric4j.gmetric.GMetric.&lt;init&gt;(GMetric.java:108) at info.ganglia.jmxetric.XMLConfigurationService.configureGangliaFromXML(XMLConfigurationService.java:165) at info.ganglia.jmxetric.XMLConfigurationService.configure(XMLConfigurationService.java:67) at info.ganglia.jmxetric.JMXetricAgent.premain(JMXetricAgent.java:51) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:382) at sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:397)The reason here is this config send from ui:...childopts: '-javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} ...'it can't find hostname {0}  however this works fine:...childopts: '-javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host= port...'So let's leave it as empty if no ganglia server is present",
            "id": "5274"
        },
        "5282": {
            "ground_truth": "0",
            "bug_report": "supervisor.enable should be removed from Ambari's Storm Config section\nAmbari exposes the supervisor.enable property and in some early builds defaults it to true which causes the supervisor's to not launch workers assigned to them. The supervisor's will start up  the assignments are there  but the supervisor's just ignore them. We need to remove the supervisor.enable from Ambari as it seems like a very dangerous switch that doesn't have broad applicability.",
            "id": "5282"
        },
        "5301": {
            "ground_truth": "0",
            "bug_report": "Table of Confirm Hosts step is collapsed\nSteps to reproduce:1. Run hosts registration2. Switch to Registering category3. Wait till all hosts become registeredResult: Table is collapsed.JS error occured: Uncaught Error: assertion failed: calling set on destroyed object",
            "id": "5301"
        },
        "5310": {
            "ground_truth": "0",
            "bug_report": "Don't do server-client version match check if App.version have not been set on ambari-web\nWith the new server-client version match check introduced  it is a bit annoying when building ambari-web locally and testing on the server for e2e testing  as the server and client versions must match exactly or you cannot proceed. We'll disable the check if App.version == ''.",
            "id": "5310"
        },
        "5311": {
            "ground_truth": "0",
            "bug_report": "Falcon fails to deploy\nLooks like Falcon service definition on the stack has changed by AMBARI-5278.Currently  any new definition of globals require duplicating them in the UI code but that was not done  so this is breaking Falcon installation.",
            "id": "5311"
        },
        "5313": {
            "ground_truth": "0",
            "bug_report": "Icon 'Asterisk' on 'Assign Masters'/-Slaves steps does not display with 8-bit depth\nIcon 'Asterisk' on 'Assign Masters'/-Slaves steps does not display with 8-bit depth",
            "id": "5313"
        },
        "5332": {
            "ground_truth": "0",
            "bug_report": "Print better logs for openssl issues on centos/rhel 6.5.\nPrint better logs for openssl issues on centos/rhel 6.5.",
            "id": "5332"
        },
        "5340": {
            "ground_truth": "0",
            "bug_report": "Tez DAG Operator hover text not wrapping wide content\nWhen a Hive Tez DAG operator has a plan with very wide content  the text is not wrapped and hence bleeds out of the hover box. An additional problem is that this causes the hover to lose focus and hence close - thereby user cannot even see the hover (opens &amp; closes almost instantly).",
            "id": "5340"
        },
        "5342": {
            "ground_truth": "0",
            "bug_report": "Ambari YARN UI - Quick Link - JMX breaks if RM port is changed\nIf you change RM port from 8088 to 50030 for migration of 1.x to 2.x   JVM metrics and few others are missing which results in yarn service summary page having multiple fields with n/a value.Step1: Change property below yarn.resourcemanager.webapp.address = localhost:50030Step 2: Start the service Click on Dashboard JMX is issue  Quick links is issue. RM gets started successfully.",
            "id": "5342"
        },
        "5344": {
            "ground_truth": "0",
            "bug_report": "Error with finding FK constraint\nSteps: Install Ambari-1.3.2 with Oracle DB. Upgrade ambari to 1.5.0 Run 'ambari-server upgrade' command.Seems like check before execute doesn't work for Oracle. Since we ignore failures this is not block the upgrade.Exception:16:41:36 719 WARN [main] DBAccessorImpl:416 - Error executing query: ALTER TABLE clusterconfigmapping ADD CONSTRAINT FK_clustercfgmap_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id)java.sql.SQLSyntaxErrorException: ORA-02275: such a referential constraint already exists in the table at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:445) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396) at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:879) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:450) at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:192) at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531) at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193) at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1033) at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1329) at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1909) at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1871) at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:318) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:413) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:399) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:262) at org.apache.ambari.server.upgrade.UpgradeCatalog150.executeDDLUpdates(UpgradeCatalog150.java:375) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:174) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:234)16:41:36 720 WARN [main] DBAccessorImpl:264 - Add FK constraint failed  constraintName = FK_clustercfgmap_cluster_id  tableName = clusterconfigmapping  errorCode = 2275  message = ORA-02275: such a referential constraint already exists in the table",
            "id": "5344"
        },
        "5359": {
            "ground_truth": "0",
            "bug_report": "unittest NagiosPropertyProviderTest fails\ntestNoNagiosServerCompoonent(org.apache.ambari.server.controller.nagios.NagiosPropertyProviderTest): Expected no alertstestNoNagiosService(org.apache.ambari.server.controller.nagios.NagiosPropertyProviderTest): Expected no alerts",
            "id": "5359"
        },
        "5362": {
            "ground_truth": "0",
            "bug_report": "Automatic bootstrap failed on CentOS 6.5 (No module named common_functions)\nSSH bootstrap of agents failed on CentOS 6.5.==========================Copying OS type check script...==========================Could not create directory '/root/.ssh'.Warning: Permanently added 'c6502.ambari.apache.org 192.168.65.102' (RSA) to the list of known hosts.scp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.pyhost=c6502.ambari.apache.org  exitcode=0==========================Running OS type check...==========================Traceback (most recent call last): File '/tmp/os_check_type1396641340.py'  line 22  in &lt;module&gt; from common_functions import OSCheckImportError: No module named common_functionsConnection to c6502.ambari.apache.org closed.SSH command execution finishedhost=c6502.ambari.apache.org  exitcode=1ERROR: Bootstrap of host c6502.ambari.apache.org fails because previous action finished with non-zero exit code (1)ERROR MESSAGE: tcgetattr: Invalid argumentConnection to c6502.ambari.apache.org closed.STDOUT: Traceback (most recent call last): File '/tmp/os_check_type1396641340.py'  line 22  in &lt;module&gt; from common_functions import OSCheckImportError: No module named common_functionsConnection to c6502.ambari.apache.org closed.",
            "id": "5362"
        },
        "5371": {
            "ground_truth": "0",
            "bug_report": "Dashboard: dashboard actions do not work for non-admin users\nFor non-admin users action 'Switch to classic dashboard' returns JS error and 'Reset all widgets to default' action throws to Login page.",
            "id": "5371"
        },
        "5381": {
            "ground_truth": "0",
            "bug_report": "Installer: 'Undo' button for repo BaseURL is unnecessarily present\nSTR:on Installer phase go to &lt;cluster&gt;:8080 /#/installer/step1Do not do any updates of 'Base Url' fieldcheck the 'Undo' button for 2.1 stack.Actual Result:Undo is present for all 3 Os reposExpected Result:Undo should be present only after any update of 'BaseURL' field----------Seems  functionally this does not affect us  but might be confusing for users.",
            "id": "5381"
        },
        "5382": {
            "ground_truth": "0",
            "bug_report": "Schema Upgrade failed when upgrading to 1.5.1\nWhile upgrade  schema upgrade fails.Example error output (Oracle):rg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=1.5.1.96  schemaVersion=1.4.1.25 at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:479) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:149) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:528)and Postgres 11:39:27 364 ERROR [main] AmbariServer:531 - Failed to run the Ambari Serverorg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=1.5.1.96  schemaVersion=1.5.0 at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:479) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:149) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:528)",
            "id": "5382"
        },
        "5386": {
            "ground_truth": "0",
            "bug_report": "Deploy stuck during generating tasks on Review page (not always reproduced)\nAfter the user clicks 'Deploy' in the Installer Wizard  sometimes an error is shown showing 'java.lang.IllegalArgumentException: Could not access base url'. This happens while the wizard tries to save the user-specified repo base URLs.Installer should not be performing validation during deploy  because it had already been done during Select Stacks.",
            "id": "5386"
        },
        "5402": {
            "ground_truth": "0",
            "bug_report": "Remove classic dashboard view from Ambari\nremove classic dashboard from Ambari  as it's not used any more and doesn't support new services",
            "id": "5402"
        },
        "5406": {
            "ground_truth": "0",
            "bug_report": "Pig unit test class named wrong\nTypo in unit test. File said class was Hcat rather than Pig",
            "id": "5406"
        },
        "5412": {
            "ground_truth": "0",
            "bug_report": "Operation 'Supervisor start' failed during installation but all supervisors are alive\nSTR:1. Deploy Hadoop by default scenario with all services.2. Try to start Storm.Actual results: 'Start All Services' operation failed because of 'Supervisor start' failed. 'Start Storm' operation does not contain 'Supervisor start' popups  but all supervisors are STARTED after it have finished.",
            "id": "5412"
        },
        "5413": {
            "ground_truth": "0",
            "bug_report": "OS type check for centos 6.5 can fail if the /etc/issue has CentOS Linux release 6.5\nI tried default centos 6.5 it works fine. But if I change /etc/issues and /etc/redhat-release to say:CentOS Linux release 6.5 (Final)then the registration fails with:INFO 2014-04-09 14:39:22 501 security.py:51 - SSL connection established. Two-way SSL authentication is turned off on the server.ERROR 2014-04-09 14:39:22 563 Controller.py:100 - Cannot register host with not supported os type  hostname=c6501.ambari.apache.org  serverOsType=redhat6  agentOstype=centos linux6In the agent logs.By default centos 6.5 has:CentOS release 6.5 (Final)but sometimes can have:CentOS Linux release 6.5 (Final)",
            "id": "5413"
        },
        "5433": {
            "ground_truth": "0",
            "bug_report": "Add Host failed on upgraded cluster on Suse\nThis is due to upgrade. We need to change it manually after upgrade://Now we have (redhat  suse  debian  other_detected_by_python)vi /etc/ambari-server/conf/ambari.properties(server.os_type=sles11) --&gt;( server.os_type=suse11)+restart the serverShould upgrade automatically deal with this?old code: os_info = platform.linux_distribution( None  None  None  ['SuSE'  'redhat' ]  0 ) os_name = os_info[0].lower() if os_name == 'suse': os_name = 'sles' os_version = os_info[1].split('.'  1)[0] master_os_type = os_name + os_version write_property(OS_TYPE_PROPERTY  master_os_type)",
            "id": "5433"
        },
        "5445": {
            "ground_truth": "0",
            "bug_report": "When new host components are created thru API  some indication should be given that Nagios has to be restarted\nNeed to fix this on API side",
            "id": "5445"
        },
        "5455": {
            "ground_truth": "0",
            "bug_report": "Ambari configuration for map join conversion and tez container size seems wrong\nFor hive:hive.auto.convert.join.noconditionaltask.size is set to 1000000000 This should be a fraction (1/3) of the container size.hive.tez.java.opts has '-Xmx1024m'This is different from both map and reduce sizes. Desired values are: map size if map size &gt; 2g else reduce sizemap size is set on the same cluster to ~500mbThe settings as the are will lead to many failed queries because the mapjoin conversion is to aggressive. If we don't change the container sizes based on cluster configs we will see wide spread problems with containers being killed or perf problems.",
            "id": "5455"
        },
        "5457": {
            "ground_truth": "0",
            "bug_report": "Host Checks: alternatives check results are not surfaced in Host Check popup\nAmbari Agent  as part of host checks  identifies conflicting 'alternatives' settings and reports back inside the 'last_agent_env' object.UI is not surfacing this in Host Checks popup.We should have a section called 'Alternatives Issues' and list out the alternatives names.For example: 'last_agent_env' : { 'stackFoldersAndFiles' : [ ... ]  'alternatives' : [ { 'name' : 'zookeeper-conf'  'target' : '/etc/zookeeper/conf.dist' }  { 'name' : 'hadoop-conf'  'target' : '/etc/hadoop/conf.dist' }  ]  'existingUsers' : [ .... ]  'existingRepos' : [ ... ]  ...In the above case  we want to highlight the fact that hadoop-conf and zookeeper-conf have conflicts.Alternatives Issues--------The following alternatives should be removedAlternativeshadoop-conf Exists on 3 hostszookeeper-conf Exists on 3 hosts",
            "id": "5457"
        },
        "5459": {
            "ground_truth": "0",
            "bug_report": "Usability: Improve Stack Definition support for repositories\n1) Remove HDP-UTILS from Ambari .repo  and move into the HDP Stack Definition (Ambari does not depend on HDP-UTILS so this causes confusion  makes it harder to doc local repo setup  and is unclear on failures if not setup correctly).2) Requires support for multiple repositories in a Stack Definition  and ability to specify multiple repositories from UI.",
            "id": "5459"
        },
        "5472": {
            "ground_truth": "0",
            "bug_report": "Use SchemaTool in Hive for init metastore DB schema\nWhen Ambari create the metastore database in MySQL it uses auto create feature. This does not create the transaction tables  so any ACID operations (including streaming ingest) will not work.",
            "id": "5472"
        },
        "5511": {
            "ground_truth": "0",
            "bug_report": "Misleading hardcoded command in paragraph 2 on step 'Manual commands' of 'Move Master' wizard\nSTR:1. Deploy cluster with multiplied NN directory(in our case /grid/0/hadoop/hdfs/namenode  /grid/1/hadoop/hdfs/namenode).2. Start NN moving.3. Reach step 'Manual commands'.Actual results:Paragraph 1 contains information about all dirs we should move.Paragraph 2 contains message 'Login to the target host XXX and change permissons for the NameNode dirs by running:' and only one command with hardcoded NN dir:chown -R hdfs:hadoop /hadoop/hdfs/namenode/Screenshot attached.When there are multiple directories specified  we need to show the actual path (but replace commas with a space).In case of '/grid/0/hadoop/hdfs/namenode /grid/1/hadoop/hdfs/namenode' as in the attached image  we should display:chown -R hdfs:hadoop /grid/0/hadoop/hdfs/namenode /grid/1/hadoop/hdfs/namenode",
            "id": "5511"
        },
        "5512": {
            "ground_truth": "0",
            "bug_report": "Move wizard and HA wizard gets stuck on any deploy step\nBecause of JS error (page refresh makes no effect).",
            "id": "5512"
        },
        "5524": {
            "ground_truth": "0",
            "bug_report": "Unit tests for number_utils  string_utils  validator and misc files.\nCreate unit tests for following files:utils/misc.jsutils/number_utils.jsutils/string_utils.jsutils/validator.js",
            "id": "5524"
        },
        "5531": {
            "ground_truth": "1",
            "bug_report": "Switch SQL standard authorization to be off by default.\nFor Ambari 1.5.1 SQL standard authorization was on by default.Users with certification suites are running into problems related to this feature (which were not bugs in the auth systems  rather they were additional requirements to using it). This needs to be turned off by default. The feature is controlled by:hive.security.authorization.enabledWe want the default value to be set to false in Ambari 1.6.0.",
            "id": "5531"
        },
        "5543": {
            "ground_truth": "0",
            "bug_report": "Unit tests for steps 6 (with small refactor)\nUnit tests for steps 6 (with small refactor)",
            "id": "5543"
        },
        "5546": {
            "ground_truth": "0",
            "bug_report": "Call for requests with 'page_size' always return 10 most recent\nProblem:Call: /api/v1/clusters/cl1/requests?to=end&amp;page_size=20Actually result: return most recent 10 requestsExpected result: return most recent 20 requests.Call: /api/v1/clusters/cl1/requests?from=start&amp;page_size=3Actually result: return first 3 requests started from the first in most recent 10Expected result: return first 3 requests started from the very first('install services')In this case  we can never get history requests made before most recent 10.",
            "id": "5546"
        },
        "5551": {
            "ground_truth": "0",
            "bug_report": "Checkbox 'client' without upper case letter\nCheckbox 'client' without upper case letter",
            "id": "5551"
        },
        "5555": {
            "ground_truth": "0",
            "bug_report": "NameNode HA wizard: Review page appears blank\nNameNode HA wizard: Review page appears blank",
            "id": "5555"
        },
        "5558": {
            "ground_truth": "0",
            "bug_report": "Navigating back from Host page to Heatmaps page is broken\nSTR: Go to Dashboard page. Switch to 'Heatmaps' tab. Click on first host. On host page click 'Back'.Actual result: Appeared 'Cluster Status and Metrics' tab.Expected result: Should appear 'Heatmaps' tab.",
            "id": "5558"
        },
        "5569": {
            "ground_truth": "0",
            "bug_report": "Fix UI Unit tests\nFix UI Unit tests",
            "id": "5569"
        },
        "5571": {
            "ground_truth": "0",
            "bug_report": "Restart option is enabled for components in 'Decommissioned' state but it should not\n'Restart' option should not be enabled if a slave component is in 'decommissioned' state  but it is.STR:1) Go to 'Host details' page2) Make any slave component 'Decommissioned'Actual result: 'Restart' option is enabled (see screenshot)Expected result: 'Restart' option should be disabled.",
            "id": "5571"
        },
        "5577": {
            "ground_truth": "0",
            "bug_report": "Turn Off Maintenance Mode for HDFS does not work (problems with ambari-agent)\nSTR:Turn On Maintenance Mode for HDFSTurn Off Maintenance Mode for HDFSExpected result:Have not problems with ambari-agent.Actual result:Have problems with ambari-agent.",
            "id": "5577"
        },
        "5603": {
            "ground_truth": "0",
            "bug_report": "Ambari version is unknown during installer via UI\nDuring installer user does not see Ambari Version from UI (see screenshot)  but on monitoring phase it's available in the same Admin -&gt; About",
            "id": "5603"
        },
        "5605": {
            "ground_truth": "0",
            "bug_report": "Usability UX: Default key actions for dialog boxes\nPROBLEM: Default action for dialog boxes in AmbariUSE CASE: Ambari UI doesn't allow you to press enter and trigger default actions. When a default action is high lighted 'green button' you should be able to hit enter and have the action be triggered. In this case add hit enter should make okay button trigger. Also  pressing escape should cancel the dialog box.",
            "id": "5605"
        },
        "5607": {
            "ground_truth": "0",
            "bug_report": "Yarn Nodemanager Metrics only update every few minutes\nYarn Nodemanager Metrics take far too long between updates.To demonstrate:Run Terasort or anything that runs mapreduce:hdfs dfs -mkdir -p benchmarks/terasorthadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Dmapred.map.tasks=72 -Dmapred.reduce.tasks=36 1000000 benchmarks/terasort/inputhadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapred.map.tasks=72 -Dmapred.reduce.tasks=36 benchmarks/terasort/input benchmarks/terasort/outputhdfs dfs -rm -R -skipTrash benchmarks/terasortThen repeatedly probe the API at:https://&lt;server&gt;:8081/api/v1/clusters/c1/services/YARN/components/NODEMANAGER?fields=host_components/metrics/yarnIt usually takes 2-3 minutes to see the metrics update  very repeatable.",
            "id": "5607"
        },
        "5612": {
            "ground_truth": "0",
            "bug_report": "Unit tests for object_utils  date  ui_effects  updater\nCreate unit tests for following files: utils/object.js utils/date_utils.js utils/ui_effects_utils.js utils/updater.js",
            "id": "5612"
        },
        "5622": {
            "ground_truth": "0",
            "bug_report": "'Upgrading schema' failed during upgrading to 1.6.0\nPostgres issue:new restart_required relies on eclipselink default type converters  we avoided this in pastorg.postgresql.util.PSQLException: ERROR: column 'restart_required' is of type boolean but expression is of type integer Hint: You will need to rewrite or cast the expression. Position: 57 at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:331) at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:447) at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:371) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:72) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)MySQL issue:Inreresting MySQL feature  there should be no space between function name and parenthesis18:57:00 629 WARN [main] DBAccessorImpl:469 - Error executing query: insert into request(request_id  cluster_id  request_context  start_time  end_time  create_time) select distinct s.request_id  s.cluster_id  s.request_context  coalesce (cmd.start_time  -1)  coalesce (cmd.end_time  -1)  -1 from (select distinct request_id  cluster_id  request_context from stage ) s left join (select request_id  min(start_time) as start_time  max(end_time) as end_time from host_role_command group by request_id) cmd on s.request_id=cmd.request_idcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: FUNCTION ambari.coalesce does not exist at sun.reflect.GeneratedConstructorAccessor14.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) at com.mysql.jdbc.Util.getInstance(Util.java:386) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2828) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2777) at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:949) at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:795) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.upgrade.UpgradeCatalog150.executeDDLUpdates(UpgradeCatalog150.java:325) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)",
            "id": "5622"
        },
        "5628": {
            "ground_truth": "0",
            "bug_report": "Explicitly disabling datanucleus l2 cache for hive\nAmbari installations of hive currently do not set any datanucleus related properties. There is such a thing as a datanucleus l2 cache  that is pretty bad for hive in a distributed environment if it is set. (If there is a lone embedded hive instance  with no other codepaths to the db  then it's fine  but that never happens in a distributed environment.)By default  if no setting is present  datanucleus defaults the l2 cache to being on  so hive ups the ante by defaulting to turning it off by default if no other setting is configured.Now  in a war of 'defaults'  the hive default should win  but this is an area where we have had recurring support issues from clients that turn it on expecting improved performance. Thus  I'd like ambari installed hive-site.xml to explicitly have this config parameter turned off  with a comment asking users to not switch it on as it impacts hive negatively.The parameter in question is 'datanucleus.cache.level2.type'   and it's value should be 'none'. (Note that I've seen some older configs that seem to do things like turning datanucleus.cache.level2 = false and stuff like that  that is bogus config and does nothing and should not be assumed to be a catch-all enabler.)As a comment  I'd like the following comment 'Disables datanucleus l2 cache. This must be set to 'none' for hive to work properly' or something to that effect.",
            "id": "5628"
        },
        "5633": {
            "ground_truth": "0",
            "bug_report": "Start Services command gets stuck for about 30 mins\nOn Security wizard Start Services command ZOOKEEPER_SERVER Start task scheduled on ambari-server host remained in QUEUED status for about 30 mins and other non-completed commands were in PENDING status. For this time interval no command was in IN_PROGRESS status.Later executing stop all services command on the same cluster also made ZOOKEEPER_SERVER Stop task scheduled on ambari-server host to remain in QUEUED status for around 20 mins.",
            "id": "5633"
        },
        "5643": {
            "ground_truth": "0",
            "bug_report": "Add Services is disabled after upgrading the stack from HDP-2.0 to HDP-2.1\nPrior to upgrade  launch Add Services wizard using Ambari 1.5.0 (crucial step) Upgrade stack to a stack with new services and Ambari to 1.5.1 Add Services button is disabled  even though there are services that have not been added to the cluster",
            "id": "5643"
        },
        "5646": {
            "ground_truth": "0",
            "bug_report": "Jobs dont show up as links for a moment when page is visited\nI had a couple of finished jobs showing the jobs page. Then I clicked on one job and went back to the jobs page. For a moment both jobs were not shown as links - though they are clickable. This is basically an appearance issue where the job doesnt look like a link for a moment.",
            "id": "5646"
        },
        "5656": {
            "ground_truth": "0",
            "bug_report": "Views: do not let the user click on the Views icon in the top nav\nWhen clicking on the Views icon in the top nav  the page content turns blank.We will show a view index page in a later release  but for now  let's just disable clicking on that icon.",
            "id": "5656"
        },
        "5668": {
            "ground_truth": "0",
            "bug_report": "JobsDiagnostic|2.1.1: No job status and end time is shown for interrupted job\nEnabled tez engine for hive;Executed select query;If while job is running I stop it by double pressign CTRL-C in hive shell then job will not have end time shown in jobs table and no failed icon (red X)  but in job details it will have status 'killed' and correct end time.If job is killed with yarn application -kill it will have correct end time and failed icon displayed in jobs table  but in job details it will not have neither status nor end time.",
            "id": "5668"
        },
        "5669": {
            "ground_truth": "0",
            "bug_report": "Alternatives issues has error message (missing translation)\nBuild 1.6.0-151Missing translation: installer.step3.hostWarningsPopup.alternatives.emptyinstaller.step3.hostWarningsPopup.alternatives.empty",
            "id": "5669"
        },
        "5688": {
            "ground_truth": "0",
            "bug_report": "Zookeeper smoke test failed after being triggered after deleting a host  containing ZookeeperServer\n1. Stopped all host components on a host ( to be deleted) with ZookeeperServer. 2. Deleted a host from cluster. 3. Ran Zookeeper Service Check.Actual result: failed. Expected: ok.",
            "id": "5688"
        },
        "5692": {
            "ground_truth": "0",
            "bug_report": "Unit tests for utils/config.js part 1\nUpdate unit tests for utils/config.js.",
            "id": "5692"
        },
        "5714": {
            "ground_truth": "0",
            "bug_report": "Views list not loading in Ambari Web\nIn build 1.6.0-159  the views call is being made but it's not listing the views in the dropdown.This was due to the API change and Ambari Web hasn't adjusted yet to this introduction of /versions/.",
            "id": "5714"
        },
        "5716": {
            "ground_truth": "1",
            "bug_report": "Disable security fails occasionally\nThere is a possibility that 'Start all Services' command will fail on Disable Security wizard if App.Service DS model was not populated on the load of 'Disable security page' (timing issue). We need to make sure that the Service model has populated before the 'Disable Security page' is rendered.",
            "id": "5716"
        },
        "5720": {
            "ground_truth": "0",
            "bug_report": "pig.properties should set pig.location.check.strict to false\npig.properties should set pig.location.check.strict to false",
            "id": "5720"
        },
        "5722": {
            "ground_truth": "0",
            "bug_report": "All Services Fail To Deploy Due To Agent Parsing Exception\nWhen deploying a brand new cluster  all services fail to install due to a parsing exception thrown from the Ambari Agents.File '/usr/lib/python2.6/site-packages/ambari_agent/CustomServiceOrchestrator.py'  line 113  in runCommandjson_path = self.dump_command_to_json(command)File '/usr/lib/python2.6/site-packages/ambari_agent/CustomServiceOrchestrator.py'  line 209  in dump_command_to_jsoncommand'clusterHostInfo' = manifestGenerator.decompressClusterHostInfo(command'clusterHostInfo')File '/usr/lib/python2.6/site-packages/ambari_agent/manifestGenerator.py'  line 116  in decompressClusterHostInfoindexes = convertRangeToList(v)File '/usr/lib/python2.6/site-packages/ambari_agent/manifestGenerator.py'  line 57  in convertRangeToListraise AgentException.AgentException('Broken data in given range  expected - ''m-n'' or ''m''  got : ' + str(r))AgentException: 'Broken data in given range  expected - m-n or m  got : -1he command being sent is{hs_host=[2]  namenode_host=[1]  snamenode_host=[2]  zookeeper_hosts=[0-2]  ganglia_server_host=[1]  nm_hosts=[0]  ganglia_monitor_hosts=[0-2]  all_hosts=[c6403.ambari.apache.org  c6401.ambari.apache.org  c6402.ambari.apache.org]  rm_host=[2]  app_timeline_server_hosts=[2]  slave_hosts=[0]  ambari_server_host=[-1]  nagios_server_host=[1]  all_ping_ports=[8670:0-2]}Notice the ambari-server-host which was added in that commit; it value is 1which would not parse correctly in manifestGenerator.pyI suspect Git e667dc7c9870864ff537374c819b7c1d1dd88e98 caused this problem.Steps to reproduce:1) Provision 3 c64 hosts2) Wipe your server database and re-create it with the embedded PSQL script3) Attempt to provision a cluster with various services.All services will fail to deploy b/c of the above exception. This was working without issues before the above suspect commit.",
            "id": "5722"
        },
        "5726": {
            "ground_truth": "0",
            "bug_report": "Adding Oozie failed at service check\nThis happens because HDFS and YARN/MapReduce requires to be restarted for Oozie smoke test to pass successfullyAs a fix to this issue: Don't run smoke test on 'Install  Start and Test' page of the add service wizard. Review page should ask user to restart all stale services.",
            "id": "5726"
        },
        "5730": {
            "ground_truth": "0",
            "bug_report": "Space Error in container-executor.cfg\nThere is a space between 'banned.user' and '=' which make configuration here is ignored by container-executor  so some default banned users works to include hdfs. Space should be deleted between 'banned.user' and '='yarn.nodemanager.local-dirs=/grid/0/hadoop/yarn/local /grid/1/hadoop/yarn/localyarn.nodemanager.log-dirs=/grid/0/hadoop/yarn/log /grid/1/hadoop/yarn/logyarn.nodemanager.linux-container-executor.group=hadoopbanned.users = hdfs yarn mapred binmin.user.id=1000It should be banned.users=hdfs yarn mapred bin",
            "id": "5730"
        },
        "5735": {
            "ground_truth": "0",
            "bug_report": "HDP deployment failed in CentOS5\nFail: Execution of 'mkdir -p /tmp/HDP-artifacts/ ; curl --noproxy hadoop -kf --retry 10 http://hadoop:8080/resources//jdk-7u45-linux-x64.tar.gz -o /tmp/HDP-artifacts//jdk-7u45-linux-x64.tar.gz' returned 2. curl: option --noproxy: is unknownversion of curl that is available at Centos 5 and SLES 11 SP1 seems to have no support for '--noproxy' option.But such workaround works:no_proxy=i.ua curl http://www.i.ua -iI'm going to replace all '--noproxy' invocations with usage of $no_proxy env variable.",
            "id": "5735"
        },
        "5739": {
            "ground_truth": "0",
            "bug_report": "File View Cleanup\nA few items that need to be rectified for the File view submission:1. Modify code to abide by Ambari Coding Standards2. JavaDoc Interfaces and Methods.https://cwiki.apache.org/confluence/display/AMBARI/Coding+Guidelines+for+Ambari.",
            "id": "5739"
        },
        "5751": {
            "ground_truth": "0",
            "bug_report": "Ambari upgrade to Ambari-1.6.0 from Ambari-1.5.1 logs PSQLException\nFollowing upgrade documentation at http://docs.hortonworks.com/HDPDocuments/Ambari-1.5.1.0/bk_upgrading_Ambari/content/ambari-chap7_2x.html On executing ambari-server upgrade  PSQLException is logged inambari-server.log:21:36:20 498 INFO [main] SchemaUpgradeHelper:211 - Upgrading schema to target version = 1.6.021:36:20 528 INFO [main] SchemaUpgradeHelper:220 - Upgrading schema from source version = 1.5.1.11021:36:20 530 INFO [main] SchemaUpgradeHelper:141 - Upgrade path: [{ org.apache.ambari.server.upgrade.UpgradeCatalog160$$EnhancerByGuice$$ff8a4f66: sourceVersion = null  targetVersion = 1.6.0 }]21:36:20 530 INFO [main] SchemaUpgradeHelper:171 - Executing DDL upgrade...21:36:20 533 INFO [main] DBAccessorImpl:463 - Executing query: ALTER SCHEMA ambari OWNER TO 'ambari';21:36:20 534 INFO [main] DBAccessorImpl:463 - Executing query: ALTER ROLE 'ambari' SET search_path to 'ambari';21:36:20 598 INFO [main] DBAccessorImpl:463 - Executing query: CREATE TABLE hostgroup_configuration (blueprint_name VARCHAR(255) NOT NULL  hostgroup_name VARCHAR(255) NOT NULL  type_name VARCHAR(255) NOT NULL  config_data BYTEA NOT NULL  PRIMARY KEY (blueprint_name  hostgroup_name  type_name))21:36:20 962 INFO [main] DBAccessorImpl:463 - Executing query: CREATE TABLE viewentity (id BIGINT NOT NULL  view_name VARCHAR(255) NOT NULL  view_instance_name VARCHAR(255) NOT NULL  class_name VARCHAR(255) NOT NULL  id_property VARCHAR(255)  PRIMARY KEY (id))21:36:21 010 INFO [main] DBAccessorImpl:463 - Executing query: ALTER TABLE hostcomponentdesiredstate ADD restart_required BOOLEAN21:36:21 078 INFO [main] DBAccessorImpl:463 - Executing query: ALTER TABLE hostgroup_configuration ADD CONSTRAINT FK_hg_config_blueprint_name FOREIGN KEY (blueprint_name) REFERENCES hostgroup (blueprint_name)21:36:21 084 WARN [main] DBAccessorImpl:469 - Error executing query: ALTER TABLE hostgroup_configuration ADD CONSTRAINT FK_hg_config_blueprint_name FOREIGN KEY (blueprint_name) REFERENCES hostgroup (blueprint_name)org.postgresql.util.PSQLException: ERROR: there is no unique constraint matching given keys for referenced table 'hostgroup' at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:395) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:337) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:321) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:85) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:250) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)21:36:21 089 WARN [main] DBAccessorImpl:339 - Add FK constraint failed  constraintName = FK_hg_config_blueprint_name  tableName = hostgroup_configurationorg.postgresql.util.PSQLException: ERROR: there is no unique constraint matching given keys for referenced table 'hostgroup' at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:395) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:337) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:321) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:85) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:250) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)",
            "id": "5751"
        },
        "5753": {
            "ground_truth": "0",
            "bug_report": "Storm fails to start after disabling security\nnimbus.childopts  ui.childopts and supervisor.childopts points to sasl configuration files after the security is disabled. web-ui should remove -Djava.security.auth.login.config parameter from these properties while disabling security.",
            "id": "5753"
        },
        "5761": {
            "ground_truth": "0",
            "bug_report": "2000-node cluster testing: during install phase of cluster deployment  install tasks were stuck in PENDING state\nActionScheduler is stucked when adding tasks to ActionQueue on large clusters (&gt;1000 nodes)",
            "id": "5761"
        },
        "5778": {
            "ground_truth": "0",
            "bug_report": "In some upgrade scenarios  Ambari Web's persist key-value store state causes the UI to act unpredictably\nDuring Ambari upgrade  automatically clear the persist state to prevent potential issues with Ambari Web not working properly (this was observed a number of times on upgraded clusters). Currently  we make the following call to get out of the inconsistent state so that Ambari Web works properly:curl -i -u admin:admin -H 'X-Requested-By: ambari' -X POST -d '{ 'CLUSTER_CURRENT_STATUS': '{/'clusterState/':/'CLUSTER_STARTED_5/'}' }' http://localhost:8080/api/v1/persistWe need to do something equivalent during upgrade.",
            "id": "5778"
        },
        "5779": {
            "ground_truth": "1",
            "bug_report": "Recommission a DN fails when https is enabled in Ambari server\nAfter https is enable in Ambari server  Recommission a DN will fails with the following error message found in the Ambari-server log:WARN &#91;qtp1103265648-610&#93; nio:651 - javax.net.ssl.SSLException: Received fatal alert: certificate_unknown 00:32:05 458 INFO &#91;ExecutionScheduler_Worker-1&#93; JobRunShell:207 - Job LinearExecutionJobs.BatchRequestJob-2-1 threw a JobExecutionException: org.quartz.JobExecutionException: org.apache.ambari.server.AmbariException: Exception occurred while performing request &#91;See nested exception: org.apache.ambari.server.AmbariException: Exception occurred while performing request&#93; at org.apache.ambari.server.scheduler.AbstractLinearExecutionJob.execute(AbstractLinearExecutionJob.java:94) at org.quartz.core.JobRunShell.run(JobRunShell.java:202) at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:573) Caused by: org.apache.ambari.server.AmbariException: Exception occurred while performing request at org.apache.ambari.server.scheduler.ExecutionScheduleManager.executeBatchRequest(ExecutionScheduleManager.java:479) at org.apache.ambari.server.state.scheduler.BatchRequestJob.doWork(BatchRequestJob.java:77) at org.apache.ambari.server.scheduler.AbstractLinearExecutionJob.execute(AbstractLinearExecutionJob.java:88) ... 2 more Caused by: com.sun.jersey.api.client.ClientHandlerException: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No name matching localhost found at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149) at com.sun.jersey.api.client.filter.CsrfProtectionFilter.handle(CsrfProtectionFilter.java:97) at org.apache.ambari.server.security.authorization.internal.InternalTokenClientFilter.handle(InternalTokenClientFilter.java:39) at com.sun.jersey.api.client.Client.handle(Client.java:648) at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670) at com.sun.jersey.api.client.WebResource.method(WebResource.java:311) at org.apache.ambari.server.scheduler.ExecutionScheduleManager.performApiRequest(ExecutionScheduleManager.java:619) at org.apache.ambari.server.scheduler.ExecutionScheduleManager.executeBatchRequest(ExecutionScheduleManager.java:469) ... 4 more Caused by: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No name matching localhost found at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1884) at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:276) at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:270) at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1341) at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:153) at sun.security.ssl.Handshaker.processLoop(Handshaker.java:868) at sun.security.ssl.Handshaker.process_record(Handshaker.java:804) at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1016) at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1312) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1339) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1323) at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:563) at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1091) at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250)",
            "id": "5779"
        },
        "5782": {
            "ground_truth": "0",
            "bug_report": "View: Files UI clean-up and adjustments\nView: Files UI clean-up and adjustments",
            "id": "5782"
        },
        "5783": {
            "ground_truth": "0",
            "bug_report": "Pig fails to install through blueprint\nDuring installation through blueprint pig install fails with next exception:Traceback (most recent call last): File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 41  in &lt;module&gt; PigClient().execute() File '/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py'  line 112  in execute method(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 30  in install self.configure(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 35  in configure pig() File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig.py'  line 40  in pig properties=params.pig_properties) File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/libraries/providers/properties_file.py'  line 48  in action_create mode = self.resource.mode File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 96  in action_create content = self._get_content() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 136  in _get_content return content() File '/usr/lib/python2.6/site-packages/resource_management/core/source.py'  line 47  in __call__ return self.get_content() File '/usr/lib/python2.6/site-packages/resource_management/core/source.py'  line 126  in get_content rendered = self.template.render(self.context) File '/usr/lib/python2.6/site-packages/jinja2/environment.py'  line 891  in render return self.environment.handle_exception(exc_info  True) File '&lt;template&gt;'  line 2  in top-level template code File '/usr/lib/python2.6/site-packages/jinja2/filters.py'  line 176  in do_dictsort return sorted(value.items()  key=sort_func)AttributeError: 'unicode' object has no attribute 'items'",
            "id": "5783"
        },
        "5855": {
            "ground_truth": "0",
            "bug_report": "Global properties are not being surfaced on service config page\nPost install  Global properties are not being surfaced on service config page.",
            "id": "5855"
        },
        "5866": {
            "ground_truth": "0",
            "bug_report": "Populate actions drop down of Slider App details page\nSlider App details page should have an actions dropdown with the following actions Freeze: show when status == RUNNING Thaw: show when status == FROZEN Flex: show when status != FINISHED Destroy: show when status == FROZENExcept for Thaw  all actions will show a confirmation dialog. Hitting OK will make the call. Destroy should call DELETE on the app endpoint Freeze and Thaw should call PUT on app endpoint app state being set to FROZEN or RUNNING.",
            "id": "5866"
        },
        "5886": {
            "ground_truth": "0",
            "bug_report": "Implement app_types endpoint to provide app definitions\nSlider Apps View should provide /api/v1/app_types endpoint to provide definitions of various app_types supported by this Slider Apps View. Only apps in this type can be created through UI.",
            "id": "5886"
        },
        "5894": {
            "ground_truth": "0",
            "bug_report": "New slider app wizard should show app-types from /apptypes endpoint\nCurrently the new slider app wizard shows hardcoded app-types. We should instead show only those app-types which are returned by http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes?fields=* endpoint.{ 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes?fields=*'  'items' : [ { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/ACCUMULO'  'id' : 'ACCUMULO'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'ACCUMULO_MASTER'  'name' : 'ACCUMULO_MASTER'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_MASTER'  'priority' : 1  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_MONITOR'  'name' : 'ACCUMULO_MONITOR'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_MONITOR'  'priority' : 3  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_GC'  'name' : 'ACCUMULO_GC'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_GC'  'priority' : 4  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_TRACER'  'name' : 'ACCUMULO_TRACER'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_TRACER'  'priority' : 5  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_TSERVER'  'name' : 'ACCUMULO_TSERVER'  'category' : 'SLAVE'  'displayName' : 'ACCUMULO_TSERVER'  'priority' : 2  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_CLIENT'  'name' : 'ACCUMULO_CLIENT'  'category' : 'CLIENT'  'displayName' : 'ACCUMULO_CLIENT'  'priority' : 0  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'The Apache Accumulo sorted  distributed key/value store is a robust /n scalable  high performance data storage system that features cell-based/n access control and customizable server-side processing. It is based on/n Google's BigTable design and is built on top of Apache Hadoop /n Zookeeper  and Thrift./n Requirements:/n 1. Ensure parent dir for path (accumulo-site/instance.dfs.dir) is accessible to the App owner.'  'typeName' : 'ACCUMULO'  'typePackageFileName' : 'accumulo_v151.zip'  'typeVersion' : '1.5.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/accumulo_v151.zip'  'config_types' : 'accumulo-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/accumulo-1.5.1-bin.tar.gz'  'site.accumulo-site.gc.port.client' : '0'  'site.accumulo-site.general.classpaths' : '$ACCUMULO_HOME/lib/accumulo-server.jar /n$ACCUMULO_HOME/lib/accumulo-core.jar /n$ACCUMULO_HOME/lib/accumulo-start.jar /n$ACCUMULO_HOME/lib/accumulo-fate.jar /n$ACCUMULO_HOME/lib/accumulo-proxy.jar /n$ACCUMULO_HOME/lib/[^.].*.jar /n$ZOOKEEPER_HOME/zookeeper[^.].*.jar /n$HADOOP_CONF_DIR /n$HADOOP_PREFIX/[^.].*.jar /n$HADOOP_PREFIX/lib/[^.].*.jar /n$HADOOP_PREFIX/share/hadoop/common/.*.jar /n$HADOOP_PREFIX/share/hadoop/common/lib/.*.jar /n$HADOOP_PREFIX/share/hadoop/hdfs/.*.jar /n$HADOOP_PREFIX/share/hadoop/mapreduce/.*.jar /n$HADOOP_PREFIX/share/hadoop/yarn/.*.jar /n/usr/lib/hadoop/.*.jar /n/usr/lib/hadoop/lib/.*.jar /n/usr/lib/hadoop-hdfs/.*.jar /n/usr/lib/hadoop-mapreduce/.*.jar /n/usr/lib/hadoop-yarn/.*.jar '  'site.accumulo-site.instance.dfs.dir' : '/apps/accumulo/data'  'site.accumulo-site.instance.secret' : 'DEFAULT'  'site.accumulo-site.instance.zookeeper.host' : '${ZK_HOST}'  'site.accumulo-site.master.port.client' : '0'  'site.accumulo-site.monitor.port.client' : '${ACCUMULO_MONITOR.ALLOCATED_PORT}'  'site.accumulo-site.monitor.port.log4j' : '0'  'site.accumulo-site.trace.port.client' : '0'  'site.accumulo-site.trace.token.property.password' : 'secret'  'site.accumulo-site.trace.user' : 'root'  'site.accumulo-site.tserver.cache.data.size' : '7M'  'site.accumulo-site.tserver.cache.index.size' : '20M'  'site.accumulo-site.tserver.memory.maps.max' : '80M'  'site.accumulo-site.tserver.port.client' : '0'  'site.accumulo-site.tserver.sort.buffer.size' : '50M'  'site.accumulo-site.tserver.walog.max.size' : '100M'  'site.global.accumulo_instance_name' : 'instancename'  'site.global.accumulo_root_password' : 'secret'  'site.global.app_install_dir' : '${AGENT_WORK_ROOT}/app/install'  'site.global.app_log_dir' : '${AGENT_LOG_ROOT}/app/log'  'site.global.app_pid_dir' : '${AGENT_WORK_ROOT}/app/run'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/accumulo-1.5.1'  'site.global.app_user' : 'yarn'  'site.global.gc_heapsize' : '64m'  'site.global.hadoop_conf_dir' : '/etc/hadoop/conf'  'site.global.hadoop_prefix' : '/usr/lib/hadoop'  'site.global.master_heapsize' : '128m'  'site.global.monitor_heapsize' : '64m'  'site.global.other_heapsize' : '128m'  'site.global.security_enabled' : 'false'  'site.global.tserver_heapsize' : '128m'  'site.global.user_group' : 'hadoop'  'site.global.zookeeper_home' : '/usr/lib/zookeeper' } }  { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/HBASE'  'id' : 'HBASE'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'HBASE_MASTER'  'name' : 'HBASE_MASTER'  'category' : 'MASTER'  'displayName' : 'HBASE_MASTER'  'priority' : 1  'instanceCount' : 1  'maxInstanceCount' : 2  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'HBASE_REGIONSERVER'  'name' : 'HBASE_REGIONSERVER'  'category' : 'SLAVE'  'displayName' : 'HBASE_REGIONSERVER'  'priority' : 2  'instanceCount' : 1  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'HBASE_CLIENT'  'name' : 'HBASE_CLIENT'  'category' : 'CLIENT'  'displayName' : 'HBASE_CLIENT'  'priority' : 0  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'Apache HBase is the Hadoop database  a distributed  scalable  big data store./n Requirements:/n 1. Ensure parent dir for path (hbase-site/hbase.rootdir) is accessible to the App owner./n 2. Ensure ZK root (hbase-site/zookeeper.znode.parent) is unique for the App instance.'  'typeName' : 'HBASE'  'typePackageFileName' : 'hbase_v096 (1).zip'  'typeVersion' : '0.96.0.2.1.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/hbase_v096.zip'  'config_types' : 'core-site hdfs-site hbase-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/hbase-0.96.1-hadoop2-bin.tar.gz'  'site.core-site.fs.defaultFS' : '${NN_URI}'  'site.global.app_install_dir' : '${AGENT_WORK_ROOT}/app/install'  'site.global.app_log_dir' : '${AGENT_LOG_ROOT}/app/log'  'site.global.app_pid_dir' : '${AGENT_WORK_ROOT}/app/run'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/hbase-0.96.1-hadoop2'  'site.global.app_user' : 'yarn'  'site.global.ganglia_server_host' : '${NN_HOST}'  'site.global.ganglia_server_id' : 'Application1'  'site.global.ganglia_server_port' : '8667'  'site.global.hbase_master_heapsize' : '1024m'  'site.global.hbase_regionserver_heapsize' : '1024m'  'site.global.security_enabled' : 'false'  'site.global.user_group' : 'hadoop'  'site.hbase-site.hbase.client.keyvalue.maxsize' : '10485760'  'site.hbase-site.hbase.client.scanner.caching' : '100'  'site.hbase-site.hbase.cluster.distributed' : 'true'  'site.hbase-site.hbase.defaults.for.version.skip' : 'true'  'site.hbase-site.hbase.hregion.majorcompaction' : '86400000'  'site.hbase-site.hbase.hregion.max.filesize' : '10737418240'  'site.hbase-site.hbase.hregion.memstore.block.multiplier' : '2'  'site.hbase-site.hbase.hregion.memstore.flush.size' : '134217728'  'site.hbase-site.hbase.hregion.memstore.mslab.enabled' : 'true'  'site.hbase-site.hbase.hstore.blockingStoreFiles' : '10'  'site.hbase-site.hbase.hstore.compactionThreshold' : '3'  'site.hbase-site.hbase.hstore.flush.retries.number' : '120'  'site.hbase-site.hbase.local.dir' : '${hbase.tmp.dir}/local'  'site.hbase-site.hbase.master.info.port' : '${HBASE_MASTER.ALLOCATED_PORT}'  'site.hbase-site.hbase.regionserver.global.memstore.lowerLimit' : '0.38'  'site.hbase-site.hbase.regionserver.global.memstore.upperLimit' : '0.4'  'site.hbase-site.hbase.regionserver.handler.count' : '60'  'site.hbase-site.hbase.regionserver.info.port' : '0'  'site.hbase-site.hbase.regionserver.port' : '0'  'site.hbase-site.hbase.rootdir' : '${NN_URI}/apps/hbase/data'  'site.hbase-site.hbase.security.authentication' : 'simple'  'site.hbase-site.hbase.security.authorization' : 'false'  'site.hbase-site.hbase.stagingdir' : '${NN_URI}/apps/hbase/staging'  'site.hbase-site.hbase.superuser' : 'yarn'  'site.hbase-site.hbase.tmp.dir' : '${AGENT_WORK_ROOT}/work/app/tmp'  'site.hbase-site.hbase.zookeeper.property.clientPort' : '2181'  'site.hbase-site.hbase.zookeeper.quorum' : '${ZK_HOST}'  'site.hbase-site.hbase.zookeeper.useMulti' : 'true'  'site.hbase-site.hfile.block.cache.size' : '0.40'  'site.hbase-site.zookeeper.session.timeout' : '30000'  'site.hbase-site.zookeeper.znode.parent' : '/hbase-unsecure'  'site.hdfs-site.dfs.namenode.http-address' : '${NN_HOST}:50070'  'site.hdfs-site.dfs.namenode.https-address' : '${NN_HOST}:50470' } }  { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/STORM'  'id' : 'STORM'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'NIMBUS'  'name' : 'NIMBUS'  'category' : 'MASTER'  'displayName' : 'NIMBUS'  'priority' : 1  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'STORM_REST_API'  'name' : 'STORM_REST_API'  'category' : 'MASTER'  'displayName' : 'STORM_REST_API'  'priority' : 2  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'SUPERVISOR'  'name' : 'SUPERVISOR'  'category' : 'SLAVE'  'displayName' : 'SUPERVISOR'  'priority' : 5  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'STORM_UI_SERVER'  'name' : 'STORM_UI_SERVER'  'category' : 'MASTER'  'displayName' : 'STORM_UI_SERVER'  'priority' : 3  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'DRPC_SERVER'  'name' : 'DRPC_SERVER'  'category' : 'MASTER'  'displayName' : 'DRPC_SERVER'  'priority' : 4  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'Apache Hadoop Stream processing framework'  'typeName' : 'STORM'  'typePackageFileName' : 'storm_v091.zip'  'typeVersion' : '0.9.1.2.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/storm_v091.zip'  'config_types' : 'storm-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/apache-storm-0.9.1.2.1.1.0-237.tar.gz'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237'  'site.global.app_user' : 'yarn'  'site.global.ganglia_server_host' : '${NN_HOST}'  'site.global.ganglia_server_id' : 'Application2'  'site.global.rest_api_admin_port' : '${STORM_REST_API.ALLOCATED_PORT}'  'site.global.rest_api_port' : '${STORM_REST_API.ALLOCATED_PORT}'  'site.global.security_enabled' : 'false'  'site.global.user_group' : 'hadoop'  'site.storm-site.dev.zookeeper.path' : '${AGENT_WORK_ROOT}/app/tmp/dev-storm-zookeeper'  'site.storm-site.drpc.childopts' : '-Xmx768m'  'site.storm-site.drpc.invocations.port' : '${DRPC_SERVER.ALLOCATED_PORT}'  'site.storm-site.drpc.port' : '${DRPC_SERVER.ALLOCATED_PORT}'  'site.storm-site.drpc.queue.size' : '128'  'site.storm-site.drpc.request.timeout.secs' : '600'  'site.storm-site.drpc.worker.threads' : '64'  'site.storm-site.java.library.path' : '/usr/local/lib:/opt/local/lib:/usr/lib'  'site.storm-site.logviewer.appender.name' : 'A1'  'site.storm-site.logviewer.childopts' : '-Xmx128m'  'site.storm-site.logviewer.port' : '${SUPERVISOR.ALLOCATED_PORT}'  'site.storm-site.nimbus.childopts' : '-Xmx1024m -Djava.security.auth.login.config=/etc/storm/storm_jaas.conf -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Nimbus_JVM'  'site.storm-site.nimbus.cleanup.inbox.freq.secs' : '600'  'site.storm-site.nimbus.file.copy.expiration.secs' : '600'  'site.storm-site.nimbus.host' : '${NIMBUS_HOST}'  'site.storm-site.nimbus.inbox.jar.expiration.secs' : '3600'  'site.storm-site.nimbus.monitor.freq.secs' : '10'  'site.storm-site.nimbus.reassign' : 'true'  'site.storm-site.nimbus.supervisor.timeout.secs' : '60'  'site.storm-site.nimbus.task.launch.secs' : '120'  'site.storm-site.nimbus.task.timeout.secs' : '30'  'site.storm-site.nimbus.thrift.max_buffer_size' : '1048576'  'site.storm-site.nimbus.thrift.port' : '${NIMBUS.ALLOCATED_PORT}'  'site.storm-site.nimbus.topology.validator' : 'backtype.storm.nimbus.DefaultTopologyValidator'  'site.storm-site.storm.cluster.mode' : 'distributed'  'site.storm-site.storm.local.dir' : '${AGENT_WORK_ROOT}/app/tmp/storm'  'site.storm-site.storm.local.mode.zmq' : 'false'  'site.storm-site.storm.messaging.netty.buffer_size' : '5242880'  'site.storm-site.storm.messaging.netty.client_worker_threads' : '1'  'site.storm-site.storm.messaging.netty.max_retries' : '30'  'site.storm-site.storm.messaging.netty.max_wait_ms' : '1000'  'site.storm-site.storm.messaging.netty.min_wait_ms' : '100'  'site.storm-site.storm.messaging.netty.server_worker_threads' : '1'  'site.storm-site.storm.messaging.transport' : 'backtype.storm.messaging.netty.Context'  'site.storm-site.storm.thrift.transport' : 'backtype.storm.security.auth.SimpleTransportPlugin'  'site.storm-site.storm.zookeeper.connection.timeout' : '15000'  'site.storm-site.storm.zookeeper.port' : '2181'  'site.storm-site.storm.zookeeper.retry.interval' : '1000'  'site.storm-site.storm.zookeeper.retry.intervalceiling.millis' : '30000'  'site.storm-site.storm.zookeeper.retry.times' : '5'  'site.storm-site.storm.zookeeper.root' : '/storm'  'site.storm-site.storm.zookeeper.servers' : '['${ZK_HOST}']'  'site.storm-site.storm.zookeeper.session.timeout' : '20000'  'site.storm-site.supervisor.childopts' : '-Xmx256m -Djava.security.auth.login.config=/etc/storm/storm_jaas.conf -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=${SUPERVISOR.ALLOCATED_PORT} -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Supervisor_JVM'  'site.storm-site.supervisor.enable' : 'true'  'site.storm-site.supervisor.heartbeat.frequency.secs' : '5'  'site.storm-site.supervisor.monitor.frequency.secs' : '3'  'site.storm-site.supervisor.slots.ports' : '[${SUPERVISOR.ALLOCATED_PORT}  ${SUPERVISOR.ALLOCATED_PORT}]'  'site.storm-site.supervisor.worker.start.timeout.secs' : '120'  'site.storm-site.supervisor.worker.timeout.secs' : '30'  'site.storm-site.task.heartbeat.frequency.secs' : '3'  'site.storm-site.task.refresh.poll.secs' : '10'  'site.storm-site.topology.acker.executors' : 'null'  'site.storm-site.topology.builtin.metrics.bucket.size.secs' : '60'  'site.storm-site.topology.debug' : 'false'  'site.storm-site.topology.disruptor.wait.strategy' : 'com.lmax.disruptor.BlockingWaitStrategy'  'site.storm-site.topology.enable.message.timeouts' : 'true'  'site.storm-site.topology.error.throttle.interval.secs' : '10'  'site.storm-site.topology.executor.receive.buffer.size' : '1024'  'site.storm-site.topology.executor.send.buffer.size' : '1024'  'site.storm-site.topology.fall.back.on.java.serialization' : 'true'  'site.storm-site.topology.kryo.factory' : 'backtype.storm.serialization.DefaultKryoFactory'  'site.storm-site.topology.max.error.report.per.interval' : '5'  'site.storm-site.topology.max.spout.pending' : 'null'  'site.storm-site.topology.max.task.parallelism' : 'null'  'site.storm-site.topology.message.timeout.secs' : '30'  'site.storm-site.topology.optimize' : 'true'  'site.storm-site.topology.receiver.buffer.size' : '8'  'site.storm-site.topology.skip.missing.kryo.registrations' : 'false'  'site.storm-site.topology.sleep.spout.wait.strategy.time.ms' : '1'  'site.storm-site.topology.spout.wait.strategy' : 'backtype.storm.spout.SleepSpoutWaitStrategy'  'site.storm-site.topology.state.synchronization.timeout.secs' : '60'  'site.storm-site.topology.stats.sample.rate' : '0.05'  'site.storm-site.topology.tick.tuple.freq.secs' : 'null'  'site.storm-site.topology.transfer.buffer.size' : '1024'  'site.storm-site.topology.trident.batch.emit.interval.millis' : '500'  'site.storm-site.topology.tuple.serializer' : 'backtype.storm.serialization.types.ListDelegateSerializer'  'site.storm-site.topology.worker.childopts' : 'null'  'site.storm-site.topology.worker.shared.thread.pool.size' : '4'  'site.storm-site.topology.workers' : '1'  'site.storm-site.transactional.zookeeper.port' : 'null'  'site.storm-site.transactional.zookeeper.root' : '/transactional'  'site.storm-site.transactional.zookeeper.servers' : 'null'  'site.storm-site.ui.port' : '${STORM_UI_SERVER.ALLOCATED_PORT}'  'site.storm-site.worker.childopts' : '-Xmx768m -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Worker_%ID%_JVM'  'site.storm-site.worker.heartbeat.frequency.secs' : '1'  'site.storm-site.zmq.hwm' : '0'  'site.storm-site.zmq.linger.millis' : '5000'  'site.storm-site.zmq.threads' : '1' } } ]}",
            "id": "5894"
        },
        "5895": {
            "ground_truth": "0",
            "bug_report": "Suppress any debug and info messages from package managers in setupAgent.py\nDuring bootstrapping of agents  installation retrieve versions of available packages from system package manager. After some test was founded that in different situations package managers can produce specific info/debug output which can be reason of wrong parsing of output.Example:Zypper command:se2mon1400652583-9:/etc/zypp/repos.d # zypper search -s --match-exact ambari-agent Building repository 'Hortonworks Data Platform Utils Version - HDP-UTILS-1.1.0.16' cache &#91;done&#93;Building repository 'Hortonworks Data Platform Utils Version - HDP-UTILS-1.1.0.17' cache &#91;done&#93;Building repository 'Hosted (SLES_11)' cache &#91;done&#93;Building repository 'ambari-1.6.0 - Updates' cache &#91;done&#93;Building repository 'Ambari 1.x' cache &#91;done&#93;Building repository 'PostgreSQL and related packages (SLE_11_SP3)' cache &#91;done&#93;Loading repository data...Reading installed packages...S | Name | Type | Version | Arch | Repository -----------------------------------+---------------------- ambari-agent  package  1.6.0-46  x86_64  ambari-1.6.0 - Updates ambari-agent  package  1.2.0.1-1  x86_64  Ambari 1.xParsed as: Building repository 'ambari-1.6.0 We should suppress any debug/info output in setupAgent.py to avoid any unexpected situation.",
            "id": "5895"
        },
        "5915": {
            "ground_truth": "0",
            "bug_report": "View: Files UI clean-up and adjustments (PART 2)\nAs a part of this ticket 1) Remove rename icon from infront of the file and make it a different column.2) reorder the icon bar in the sequence Download  Move  Rename and Delete",
            "id": "5915"
        },
        "5922": {
            "ground_truth": "0",
            "bug_report": "Predicates don't work on fields with float values\nAPI should be able to process filter with predicates(&lt; &gt; =) for fields with float values; For example field: metrics/load/load_one.Currently the Greater than predicate will fail for values in between 0.0 and 1.0",
            "id": "5922"
        },
        "5930": {
            "ground_truth": "1",
            "bug_report": "Some HBase properties are empty  but required to be filled\nThis HBase properties are empty after install via blueprint: hbase.coprocessor.region.classes hbase.coprocessor.master.classesBut they required to be filled.After enabling security they became filled.",
            "id": "5930"
        },
        "5932": {
            "ground_truth": "0",
            "bug_report": "Slider apps table does not remove entry when app is removed\nLets say I have a running app. I freeze it and then destroy it. The app goes away from the /apps response. However the UI still continues to show it. I think the mapper is not removing deleted entries from the model.Mapper should remove entries not being sent by /apps.",
            "id": "5932"
        },
        "5935": {
            "ground_truth": "0",
            "bug_report": "Maintenance state and status commands perf improvements.\ngetEffectiveState should not fetch map of all hosts every time Status commands should not be sent until component is installed",
            "id": "5935"
        },
        "5941": {
            "ground_truth": "0",
            "bug_report": "Ambari should generate config files in sorted order\nAmbari should generate config files sorted by key. It would make doing compares between files much easier.",
            "id": "5941"
        },
        "5956": {
            "ground_truth": "0",
            "bug_report": "DB connection check error if jdk_name does not exist.\nWe can get such situation when user using custom java.",
            "id": "5956"
        },
        "5957": {
            "ground_truth": "0",
            "bug_report": "Bootstrap API call says bootstrap is running even though all agents have installed and registered\nUI keeps showing that the agents are being installed  because the bootstrap API GET call keeps returning that hostsStatus/status is RUNNINGI suspected time.sleep(1) instruction. If restart takes too long time  situation  when ambari-agent.log has not been created yet is possible. So  tail command returned not 0 retcode and exit from procedure. But in logs I cant see any non-zero retcodes. Thus  too short logs could be explaned by this issue.",
            "id": "5957"
        },
        "5960": {
            "ground_truth": "0",
            "bug_report": "Add support for auth proxy\nWhen using a proxy for internet access  add support to set properties for proxy user and password.Dhttp.proxyUser=someUserName -Dhttp.proxyPassword=somePassword",
            "id": "5960"
        },
        "5994": {
            "ground_truth": "0",
            "bug_report": "Storm UI in Ambari quick link fails when Storm UI server is not co-hosted with nimbus host\nquick link for Storm UI was linked with nimbus server. it should use Storm UI server host.",
            "id": "5994"
        },
        "6003": {
            "ground_truth": "0",
            "bug_report": "metrics hostname is not correct for storm\nSome of the storm configs have extra 'localhost' appended to the host name.'nimbus.childopts' : '-Xmx1024m -Djava.security.auth.login.config=/etc/storm/conf/storm_jaas.conf -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8649 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Nimbus_JVM''supervisor.childopts' : '-Xmx256m -Djava.security.auth.login.config=/etc/storm/conf/storm_jaas.conf -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=56431 -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8650 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Supervisor_JVM''worker.childopts' : '-Xmx768m -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8650 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Worker_%ID%_JVM'",
            "id": "6003"
        },
        "6014": {
            "ground_truth": "0",
            "bug_report": "HDFS alert hangs for a long time after enabling Maintenance mode\nSTR: Stop HDFS with started Nagios service. Turn on Maintenance mode for HDFS.Result: All alerts dissapear  except 'HDFS capacity utilization' - it hangs for a long time (in my case it was smthng about 10-11 minutes)",
            "id": "6014"
        },
        "6034": {
            "ground_truth": "0",
            "bug_report": "Services -> Configs page for Yarn  HIVE  MapReduce services is not displayed\nConfig page for services that use App.YARNDefaultsProvider is not displayed.Following JS error thrown:Uncaught TypeError: Cannot call method 'forEach' of null yarn_defaults_provider.js:291",
            "id": "6034"
        },
        "6044": {
            "ground_truth": "0",
            "bug_report": "Issues with jdbc properties\nremove dots in --help params description and in warningscopy fails if jdbc selected by --jdbc-driver is already in resourceshide other points of ambari-server setup if jdbc options are passed and server is running",
            "id": "6044"
        },
        "6045": {
            "ground_truth": "0",
            "bug_report": "Master components are missing\nMaster components info is missing in service summary.",
            "id": "6045"
        },
        "6048": {
            "ground_truth": "0",
            "bug_report": "Ambari Agent script should check for running processes before starting\nPROBLEM: If the Ambari Agent installation fails for whatever reason then a process of ambari-agent is left running. This results in the ambari-agent status to show as it not running. If you then start another ambari-agent it dies because the port is already in use.If the script could check the PID file and check for a running process then it would resolve this issue.BUSINESS IMPACT: Not a huge business impact as the workaround is to kill the running ambari-agent processWorkaround: Kill running ambari-agent process before startingANALYSIS: I cannot reproduce this issue in house and the SE who raised it can not reproduce on demand.",
            "id": "6048"
        },
        "6053": {
            "ground_truth": "0",
            "bug_report": "Add Host wizard get stuck on Confirm Hosts step\nAdd Host wizard get stuck on Confirm Hosts step",
            "id": "6053"
        },
        "6056": {
            "ground_truth": "0",
            "bug_report": "Agent Custom Command Output Coerces Integers to Floats\nWhen posting a command such as{ 'RequestInfo': { 'action': 'check_host'  'context': 'Check host'  'parameters': { 'check_execute_list': 'host_resolution_check'  'hosts': 'c6401.ambari.apache.org  c6402.ambari.apache.org  c6403.ambari.apache.org  foobar'  'threshold': '20' } }  'Requests/resource_filters': [ { 'hosts': 'c6401.ambari.apache.org c6402.ambari.apache.org' } ]The returned result from the custom action has some integer values coerced into floats: 'structured_out' : { 'host_resolution_check' : { 'exit_code' : '0'  'failed_count' : 0.0  'failures' : [ ]  'message' : 'All hosts resolved to an IP address.'  'success_count' : 4.0 }The structured_output written out to disk does NOT have the float values:{'host_resolution_check': {'failures': []  'message': 'All hosts resolved to an IP address.'  'failed_count': 0  'success_count': 4  'exit_code': '0'}} Therefore this is a problem with the framework and not the command.",
            "id": "6056"
        },
        "6059": {
            "ground_truth": "0",
            "bug_report": "Add refreshQueues custom command to YARN service\nCall refreshqueues from the REST API (replace resource.manager.host and YourClusterName). And be sure to include header 'X-Requested-By' : 'ambari' and set authentication.POST/api/v1/clusters/YourClusterName/requests/{ 'RequestInfo' : { 'command' : 'REFRESHQUEUES'  'context' : 'Refresh YARN Capacity Scheduler' }  'Requests/resource_filters': [{ 'service_name' : 'YARN'  'component_name' : 'RESOURCEMANAGER'  'hosts' : 'resource.manager.host' }]}",
            "id": "6059"
        },
        "6063": {
            "ground_truth": "0",
            "bug_report": "'ambari-server start command' hangs if was executed via ssh command\nProblem:Ambari Server command 'ssh root@vmhost ambari-server start' hangs on message 'Ambari Server 'start' completed successfully.'. Same command executed successfully on the local console  as a result this behavior can be reproduced only via remote execution of commands.How to reproduce: Deploy server Stop server locally Start server from another host using ssh command",
            "id": "6063"
        },
        "6080": {
            "ground_truth": "0",
            "bug_report": "Hosts Components filter on Service summary page doesn't work\nWhen user clicks on some component filter on Service summary page  it opens hosts page  but filter is not applied.",
            "id": "6080"
        },
        "6087": {
            "ground_truth": "0",
            "bug_report": "Multiple ATS appear on YARN summary page\nSTR1. Go to add Service Wizard.2. Select some new services and proceed to deploy.3. Close Wizard (Esc-button).4. Wait a little bit (maybe page-refresh needed).5. Go to YARN summary.6. New 'none' components will appear periodically.See screenshot.",
            "id": "6087"
        },
        "6106": {
            "ground_truth": "0",
            "bug_report": "Customize the Hadoop metrics sink to write to MySQL store\nThe SqlServerSink should support pushing metrics to MySQL store.This Jira addresses changes needed to support sink to a MySQL store.",
            "id": "6106"
        },
        "6112": {
            "ground_truth": "0",
            "bug_report": "Filter by alerts fails on Hosts table\nSteps to reproduce:1. Go to Hosts page2. Choose filter AlertsResult:Hosts are not filtered by alerts.The request with filter by alerts has incorrect url data.",
            "id": "6112"
        },
        "6113": {
            "ground_truth": "0",
            "bug_report": "Nagios install fails on SLES due to php5-json not available\nUsing SLES 11 SP3 quick-start image on EC2. Doesn't look like php5-json is available  but php53-json is available.WORKAROUND:I modified NAGIOS/metainfo.xml and this worked. &lt;package&gt; &lt;name&gt;php5*-json&lt;/name&gt; &lt;/package&gt;",
            "id": "6113"
        },
        "6123": {
            "ground_truth": "0",
            "bug_report": "issues with dialog keypresses\n1) on 'manage config groups' pressing return does something even though return is not valid (i.e. can't save)2) Once you press return in #1  then you have to press esc twice to close the dialog3) once you open the nested dialog (to add a group)  esc closes the parent  then esc again  closes the nested dialog4) once you open the nested dialog  also notice it doesn't start focus on the name field  you have to click to get that focus.",
            "id": "6123"
        },
        "6125": {
            "ground_truth": "0",
            "bug_report": "Ambari Groovy client enhancements\nAmbari REST client (Groovy) enhancements -refactor - simplified unit tests taking leverage groovy's metaclass capabilities-more REST calls implemented-api compatibility with version 1.6.0",
            "id": "6125"
        },
        "6137": {
            "ground_truth": "0",
            "bug_report": "Bulk operations confirmation popup\nHosts page  'Actions' menu (dropdown)Each action shows confirmation popup with list of affected hosts.For big cluster this may be 2000+ hosts.If there are more than 3 hosts  then show 'host1  host2  host3  and X more hosts show all'",
            "id": "6137"
        },
        "6140": {
            "ground_truth": "0",
            "bug_report": "Step3. Hosts checks requests\nProceed to step3  wait while registration is complete.Host checks requests are set every second.UI should set next request only when previous is completed.",
            "id": "6140"
        },
        "6146": {
            "ground_truth": "0",
            "bug_report": "It's not possible to input 'Enter' on 'Target hosts' textarea on 2nd step of Installer wizard'\nIt's not possible to input 'Enter' on 'Target hosts' textarea.",
            "id": "6146"
        },
        "6147": {
            "ground_truth": "0",
            "bug_report": "Ambari Dashboard page  click NameNode link returns wrong page\nSTR:1. go to dashboard page2. click NameNode link inside HDFS Links widget.instead of go to NameNode host detail page  it returns an empty page",
            "id": "6147"
        },
        "6155": {
            "ground_truth": "0",
            "bug_report": "JS error on POST config group request (step7 installer)\nGo to installer step7Click override for some propertySelect 'New Config Group'Click 'OK'JS-error appears - 404 error. Missing clusterName in request URL.",
            "id": "6155"
        },
        "6162": {
            "ground_truth": "0",
            "bug_report": "Behavior change: host filtering no longer handles startsWith matches\nIf I have a host that has IP 10.0.2.15  in Ambari 1.6.0 if I start to filter by IP (by typing 10.0....)  the hosts that match 'startsWith' stay displayed.In Ambari 1.6.1  now it only does exact match  so once I start typing  all hosts disappear until I finally type the whole thing in for exact match.Hosts/ip.matches(10)",
            "id": "6162"
        },
        "6169": {
            "ground_truth": "0",
            "bug_report": "Installer wizard: ambari web-client issues invalid requests after switching stacks\nSteps To Reproduce Select 2.X stack and go ahead to 'Select Services' page Navigate back to 'Select Stack' page and select 1.x stack Go ahead to step-8 'Review' page. On clicking next  API call to create components for HDFS service fails with UI displaying an error message.Invalid Request: Unsupported or invalid component in stack  clusterName=cc  serviceName=HDFS  componentName=JOURNALNODE  stackInfo=HDP-1.3",
            "id": "6169"
        },
        "6184": {
            "ground_truth": "1",
            "bug_report": "Incorrect value for started_count of Datanode component\nSTR:  Installed a 3-node cluster for HDP 1.3 stack HDFS+MapReduce+Nagios+Ganglia+zooKeeper installed with slave components installed on all 3 hosts. Enable security with no kerberos setup On expected failure of security wizard  Disable security. After successfully disabling security  Following API returns incorrect number for started_count of Datanode. It says 0 but Datanode is actually running on all hostshttp://server:8080/api/v1/clusters/c1/components/?ServiceComponentInfo/category.in(SLAVE CLIENT)&amp;fields=ServiceComponentInfo/service_name ServiceComponentInfo/installed_count ServiceComponentInfo/started_count ServiceComponentInfo/total_count&amp;minimal_response=trueReason:During wrong kerberos setup DN processes fail to start  but leave stale pid file owned by root. Next one DN start command starts DN process  but can not override pid file. So the server considers DN as stopped. If we start DN once more  commands fail soon after start (due to lock file at data dir owned by already running DN). Agent reports to server that DN is not running  so server displays a correct information from his point of view.",
            "id": "6184"
        },
        "6194": {
            "ground_truth": "0",
            "bug_report": "Unsuitable height of dropdown menu on metrics page\nSTR:Delete all widgets from dashboard.Go to Metrics-&gt;Add menu.Result: Appeared inappropriate dropdown: bad_metrics.png",
            "id": "6194"
        },
        "6197": {
            "ground_truth": "0",
            "bug_report": "Decommissioned running DataNode has 'delete' menu item in action pulldown\nDelete operation is not allowed for a hostComponent if it is in STARTED state.Currently delete menu item is shown when a hostComponent is flagged decommissioned and in STARTED state. Performing delete operation in this condition returns API 500 server error. If the hostComponent is brought in INSTALLED state with the decommissioned flag and then delete operation is performed then it happens as expectedDelete menu item should be shown When hostComponent is in INSTALLED state and should be grayed when it is in STARTED state. ambari-web client should not consider decommission status of a hostComponent while validating the required condition to enable/disable 'delete' menu item.",
            "id": "6197"
        },
        "6199": {
            "ground_truth": "0",
            "bug_report": "ListBoxes with hostnames on 'Select Hosts' page of 'Enable NameNode HA Wizard' do not work\nOn the second step of HA 'Enable NameNode HA Wizard' do not work listboxes with hostnames. Real additional components position does not depend from values in listboxes (see screenshots).It is possible to choose any host in any listbox  which is incorrect.",
            "id": "6199"
        },
        "6201": {
            "ground_truth": "0",
            "bug_report": "Fix sub-resource names in /stacks API\nThe /stacks api uses sub-resource names such as stackServices and serviceComponents instead of services and components which are the names of the resources specified in the URL. These incorrect resource names would need to be used in any queries for stack resources.For example:To get stack service named HDFS the URL would be:api/v1/stacks/HDP/versions/2.1/services/HDFSBut  if we wanted to do a query of for HDFS services across all versions:api/v1/stacks/HDP/versions?stackServices/StackServices/service_name=HDFSInstead this should be:api/v1/stacks/HDP/versions?services/StackServices/service_name=HDFSFix all sub-resource names that are returned and fix sub-resource names used in queries and partial response.",
            "id": "6201"
        },
        "6203": {
            "ground_truth": "0",
            "bug_report": "Unable to assign host in HA wizard\nSTR:1. Open HA wizard2. Proceed to Select Host step3. Try to select host for any master componentResult:nothing changing  js error emerge.",
            "id": "6203"
        },
        "6206": {
            "ground_truth": "0",
            "bug_report": "BGO popup: Incorrect number of tasks in category\nTasks with 'aborted' status are not included in any 'Aborted' category.",
            "id": "6206"
        },
        "6207": {
            "ground_truth": "0",
            "bug_report": "Installer Wizard Step 7-8: namenode_heapsize property incorrect value.\nOn the Step 7 namenode_heapsize value is empty and on deploy even if its value filled it saved as 'm' which cause error on NameNode startup.Part of object passed for configuration saving:namenode_heapsize: 'm'namenode_opt_maxnewsize: '200m'namenode_opt_newsize: '200m'nodemanager_heapsize: '1024'",
            "id": "6207"
        },
        "6208": {
            "ground_truth": "0",
            "bug_report": "Nagios 'Restart all components' button does not work\nOn service page for Nagios  under 'Service actions' menu  'Restart all' operation does not work correctly. There is dialog window after pressing  but after confirmation there is not any activity. In API there is not any new request.Note: described situation relates only for Nagios. Other services make restart correctly (including generating new requests in API).",
            "id": "6208"
        },
        "6215": {
            "ground_truth": "0",
            "bug_report": "Default add host sequence triggers many unseen before cluster-wide operations\nI've added 1 host through the add host wizardon deploy step all hosts were present. and install operations were performed on all of them.",
            "id": "6215"
        },
        "6221": {
            "ground_truth": "0",
            "bug_report": "Ambari Server reset show wrong commands for DB manipulation\nWhen performing ambari-server reset with external DB  ambari-server does nothing but outputs commands for resetting it manually.But:1. Commands are wrong  at least on Suse:su -postgres --command=psql -f /var/lib/ambari-server/resources/Ambari-DDL-Postgres-DROP.sql -v username=''ambari'' -v password=''bigdata''su: invalid option -- 'o'Try 'su --help' for more information.2. This commands should take in account that external DB can be located on the another host.3. Maybe the best option would be to give user ability to reset automatically  for example via command line switch like ambari-server reset -a",
            "id": "6221"
        },
        "6228": {
            "ground_truth": "0",
            "bug_report": "host checks 'Show Report' link is missing from dialog\nOn 'Confirm Hosts' step &gt; Hosts Check popup window  the 'Show Reports' link is missing even if the host check warnings existed.Reason:The newly added hosts checks (jdk  disk  repo and hostNameResolution) dont trigger the ''show reports' link to show up.",
            "id": "6228"
        },
        "6234": {
            "ground_truth": "1",
            "bug_report": "Security issue - private key password show in logs\nDuring generating private key and certificates using openssl password of key shown in logs:11:21:30 735 INFO [main] ShellCommandUtil:44 - Command openssl genrsa -des3 -passout pass:**** -out /var/lib/ambari-server/keys/ca.key 4096 was finished with exit code: 0 - the operation was completely successfully.11:21:30 750 INFO [main] ShellCommandUtil:44 - Command openssl req -passin pass:**** -new -key /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.csr -batch was finished with exit code: 0 - the operation was completely successfully.11:21:30 766 INFO [main] ShellCommandUtil:44 - Command open**** ca -create_serial -out /var/lib/ambari-server/keys/ca.crt -days 365 -keyfile /var/lib/ambari-server/keys/ca.key -key vgGAzzSaCPkI3F7UU7qZZY6CahDUTSnY7B9a8TH0YiGDB10LdJ -selfsign -extensions jdk7_ca -config /var/lib/ambari-server/keys/ca.config -batch -infiles /var/lib/ambari-server/keys/ca.csr was finished with exit code: 0 - the operation was completely successfully.11:21:30 773 INFO [main] ShellCommandUtil:44 - Command openssl pkcs12 -export -in /var/lib/ambari-server/keys/ca.crt -inkey /var/lib/ambari-server/keys/ca.key -certfile /var/lib/ambari-server/keys/ca.crt -out /var/lib/ambari-server/keys/keystore.p12 -password pass:**** -passin pass:**** see '-key vgGAzzSaCPkI3F7UU7qZZY6CahDUTSnY7B9a8TH0YiGDB10LdJ'",
            "id": "6234"
        },
        "6236": {
            "ground_truth": "0",
            "bug_report": "Hosts page: there is no indication that filtering/sorting/paging is happening or not (confusing)\nIn 1.6.1  filtering/sorting/paging on the Hosts page has been converted from client-side to server-side in order to address scalability issues.As a result of that  the responsiveness of UI is dependent upon how quickly the server can respond to filtering/sorting/paging calls (and also depends on the network). While UI is waiting for the new table content to come from the server  there should be some indication that work is in progress. For example  we can put an overlay on the table with a spinner (gray out the table with a spinner on top - kind of like when switching filters on JIRA's Agile Board).",
            "id": "6236"
        },
        "6244": {
            "ground_truth": "0",
            "bug_report": "Restart icon is present after Service Actions->Restart all button click.\nSTR:Change property for some service (Hive as example)Save changesClick Service Actions-&gt;Restart all buttonActual result:Service Actions-&gt;Restart all button does not work (do nothing). Restart passed  but restart icon still present.Expected result:Service Actions-&gt;Restart all button works. Restart passed  restart icon is not present after action.",
            "id": "6244"
        },
        "6247": {
            "ground_truth": "0",
            "bug_report": "Add host stops all services\nSTR Install cluster Add hostActual resultAfter adding host all components on all hosts are stoppedExpected resultAll components on all hosts are started",
            "id": "6247"
        },
        "6265": {
            "ground_truth": "0",
            "bug_report": "Have spinners instead of charts at Dashboard and Service tabs on IE 11\nSTR:1) Deploy cluster with defualt settings.2) Navigate on Dashboard pageExpected result:All charts are present.Actual result:Have spinners instead some charts.",
            "id": "6265"
        },
        "6266": {
            "ground_truth": "0",
            "bug_report": "Get sql metrics logic should be reviewed\nGet sql metrics logic should be reviewed",
            "id": "6266"
        },
        "6267": {
            "ground_truth": "0",
            "bug_report": "Add Services fails with a server error under some conditions\nUpon clicking on 'Deploy' from Review page in Add Services Wizard  sometimes the UI shows a server error saying 'Resource Already Exists'.It looks like the UI is trying to add client components on hosts that already have them.I've seen it for HDFS_CLIENT  MAPREDUCE2_CLIENT  etc.  when trying to add Oozie (for sure)  Storm (I think)  and possibly others.",
            "id": "6267"
        },
        "6268": {
            "ground_truth": "0",
            "bug_report": "step 6 pagination is slow\nstep 6 pagination is slow",
            "id": "6268"
        },
        "6270": {
            "ground_truth": "0",
            "bug_report": "Repoinfo.xml should use family tag rather than type tag\nThat is a bit confusing we should change that  since we changed the way itworks &lt;os type='redhat6'&gt;should be &lt;os family='redhat6'&gt;",
            "id": "6270"
        },
        "6273": {
            "ground_truth": "0",
            "bug_report": "Manage Config Groups: if Ganglia is not installed config group popup doesn't appear\nWe take some data from Ganglia metrics for hosts and if Ganglia is not installed browser throw js error which blocks popup initializing.",
            "id": "6273"
        },
        "6276": {
            "ground_truth": "0",
            "bug_report": "Prompt to put Service in Maintenance Mode when doing Rolling Restart / Service Stop\nWhen initiating Rolling Restart / Service Stop  they would like an option (via a checkbox  for example) to put the service in maintenance mode (if it is not already in MM) to avoid getting a lot of alerts.",
            "id": "6276"
        },
        "6292": {
            "ground_truth": "0",
            "bug_report": "Python client caches curl flags between requests causing problems\nCurrently if you use the python client  if you issue a DELETE request followed by a GET request  that GET request becomes a DELETE request. You can imagine why that's undesirable.The problem is that we set the CUSTOMREQUEST field on the DELETE  but we don't unset it on the next GET. pycurl sees it still set and assumes we're still doing a DELETE.",
            "id": "6292"
        },
        "6295": {
            "ground_truth": "0",
            "bug_report": "UI freezes for more than 2 seconds  every 15 seconds  on 2K-node cluster\nIt seems that we call App.componentConfigMapper every 15 seconds.This mapper takes more than 2 seconds to run. While the mapper is running  the entire UI is frozen.",
            "id": "6295"
        },
        "6298": {
            "ground_truth": "0",
            "bug_report": "Custom Command execution takes too long\nIt takes significant amount of time to populate clusterHostInfo for every Execution Cmd on a 2000 node cluster.org.apache.ambari.server.utils.StageUtils.getClusterHostInfo(Map  Cluster)",
            "id": "6298"
        },
        "6299": {
            "ground_truth": "0",
            "bug_report": "JMXPropertyProvider makes call to endpoint without checking support for properties\nCauses CPU usage go upto 1500 % and UI becomes unusable.1000's of Exception:00:01:50 666 ERROR [pool-1-thread-17] JMXPropertyProvider:539 - Caught exception getting JMX metrics : Connection refusedAll JMX endpoints called for any JMX metric.",
            "id": "6299"
        },
        "6311": {
            "ground_truth": "0",
            "bug_report": "Bulk Ops only targets a maximum of 'page size' hosts\nIt seems that the max number of target hosts for a bulk operation is capped by the current 'page size' on the Hosts page.For example  on the 2K cluster with 2K DataNodes: Go to Hosts page Click on Actions. Either going to Filtered Hosts (2005) or All Hosts (2005)  select 'DataNodes &gt; Stop' A confirmation popup tells me that 50 DataNodes are about to be stopped. This should have been ~ 2000 DataNodes instead. 50 is the current page size on the Hosts page that I set.Another scenario: Set page size to 50. Select 100 hosts using 'check all'  'next page'  then 'check all' Go to 'Actions &gt; Selected Hosts &gt; DataNodes &gt; Stop'. Again  it tries to perform actions on only 50 DataNodes.",
            "id": "6311"
        },
        "6322": {
            "ground_truth": "0",
            "bug_report": "Choosing bulk hosts to decommission on 120 node cluster on the hosts page just spins.\nSteps to reproduce1. Filter by DataNodes2. Increase the page size to 503. Select all 50 and decommision datanodes.This ends up with a spinner .This might be a blocker. Attaching snapshot.",
            "id": "6322"
        },
        "6327": {
            "ground_truth": "0",
            "bug_report": "Rolling Restart: 'only start stale' checkbox not worked functionally\nRolling restart page  there is a ' Only restart NodeManagers with stale configs' checkbox  nothing happened no matter if that one was checked or not. 1. On service(HDFS or YARN) actions &gt; restart all DN (NM). All DN(NM) will be restart no matter if the ' Only restart NodeManagers with stale configs' option checked.",
            "id": "6327"
        },
        "6332": {
            "ground_truth": "0",
            "bug_report": "Bulk Decommission: Background Operation Popup content looks broken\nheader in task details popup looks broken when we run decommission for many Datanodes.",
            "id": "6332"
        },
        "6335": {
            "ground_truth": "0",
            "bug_report": "Add service wizard removes any new property added to core-site and global after cluster installation\nAdd service wizard removes any new property added to core-site and global after cluster installation",
            "id": "6335"
        },
        "6337": {
            "ground_truth": "0",
            "bug_report": "Hosts page. Incorrect total number of hosts after filtering by installed component\nSTR1. Go to hosts page2. Filter by Components. Check DATANODE3. At Bottom right corner I see Show 10 | 1-10 of 10",
            "id": "6337"
        },
        "6340": {
            "ground_truth": "0",
            "bug_report": "filtering by selected hosts resets pagination\n1. page size 102. select all 10 hosts on first page3. go to page 2  select 2 hosts (total 12 selection)4. click on '12 host selected' and it goes to page 1  1-10 of 12 hosts (correct)5. click to go to page 2 (to see the 2 hosts selected) but it resets to page 1  1-10 of 103",
            "id": "6340"
        },
        "6349": {
            "ground_truth": "0",
            "bug_report": "After changing property and clicking save button in MapReduce Config Page  no comfirmation popup\nSTR1. Go to MapReduce Config Page2. Change JobTracker new generation size with a new value3. Clike Save buttonResult no confirmation popup  and save button turns grey  when switching pages the save button in the warning popup is not working neither.",
            "id": "6349"
        },
        "6350": {
            "ground_truth": "0",
            "bug_report": "Unable to restart all host-components on 110 node cluster\nOn a 110 node cluster I went to the hosts page and went into All Hosts &gt; Hosts &gt; Restart All Components. Following that I got an error dialog (image attached) and the below exceptions in the log21:03:20 017 ERROR [qtp1391464722-1188] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:Local Exception Stack:Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO requestoperationlevel (operation_level_id  cluster_name  host_component_name  host_name  level_name  request_id  service_name) VALUES (2  'Horton'  NULL  'horton-1.c.pramod-thangali.internal horton-10.c.pramod-thangali.internal horton-100.c.pramod-thangali.internal horton-101.c.pramod-thangali.internal horton-103.c.pramod-thangali.internal horton-104.c.pramod-thangali.internal horton-105.c.pramod-thangali.internal horton-106.c.pramod-thangali.internal horton-107.c.pramod-thangali.internal horton-108.c.pramod-thangali.internal horton-109.c.pramod-thangali.internal horton-11.c.pramod-thangali.internal horton-12.c.pramod-thangali.internal horton-13.c.pramod-thangali.internal horton-14.c.pramod-thangali.internal horton-15.c.pramod-thangali.internal horton-16.c.pramod-thangali.internal horton-17.c.pramod-thangali.internal horton-18.c.pramod-thangali.internal horton-19.c.pramod-thangali.internal horton-2.c.pramod-thangali.internal horton-20.c.pramod-thangali.internal horton-21.c.pramod-thangali.internal horton-22.c.pramod-thangali.internal horton-23.c.pramod-thangali.internal horton-24.c.pramod-thangali.internal horton-25.c.pramod-thangali.internal horton-26.c.pramod-thangali.internal horton-27.c.pramod-thangali.internal horton-28.c.pramod-thangali.internal horton-29.c.pramod-thangali.internal horton-3.c.pramod-thangali.internal horton-30.c.pramod-thangali.internal horton-31.c.pramod-thangali.internal horton-32.c.pramod-thangali.internal horton-33.c.pramod-thangali.internal horton-34.c.pramod-thangali.internal horton-35.c.pramod-thangali.internal horton-36.c.pramod-thangali.internal horton-37.c.pramod-thangali.internal horton-38.c.pramod-thangali.internal horton-39.c.pramod-thangali.internal horton-4.c.pramod-thangali.internal horton-40.c.pramod-thangali.internal horton-41.c.pramod-thangali.internal horton-42.c.pramod-thangali.internal horton-43.c.pramod-thangali.internal horton-44.c.pramod-thangali.internal horton-45.c.pramod-thangali.internal horton-46.c.pramod-thangali.internal horton-47.c.pramod-thangali.internal horton-48.c.pramod-thangali.internal horton-49.c.pramod-thangali.internal horton-5.c.pramod-thangali.internal horton-50.c.pramod-thangali.internal horton-51.c.pramod-thangali.internal horton-52.c.pramod-thangali.internal horton-53.c.pramod-thangali.internal horton-54.c.pramod-thangali.internal horton-55.c.pramod-thangali.internal horton-56.c.pramod-thangali.internal horton-57.c.pramod-thangali.internal horton-58.c.pramod-thangali.internal horton-59.c.pramod-thangali.internal horton-6.c.pramod-thangali.internal horton-60.c.pramod-thangali.internal horton-61.c.pramod-thangali.internal horton-62.c.pramod-thangali.internal horton-63.c.pramod-thangali.internal horton-64.c.pramod-thangali.internal horton-65.c.pramod-thangali.internal horton-66.c.pramod-thangali.internal horton-67.c.pramod-thangali.internal horton-68.c.pramod-thangali.internal horton-69.c.pramod-thangali.internal horton-7.c.pramod-thangali.internal horton-70.c.pramod-thangali.internal horton-71.c.pramod-thangali.internal horton-72.c.pramod-thangali.internal horton-73.c.pramod-thangali.internal horton-74.c.pramod-thangali.internal horton-75.c.pramod-thangali.internal horton-76.c.pramod-thangali.internal horton-77.c.pramod-thangali.internal horton-78.c.pramod-thangali.internal horton-79.c.pramod-thangali.internal horton-8.c.pramod-thangali.internal horton-80.c.pramod-thangali.internal horton-81.c.pramod-thangali.internal horton-82.c.pramod-thangali.internal horton-83.c.pramod-thangali.internal horton-84.c.pramod-thangali.internal horton-85.c.pramod-thangali.internal horton-86.c.pramod-thangali.internal horton-87.c.pramod-thangali.internal horton-88.c.pramod-thangali.internal horton-9.c.pramod-thangali.internal horton-90.c.pramod-thangali.internal horton-91.c.pramod-thangali.internal horton-92.c.pramod-thangali.internal horton-93.c.pramod-thangali.internal horton-94.c.pramod-thangali.internal horton-95.c.pramod-thangali.internal horton-96.c.pramod-thangali.internal horton-97.c.pramod-thangali.internal horton-98.c.pramod-thangali.internal horton-99.c.pramod-thangali.internal horton-master-1.c.pramod-thangali.internal horton-master-2.c.pramod-thangali.internal horton-master-3.c.pramod-thangali.internal'  'Host'  25  NULL) was aborted. Call getNextException to see the cause.Error Code: 0Call: INSERT INTO requestoperationlevel (operation_level_id  cluster_name  host_component_name  host_name  level_name  request_id  service_name) VALUES (?  ?  ?  ?  ?  ?  ?) bind =&gt; [7 parameters bound]Query: InsertObjectQuery(org.apache.ambari.server.orm.entities.RequestResourceFilterEntity@2c6ae575) at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:333) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1501) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeJDK12BatchStatement(DatabaseAccessor.java:875) at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:145) at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.appendCall(ParameterizedSQLBatchWritingMechanism.java:88) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:571) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:537) at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:1800) at org.eclipse.persistence.sessions.server.ClientSession.executeCall(ClientSession.java:286) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:207) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:193) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.insertObject(DatasourceCallQueryMechanism.java:342) at org.eclipse.persistence.internal.queries.StatementQueryMechanism.insertObject(StatementQueryMechanism.java:162) at org.eclipse.persistence.internal.queries.StatementQueryMechanism.insertObject(StatementQueryMechanism.java:177) at org.eclipse.persistence.internal.queries.DatabaseQueryMechanism.insertObjectForWrite(DatabaseQueryMechanism.java:471) at org.eclipse.persistence.queries.InsertObjectQuery.executeCommit(InsertObjectQuery.java:80) at org.eclipse.persistence.queries.InsertObjectQuery.executeCommitWithChangeSet(InsertObjectQuery.java:90) at org.eclipse.persistence.internal.queries.DatabaseQueryMechanism.executeWriteWithChangeSet(DatabaseQueryMechanism.java:286) at org.eclipse.persistence.queries.WriteObjectQuery.executeDatabaseQuery(WriteObjectQuery.java:58) at org.eclipse.persistence.queries.DatabaseQuery.execute(DatabaseQuery.java:852) at org.eclipse.persistence.queries.DatabaseQuery.executeInUnitOfWork(DatabaseQuery.java:751) at org.eclipse.persistence.queries.ObjectLevelModifyQuery.executeInUnitOfWorkObjectLevelModifyQuery(ObjectLevelModifyQuery.java:108) at org.eclipse.persistence.queries.ObjectLevelModifyQuery.executeInUnitOfWork(ObjectLevelModifyQuery.java:85) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.internalExecuteQuery(UnitOfWorkImpl.java:2875) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1602) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1584) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1535) at org.eclipse.persistence.internal.sessions.CommitManager.commitNewObjectsForClassWithChangeSet(CommitManager.java:224) at org.eclipse.persistence.internal.sessions.CommitManager.commitAllObjectsForClassWithChangeSet(CommitManager.java:191) at org.eclipse.persistence.internal.sessions.CommitManager.commitAllObjectsWithChangeSet(CommitManager.java:136) at org.eclipse.persistence.internal.sessions.AbstractSession.writeAllObjectsWithChangeSet(AbstractSession.java:3914) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitToDatabase(UnitOfWorkImpl.java:1419) at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitToDatabase(RepeatableWriteUnitOfWork.java:634) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitToDatabaseWithChangeSet(UnitOfWorkImpl.java:1509) at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:266) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1147) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commitInternal(EntityTransactionImpl.java:84) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:63) at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91) at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72) at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52) at org.apache.ambari.server.actionmanager.ActionDBAccessorImpl$$EnhancerByGuice$$ddd0d231.persistActions(&lt;generated&gt;) at org.apache.ambari.server.actionmanager.ActionManager.sendActions(ActionManager.java:95) at org.apache.ambari.server.actionmanager.ActionManager.sendActions(ActionManager.java:84) at org.apache.ambari.server.controller.AmbariManagementControllerImpl.createAction(AmbariManagementControllerImpl.java:2583) at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:124) at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:121) at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:237) at org.apache.ambari.server.controller.internal.RequestResourceProvider.createResources(RequestResourceProvider.java:121) at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:274) at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:75) at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:36) at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72) at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:103) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:72) at org.apache.ambari.server.api.services.RequestService.createRequests(RequestService.java:119) at sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)",
            "id": "6350"
        },
        "6359": {
            "ground_truth": "0",
            "bug_report": "Hosts page : mysterious 'selected hosts' keep coming back\nSee attached video.Somehow  there are always 10 hosts selected. After explicitly hitting 'clear selection'  page refresh somehow reverts back to 10 hosts being selected.",
            "id": "6359"
        },
        "6369": {
            "ground_truth": "0",
            "bug_report": "hostname wrapping with icon asterisks on assign slaves\nsee attached.",
            "id": "6369"
        },
        "6376": {
            "ground_truth": "0",
            "bug_report": "Excessive requests are sending on hosts page\nWhen we open hosts page  we can see  that we have excessive requests as spinner appears two times one after another.",
            "id": "6376"
        },
        "6379": {
            "ground_truth": "0",
            "bug_report": "In Host Detailed Page  decommissioned NM is labeled as STOPPED\nIn Yarn Service page  NodeManagers Status: 3 active / 0 lost / 0 unhealthy / 0 rebooted / 1 decommissionedIn Host Detailed Page of that decommissioned NM  the state of the NM is labeled as 'stopped'  which is not consistent comparing to Service page.API call:'href' : 'http://172.18.145.115:8080/api/v1/clusters/cl1/hosts/us3mon1404188570-3.cs1cloud.internal/host_components/NODEMANAGER'  'HostRoles' : { 'cluster_name' : 'cl1'  'component_name' : 'NODEMANAGER'  'desired_admin_state' : 'DECOMMISSIONED'  'desired_stack_id' : 'HDP-2.1'  'desired_state' : 'STARTED'  'host_name' : 'us3mon1404188570-3.cs1cloud.internal'  'maintenance_state' : 'OFF'  'service_name' : 'YARN'  'stack_id' : 'HDP-2.1'  'stale_configs' : false  'state' : 'INSTALLED' As a result  user cannot start the NodeManger as it is considered to be decommissioned by ResourceManger.",
            "id": "6379"
        },
        "6402": {
            "ground_truth": "0",
            "bug_report": "Many services failed to start when using custom user names and groups\nOn Customize services page set next values for users and groups:Name: ValueProxy group for Hive WebHCat Oozie and Falcon: custom-usersgrHDFS User: custom-hdfsMapReduce User: custom-mapredYARN User: custom-yarnHBase User: custom-hbaseHive User: custom-hiveHCat User: custom-hcatWebHCat User: custom-hcatOozie User: custom-oozieFalcon User: custom-falconStorm User: custom-stormZooKeeper User: custom-zookeeperGanglia User: custom-nobodyNagios User: custom-nagiosNagios Group: custom-nagiosgrSmoke Test User: ambari-qaTez User: custom-tezHadoop Group: custom-hadoopgrSkip group modifications during install: falseAfter deploy many services fail to start:MapReduce  Hbase  Hive  WebHcat  Falcon  OozieSame error for all:Call From ins1404365245-9.cs1cloud.internal/172.18.145.154 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused",
            "id": "6402"
        },
        "6405": {
            "ground_truth": "0",
            "bug_report": "Move Wizard: get stuck if wizard is running second time for the same component without page refresh\nSTR:1. Run Move Wizard for some master component.2. Do not click Complete button after all operation will be finished. Close wizard (confirm closing).3. Without page refresh open wizard for the same master component and try it to reassign to another host (ex. back to the previous host).Wizard will get stuck  as source host for master component will be calculated incorrectly.This issue is related to host components mapper. After first running of move wizard in the model there were 2 masters one with host before moving and the new one. Old host component was deleted on server but UI model still contains it. And it brakes algorithm of calculating source host on assign master step.",
            "id": "6405"
        },
        "6406": {
            "ground_truth": "0",
            "bug_report": "Move Wizard: assign master step next button is not disabled  when host input has error\nWhen Move Wizard is running on cluster with a large number of hosts  assign master step has input to enter hostname instead of select dropdown. If this dropdown is empty  it shows an error. But if in the same time target host was selected correctly the next button is enabled.",
            "id": "6406"
        },
        "6409": {
            "ground_truth": "0",
            "bug_report": "Postgres create script generates errors for clusterconfig\nManually running the postgres create sql script encountered errors.Errors: psql:Ambari-DDL-Postgres-CREATE.sql:22: ERROR: column 'config_attributes' specified more than once psql:Ambari-DDL-Postgres-CREATE.sql:103: ERROR: relation 'clusterconfig' does not exist psql:Ambari-DDL-Postgres-CREATE.sql:126: ERROR: relation 'clusterconfig' does not existRepro Steps:1. Spin up a CentOS 6.4 VM vagrant up c6401 vagrant ssh c6401 sudo su - wget http://public-repo-1.hortonworks.com/ambari/centos6/1.x/updates/1.6.0/ambari.repo cp ambari.repo /etc/yum.repos.d yum install ambari-server -y2. Setup a postgres database vi /etc/yum.repos.d/CentOS-Base.repo edit the &#91;base&#93; and &#91;updates&#93; sections by adding the following line without quotes  'exclude=postgresql*' yum localinstall http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-centos93-9.3-1.noarch.rpm yum install postgresql-server cd /etc/yum.repos.d/ service postgresql initdb chkconfig postgresql on service postgresql start3. Run the script cp /vagrant/Ambari-DDL-Postgres-CREATE.sql /var/lib/ambari-server/resources/ (assuming you have the latest file from trunk) cp /var/lib/ambari-server/resources/Ambari-DDL-Postgres-CREATE.sql /var/lib/pgsql/ su - postgres psql Follow step #1 at http://docs.hortonworks.com/HDPDocuments/Ambari-1.6.0.0/bk_ambari_reference/content/nndb-using-ambari-postgresql.html where $AMBARIDATABASE is ambari  $AMBARIUSER is postgres  and $AMBARISCHEMA is ambari. /i Ambari-DDL-Postgres-CREATE.sql;Then verify the errors. When done  issue /q on the psql command line to exit.",
            "id": "6409"
        },
        "6418": {
            "ground_truth": "0",
            "bug_report": "Config pages load slowly on 2k-node cluster for some services\nOn the 2k-node cluster  service config pages load slowly (though the load time has improved significantly from before).The load time depends on the service.Here are some sample load times:HDFS: 5sYARN: 17sMR2: 16sTEZ: 10sHBASE: 3sHIVE: 13sWEBHCAT: 3sFALCON: 3sSTORM: 17sOOZIE: 11sGANGLIA: 1sNAGIOS: 1sZOOKEEPER: 2sPIG: 2sSQOOP: n/a (no config page)",
            "id": "6418"
        },
        "6426": {
            "ground_truth": "0",
            "bug_report": "Link to ExtJS license in 'Choose Services' page has moved\nStep 4 of the Ambari installer has a broken link.When 'Choosing Services'  the link for the 'ExtJS' library license under the 'Oozie' service has moved from http://www.sencha.com/products/extjs/license/It should be updated to the URL below since it covers that Ext JS is available under GPL v3 license:http://www.sencha.com/legal/open-source-faq/",
            "id": "6426"
        },
        "6439": {
            "ground_truth": "0",
            "bug_report": "Filter state does not clear in Background Operation popup and causes a lot of confusion\nIn the Background Operations popup  the filter state persists at the Host and Task levels  even after closing the popup (the filter should be cleared once the user goes back up one level or closes the popup).This causes a lot of confusion because the user will expect these filters to be cleared but instead cannot see the hosts and tasks that they are expecting.",
            "id": "6439"
        },
        "6440": {
            "ground_truth": "0",
            "bug_report": "Unable to move NameNode\nSTR: Enabled NNHA.Active NameNode is on suse1101 hostStandby NameNode is on suse1102 host Stopped SNN. Opened move NN wizardResult: Next button is active when I choose existing topology (suse1101 and suse1102)Next button is inactive when I choose custom topology (suse1103 and suse1101)UPD: Reproduced also on non-HA cluster.",
            "id": "6440"
        },
        "6442": {
            "ground_truth": "0",
            "bug_report": "JS error occurs periodically when Job Details page is opened\n'Uncaught TypeError: Cannot read property 'properties' of undefined'at /ambari-web/app/views/common/quick_view_link_view.js:246",
            "id": "6442"
        },
        "6471": {
            "ground_truth": "0",
            "bug_report": "Add Services Wizard can corrupt config files unpredictably\nTesting using the 1.6.1 RC bits (branch-1.6.1  hash=ffb702b252a8b979529864ec5579465a122060b5) revealed that Add Services can corrupt config files in an unpredictable manner.We have seen: adding Pig resulted in core-site dropping all existing properties except for 'hadoop.proxyuser.falcon.hosts and hadoop.proxyuser.falcon.groups' add Storm resulted in core-site retaining all existing properties but dropping hadoop.proxyuser.* settings for all services except Falcon adding Storm dropped a number of existing properties in yarn-site (and NodeManagers can no longer restart after that)The bottomline here is that behavior seems pretty random (adding the same service on different clusters results in different behavior).Also  when adding a service  certain configs (like hdfs-site) were not cloned while most other configs (like yarn-site  mapred-site) get cloned even if no property changes are made regardless of which service is added.Update: changes to hdfs-site during Add Services Wizard never persists.",
            "id": "6471"
        },
        "6477": {
            "ground_truth": "0",
            "bug_report": "AddHost Wizard: error for existing host is not present\nSTR: Click AddHost Type existing host Click 'Register and Confirm' buttonActual result: There present only message about 'SSH Private Key is required'Excpected Result: SHould be also present message like 'These hosts are already exists'.---------------------------------Note: the excpected message only present if we type/select ss key and will click 'Register and Confirm'",
            "id": "6477"
        },
        "6483": {
            "ground_truth": "0",
            "bug_report": "JS errors on Jobs page if ATS is stopped\nIf ATS stopped  Jobs page continuously produces following js errors:NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?fields=events primaryfilters otherinfo&amp;secondaryFilter=tez:true&amp;limit=11&amp;_=1402578530622'proxy?...8530622'NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?limit=1&amp;secondaryFilter=tez:true&amp;_=1402578531674'proxy?...8531674'NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?limit=1&amp;secondaryFilter=tez:true&amp;_=1402578537654'",
            "id": "6483"
        },
        "6491": {
            "ground_truth": "0",
            "bug_report": "Default config group name contains 'undefined' instead of service name in Manage Config Groups popup\nDefault config group name contains 'undefined' instead of service name in Manage Config Groups popup",
            "id": "6491"
        },
        "6499": {
            "ground_truth": "0",
            "bug_report": "Unable to proceed from step 4 of installer if YARN-dependent services are selected but YARN isn't (HDP 2.0)\nSTROn step 4 of Install Wizard  select any of the services dependent on YARN (Piq  Oozie  Hive) but don't select YARN itself. Press 'Next' button.Expected resultPopup with the info about YARN dependence appears.Actual resultNothing happens. JS error occurs:Uncaught TypeError: Cannot read property 'get' of undefinedatapp/controllers/wizard/step4_controller.js:180",
            "id": "6499"
        },
        "6504": {
            "ground_truth": "0",
            "bug_report": "Incorrect errors count for YARN on step 7 of Install Wizard (HDP 2.0)\n'Customize Services' step indicates that 8 YARN properties need revision but no one is highlighted as incorrect.",
            "id": "6504"
        },
        "6521": {
            "ground_truth": "0",
            "bug_report": "Hostname case sensitivity\nHostname CaSe SeNsiTiVity causes hostnames don't match error.'in the install wizard  the hostnames MUST be lower case in the user input  otherwise the 'hostnames don't match' during the install and it fails. Ambari should tolower() both hosts before comparing.'Effected Install/Add Host Wizard.",
            "id": "6521"
        },
        "6525": {
            "ground_truth": "0",
            "bug_report": "Unable to install cluster after coming back to step 1 and selecting different stack version\nSTRProceed to step 7 of Install Wizard. Return to step 1 and choose a different stack version. Go forward.ResultUnable to proceed from step 3. JS error occurs:Uncaught Error: &lt;DS.StateManager:ember26192&gt; could not respond to event didChangeData in state rootState.loaded.updated.uncommitted.WorkaroundTo log out and log in again before perforfing installation with different stack version.",
            "id": "6525"
        },
        "6527": {
            "ground_truth": "0",
            "bug_report": "Ambari support on CentOS7 /RHEL7 - I\nBackground : Centos7 is very different from centos6.5 and currently Ambari &amp; HDP are not supported on Centos7Supporting Ambari on CentOS7/RHEL7 has multiple elements :1)mvn build of Ambari should be successful. document required steps in wiki.2)Ambari server &amp; agent should be modified to support centos7/redhat7. following changes are necessary :2.1)/usr/lib/python2.6 symbolic link .2.2)code changes to add redhat7 as a new OS family.3)HDP centos7 rpms + new repo to be made available to deploy different services on centos7/redhat7 .Currently centos7 is not supported by the Ambari/HDP build team.This JIRA captures 2.2)",
            "id": "6527"
        },
        "6539": {
            "ground_truth": "0",
            "bug_report": "Create main page with table of jobs\nCreate main page with table of jobs",
            "id": "6539"
        },
        "6544": {
            "ground_truth": "0",
            "bug_report": "Add ZKFC component to summary page (when NNHA enabled)\nAdd ZKFC component on summary page. Each Zkfc should under related Namenode  with a smaller size.",
            "id": "6544"
        },
        "6558": {
            "ground_truth": "0",
            "bug_report": "FE allows add host on host that is in UNKNOWN state (agent was stopped)\nadded a host to a cluster (manually installed and started an agent)http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org{ 'href' : 'http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org'  'Hosts' : { 'host_health_report' : ''  'host_name' : 'c6401.ambari.apache.org'  'host_state' : 'HEALTHY'  'host_status' : 'HEALTHY' }}After the agent is stopped for the host{ 'href' : 'http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org'  'Hosts' : { 'host_health_report' : ''  'host_name' : 'c6401.ambari.apache.org'  'host_state' : 'HEARTBEAT_LOST'  'host_status' : 'UNKNOWN' }}At this point I can perform AddHost and start the wizard. But AddHost will eventually fail - although now I see it just waiting.We need some form of error indication when host is not lost.",
            "id": "6558"
        },
        "6568": {
            "ground_truth": "0",
            "bug_report": "Service pluggability: New added services in the stack has Add Property link in log-4j and env categories\nService pluggability: New added services in the stack has Add Property link in log-4j and env categories",
            "id": "6568"
        },
        "6570": {
            "ground_truth": "0",
            "bug_report": "Fast user can skip 'Customize Services' step (add service wizard) while configs are not loaded\nSTR Install cluster Go to Add Service Wizard Proceed to step 'Customize services' While configs are loading click 'Next'.If user tried to install services with configs that should be provided manually using UI (like Nagios)  he will get JS-error on next step.",
            "id": "6570"
        },
        "6571": {
            "ground_truth": "0",
            "bug_report": "'all/none' links affect already installed services on 'Select Services' step\n'all/none' links affect already installed services on 'Select Services' step",
            "id": "6571"
        },
        "6575": {
            "ground_truth": "0",
            "bug_report": "Linked Ganglia charts from Host Details page don't show any graphs (except on NameNode host)\nThe UI links to Ganglia Web to display the graphs for a specific host as http://&lt;ganglia-host&gt;/ganglia/mobile_helper.php?show_host_metrics=1&amp;h=&lt;target-fqdn&gt;&amp;c=HDPNameNode&amp;r=hour&amp;cs=&amp;ce=Note that c=HDPNameNode is static.I believe this worked before as the NameNode gmond was repeated on all the hosts (we should not have and this has been fixed but now it's causing this issue).For this ticket  let's change the call the UI makes from 'HDPNameNode' to 'HDPSlaves' and verify that host-level Ganglia links work for all hosts on a multi-node cluster (including hosts with clients only).",
            "id": "6575"
        },
        "6585": {
            "ground_truth": "0",
            "bug_report": "Deleted hosts come back to life after ambari-server restart\nWhen attempting to delete a host through the UI  and then re-add it  the re-add operation fails because a record already exists in the clusterhostmapping table.This can be reproduced as follows (host names will change of course) 1. Create a cluster and add a host so that it is populated in the clusterhostmapping table.2. Make sure the agent is running.3. On the server  run ambari-server restart  and immediately run the following repeatedly in another terminal window before the restart finishes  curl --write-out %{http_code} --show-error -u admin:admin -H 'X-Requested-By:1' -i -X DELETE http://c6404.ambari.apache.org:8080/api/v1/clusters/dev/hosts/c6407.ambari.apache.orgHTTP/1.1 200 OKSet-Cookie: AMBARISESSIONID=z91px2l41uc6dwjv52zl2mcu;Path=/Expires: Thu  01 Jan 1970 00:00:00 GMTContent-Type: text/plainContent-Length: 0Server: Jetty(7.6.7.v20120910)4. Quickly verify that the host name is removed from the clusterhostmapping table.5. On the agent  run ambari-agent restart  and repeatedly requery the clusterhostmapping table  until the record is reinserted (should take no more than 30 seconds to appear).6. Run the curl command to attempt to re-add the host  and receive the error message curl --write-out %{http_code} --show-error -u admin:admin -H 'X-Requested-By:1' -i POST http://c6404.ambari.apache.org:8080/api/v1/clusters/dev/hosts/c6407.ambari.apache.orgHTTP/1.1 500 Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseException Internal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (2  'c6407.ambari.apache.org') was aborted. Call getNextException to see the cause. Error Code: 0 Call: INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (?  ?) bind =&gt; [2 parameters bound]Set-Cookie: AMBARISESSIONID=1je1wahcml82f11gjrserxgdyl;Path=/Content-Type: text/plain;charset=ISO-8859-1Content-Length: 530Server: Jetty(7.6.7.v20120910){ 'status': 500  'message': 'Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseException/nInternal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (2  /u0027c6407.ambari.apache.org/u0027) was aborted. Call getNextException to see the cause./nError Code: 0/nCall: INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (?  ?)/n/tbind /u003d/u003e [2 parameters bound]'At this point  here is the state of the tables.select * from clusterhostmapping where host_name = 'c6407.ambari.apache.org'; cluster_id | host_name------------+------------------------- 2 | c6407.ambari.apache.orgselect * from hoststate where host_name = 'c6407.ambari.apache.org'; agent_version | available_mem | current_state | health_status | host_name | time_in_state | maintenance_state---------------------+---------------+---------------+----------------------------------------------+-------------------------+---------------+------------------- {'version':'1.6.0'} | 250232 | INIT | {'healthStatus':'HEALTHY' 'healthReport':''} | c6407.ambari.apache.org | 1405718796141 | {'2':'ON'}I then deleted both records  restarted the server  and was then able to add the host successfully.This is a bug in the persistence layer.",
            "id": "6585"
        },
        "6587": {
            "ground_truth": "0",
            "bug_report": "Views: exception on server restart after creating view instance\n1) installed the files view (built from source)2) I created an instance of the viewPOSThttp://c6401.ambari.apache.org:8080/api/v1/views/FILES/versions/0.1.0/instances/MyFiles[ {'ViewInstanceInfo' : { 'properties' : { 'dataworker.defaultFs' : 'webhdfs://c6401.ambari.apache.org:50070'  'dataworker.username' : 'ambari-qa' } }} ]3) I restart ambari-server and get this exception  so ambari-server can't start up. If I delete the view jar and restart  then I can get ambari-server to start.00:41:59 433 INFO [main] Server:266 - jetty-7.6.7.v2012091000:41:59 914 WARN [main] WebAppContext:489 - Failed startup of context o.e.j.w.WebAppContext{/views/FILES/0.1.0/MyFiles file:/var/lib/ambari-server/resources/views/work/FILES%7B0.1.0%7D/} /var/lib/ambari-server/resources/views/work/FILES{0.1.0}java.util.zip.ZipException: invalid entry size (expected 12027 but got 11985 bytes) at java.util.zip.ZipInputStream.readEnd(ZipInputStream.java:403) at java.util.zip.ZipInputStream.read(ZipInputStream.java:195)",
            "id": "6587"
        },
        "6589": {
            "ground_truth": "0",
            "bug_report": "Management Console: UI Layout  Basic Routing and Create User Management Page (with mock data)\nManagement Console: UI Layout  Basic Routing and Create User Management Page (with mock data)",
            "id": "6589"
        },
        "6603": {
            "ground_truth": "0",
            "bug_report": "Install wizard should have ability to load default 'final' configs  and save them\nIn Ambari if a stack service has configs with final=true  then during install wizard these configs should have the final-checkbox checked. Also  if the user changes these checkboxes  the values should be stored in the properties_attributes of configs when Deploy is hit.",
            "id": "6603"
        },
        "6609": {
            "ground_truth": "0",
            "bug_report": "Ambari-DDL-Postgres-CREATE.sql fix for CLUSTER.OPERATE\nAmbari-DDL-Postgres-CREATE.sql fix for CLUSTER.OPERATE",
            "id": "6609"
        },
        "6622": {
            "ground_truth": "0",
            "bug_report": "BlueprintResourceProviderTest fails on exception message assertion\ntestDecidePopulationStrategy_unsupportedSchema(org.apache.ambari.server.controller.internal.BlueprintResourceProviderTest) Time elapsed: 0.02 sec &lt;&lt;&lt; FAILURE!java.lang.AssertionError:Expected: (exception with message a string containing 'Configuration definition schema is not supported' and an instance of java.lang.IllegalArgumentException) got: &lt;java.lang.IllegalArgumentException: 'Configuration format provided in Blueprint is not supported'&gt;",
            "id": "6622"
        },
        "6623": {
            "ground_truth": "0",
            "bug_report": "Fix unit tests that are broken after AMBARI-6533\nUnit tests are broken after commit f3aab68ec417d8e2c39c216962e1e1d47d56d401The root cause is the properties in InMemoryDefaultTestModule.java.",
            "id": "6623"
        },
        "6626": {
            "ground_truth": "0",
            "bug_report": "HDP-1.3 stack shows HUE in select services page\nHUE is defined in HDP-1.3.2 stack with configuration sites but there is no agent scripts defining configure/stop/start/status functions for the service.HUE is not supported to work with Ambari. In that case we should not expose HUE via stack definition API as supported service of HDP-1.x stack.",
            "id": "6626"
        },
        "6628": {
            "ground_truth": "0",
            "bug_report": "Cannot start ambari-server due to empty metainfo table\nTrying to deploy latest trunk build ambari-server-1.7.0-70 and hit the following when running ambari-server startambari-server.log01:30:53 074 INFO [main] AmbariServer:157 - ********* Meta Info initialized **********01:30:53 086 INFO [main] ClustersImpl:103 - Initializing the ClustersImpl01:30:53 679 INFO [main] AmbariManagementControllerImpl:230 - Initializing the AmbariManagementControllerImpl01:30:53 894 INFO [main] AmbariServer:487 - Checking DB store version01:30:53 894 WARN [main] AmbariServer:502 - Current database store version is not compatible with current server version  serverVersion=null  schemaVersion=null01:30:53 895 ERROR [main] AmbariServer:592 - Failed to run the Ambari Serverorg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=null  schemaVersion=null at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:503) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:164) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:589)The metainfo table is empty with no values indicating version of Ambari installed.",
            "id": "6628"
        },
        "6630": {
            "ground_truth": "0",
            "bug_report": "Add Service wizard: Retry installation functionality does not work\nAPI triggered on clicking Retry button results in failure with 400 status code",
            "id": "6630"
        },
        "6638": {
            "ground_truth": "0",
            "bug_report": "Config categories aren't displayed on 'Configure Services' step of Secrity Wizard\nSee the screenshot attached.JS error Uncaught TypeError: Cannot read property 'filterProperty' of undefined at ambari-web/app/views/common/configs/services_config.js:338",
            "id": "6638"
        },
        "6640": {
            "ground_truth": "1",
            "bug_report": "Memory leaks during tabs switching on 'Customize Services' page\nSteps:Go to 'Customize services page'.Switch to other tab many a time (50 switches = about 100MB).Result: firefox browser gets 1.2 GB in memory.",
            "id": "6640"
        },
        "6664": {
            "ground_truth": "0",
            "bug_report": "Oozie service can not be started on Ambari server with external Postgres\nSTR: Install single node cluster with Postgres 9 server as Ambari and Oozie DB (Stack - 2.0). Postgres server should be on dedicated host; Start all services.Actual result: Oozie server will not start.Error message for Oozie:2014-07-23 08:42:57 947 - Error while executing command 'start':Traceback (most recent call last): File '/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py'  line 111  in execute method(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_server.py'  line 43  in start oozie_service(action='start') File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_service.py'  line 43  in oozie_service Execute( db_connection_check_command  tries=5  try_sleep=10) File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 239  in action_run raise exFail: Execution of '/usr/jdk64/jdk1.7.0_45/bin/java -cp /usr/lib/ambari-agent/DBConnectionVerification.jar:/usr/lib/oozie/libserver/postgresql-9.0-801.jdbc4.jar org.apache.ambari.server.DBConnectionVerification jdbc:postgresql://&lt;host&gt;:5432/ooziedb oozieuser [PROTECTED] org.postgresql.Driver' returned 1. ERROR: Unable to connect to the DB. Please check DB connection properties.java.lang.ClassNotFoundException: org.postgresql.Driver",
            "id": "6664"
        },
        "6667": {
            "ground_truth": "0",
            "bug_report": "Unit test failures on jenkins for Ambari 1.7.0 related to alerts.\nTests run locally and pass 100% consistently.These test results are fishy; they randomly fail on different OS deployments. Even the simple logic ones fail. There are only 5 alert targets ever created  yet there are 9 returned sometimes. I'm wondering if this is because we tried to load some data in the @BeforeClass instead of @Before - maybe there's a weird Guice/JUnit race condition going on.I can't say I can fix this with confidence since I can't reproduce it. I'm going to move some things to @Before and hope it helps.java.lang.AssertionError: expected:&lt;5&gt; but was:&lt;9&gt; at org.junit.Assert.fail(Assert.java:93) at org.junit.Assert.failNotEquals(Assert.java:647) at org.junit.Assert.assertEquals(Assert.java:128) at org.junit.Assert.assertEquals(Assert.java:472) at org.junit.Assert.assertEquals(Assert.java:456) at org.apache.ambari.server.orm.dao.AlertDispatchDAOTest.testFindAllTargets(AlertDispatchDAOTest.java:117)",
            "id": "6667"
        },
        "6672": {
            "ground_truth": "0",
            "bug_report": "Oozie fails for stack 2.0 and 2.1\nOozie fails for stack 2.0 and 2.1",
            "id": "6672"
        },
        "6716": {
            "ground_truth": "0",
            "bug_report": "'Refresh configs' action doesn't work for Flume\nSTR: Deployed cluster with Flume  without Flume Agents. Added Flume agent on each host. Configured 7 agents  assigned to different hosts. Changed config of one Flume agent. Clicked 'Refresh configs' action.Result: Nothing happened.",
            "id": "6716"
        },
        "6721": {
            "ground_truth": "0",
            "bug_report": "Capacity Scheduler config cannot be saved\nSTR: Go to YARN &gt; Config Under 'Scheduler' section  modify 'Capacity Scheduler' configs. Hit Save. The page reloads. The modifications are reverted back.",
            "id": "6721"
        },
        "6729": {
            "ground_truth": "0",
            "bug_report": "RM HA wizard is experimental but shouldn't be\nInstalled 1.7.0 build and RM HA is not available unless I go enable #experimental.Should not be experimental. RM HA is available with HDP 2.1+ Stack (but not with HDP 2.0.* stack).",
            "id": "6729"
        },
        "6735": {
            "ground_truth": "0",
            "bug_report": "Once RM HA is config'd  none of the RM summary info or RM dashboard widgets show data\nConfigure RM HA. None of the summary info shows in Services &gt; YARN &gt; Summary All of the RM Dashboard widgets show N/A",
            "id": "6735"
        },
        "6741": {
            "ground_truth": "0",
            "bug_report": "On HDFS config page edit boxes with memory size values have incorrect behavior.\nGo to HDFS config page.Change Name Node java heap size.Save configs.'Successful' message appears  but on config page edit boxes have wrong values.After clicking 'OK' fields get right value again.Same issue reproduced on Nano but it looks like there it fails to save configs.",
            "id": "6741"
        },
        "6744": {
            "ground_truth": "0",
            "bug_report": "'Select Host' page on Resource Manager HA Enabling wizard does not save selected values after next/back steps\nSTR:1) Deploy cluster2) Go to Enable Resourse Manager Enable HA wizard3) Go to 'Select Host' page4) Select some different from default value in combobox 'Additional Resourse manager'5) Click 'Next' and go to 'Review' page6) Click 'Back' and again go to 'Select Host' page.Actual result: value in 'Additional Resourse Manager' is default againExpected result: value in 'Additional Resourse Manager' is the same with choosen previously.",
            "id": "6744"
        },
        "6749": {
            "ground_truth": "0",
            "bug_report": "Flume service should be in STARTED state when no agents configured\nWhen installing Flume service by default if we dont configure any agents  we end up with Flume being the only service shown in a red STOPPED state. For the case where there are no agents  this should be set to STARTED.",
            "id": "6749"
        },
        "6751": {
            "ground_truth": "0",
            "bug_report": "HostNames and Slaves broken in two lines on Assign Slaves step\nOn Assign Slaves step  elements in the table got broken in two lines.see attached.",
            "id": "6751"
        },
        "6754": {
            "ground_truth": "0",
            "bug_report": "Resource Manager HA: after enabling RM HA  UI does not display standby and active Resource Managers on Summary tab\nSTR:1) Deploy 3-node cluster2) Enable RM HA3) Go to YARN Service page  Summary tabResult: There are not Resource Managers on Summary tab4) Select config tab and after that return back to Summary tabResult: Summary tab is as expected",
            "id": "6754"
        },
        "6769": {
            "ground_truth": "0",
            "bug_report": "Add security configs on Add service wizard\nadd service wizard doesn't add secure cinfigs",
            "id": "6769"
        },
        "6782": {
            "ground_truth": "0",
            "bug_report": "Wizard: Adding Master component does not create and install the master host component\nSTR: On 3-node cluster  Go to Assign Masters page. Add another HBase Master. (Note: HBase Master and ZK server are only addable components.) Go forward to the review page. Review page will show correct information that 2 HBase master are selected for installation Eventually on deploying the cluster installation  HBase Master is only created and installed on the host that was a default selection on Assign Master page",
            "id": "6782"
        },
        "6787": {
            "ground_truth": "0",
            "bug_report": "Cleanup of Cluster > Admin tab\n1. Remove 'Users' and 'Access' category from 'Admin' tab on the main menu.2. Rename 'Clusters' --&gt; 'Repositories'3. Rename 'misc' --&gt; 'Service Accounts'",
            "id": "6787"
        },
        "6788": {
            "ground_truth": "0",
            "bug_report": "Ambari installation webhcat templeton.hive.properties set thrift host name to be localhost\nPROBLEM:default installation of Ambari sets webhcat-site.xmltempleton.hive.properties=hive.metastore.local=false  hive.metastore.uris=thrift://localhost:9933  hive.metastore.sasl.enabled=falsethe proper value should be the FQDN of the thrift host name  plus hive.metastore.execute.setugi=truefor example:hive.metastore.local=false  hive.metastore.uris=thrift://this.fqdn.com:9933  hive.metastore.sasl.enabled=false hive.metastore.execute.setugi=true",
            "id": "6788"
        },
        "6793": {
            "ground_truth": "0",
            "bug_report": "Need ResourceManager UI Quicklinks for active / standby\nSimilar to NN HA.",
            "id": "6793"
        }
    }
}