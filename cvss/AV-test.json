{
    "AV": {
        "iwctl_giwsens": {
            "ground_truth": "2",
            "function": "iwctl_giwsens",
            "description": "wireless handler get sensitivity",
            "id": "iwctl_giwsens"
        },
        "hns3_dcbnl_setdcbx": {
            "ground_truth": "1",
            "function": "hns3_dcbnl_setdcbx",
            "description": "driver net ethernet hisilicon hns3 hns3 dcbnl return if successful otherwise fail",
            "id": "hns3_dcbnl_setdcbx"
        },
        "tcp_retransmit_timer": {
            "ground_truth": "1",
            "function": "tcp_retransmit_timer",
            "description": "net ipv4 tcp timer the tcp retransmit timeout handler struct sock sk pointer to the current socket this function get called when the kernel timer for tcp packet of this socket expires it handle retransmission timer adjustment and other necesarry measure nothing void",
            "id": "tcp_retransmit_timer"
        },
        "device_pm_move_to_tail": {
            "ground_truth": "3",
            "function": "device_pm_move_to_tail",
            "description": "driver base core move set of device to the end of device list this is device reorder to tail wrapper taking the requisite lock it move the dev along with all of it child and all of it consumer to the end of the device kset and dpm list recursively struct device dev device to move",
            "id": "device_pm_move_to_tail"
        },
        "sock_alloc": {
            "ground_truth": "1",
            "function": "sock_alloc",
            "description": "allocate socket allocate new inode and socket object the two are bound together and initialised the socket is then returned if we are out of inodes null is returned this function us gfp kernel internally void no argument",
            "id": "sock_alloc"
        },
        "snd_device_new": {
            "ground_truth": "3",
            "function": "snd_device_new",
            "description": "sound core device create an alsa device component creates new device component for the given data pointer the device will be assigned to the card and managed together by the card the data pointer play role a the identifier too so the pointer address must be unique and unchanged struct snd card card the card instance enum snd device type type the device type sndrv dev xxx void device data the data pointer of this device const struct snd device ops ops the operator table zero if successful or negative error code on failure",
            "id": "snd_device_new"
        },
        "sony_check_add_dev_list": {
            "ground_truth": "2",
            "function": "sony_check_add_dev_list",
            "description": "driver hid hid sony if controller is plugged in via usb while already connected via bluetooth it will show up a two device global list of connected controller and their mac address is maintained to ensure that device is only connected once",
            "id": "sony_check_add_dev_list"
        },
        "iwctl_siwscan": {
            "ground_truth": "2",
            "function": "iwctl_siwscan",
            "description": "wireless handler set scan",
            "id": "iwctl_siwscan"
        },
        "cfg80211_send_layer2_update": {
            "ground_truth": "2",
            "function": "cfg80211_send_layer2_update",
            "description": "net wireless util send layer update frame wireless driver can use this function to update forwarding table in bridge device upon sta association regulatory enforcement infrastructure todo struct net device dev network device const u8 addr sta mac address",
            "id": "cfg80211_send_layer2_update"
        },
        "nfc_activate_target": {
            "ground_truth": "2",
            "function": "nfc_activate_target",
            "description": "net nfc core prepare the target for data exchange struct nfc dev dev the nfc device that found the target u32 target idx index of the target that must be activated u32 protocol nfc protocol that will be used for data exchange",
            "id": "nfc_activate_target"
        },
        "__sock_create": {
            "ground_truth": "1",
            "function": "__sock_create",
            "description": "creates socket struct net net net namespace int family protocol family af inet int type communication type sock stream int protocol protocol struct socket re new socket int kern boolean for kernel space socket creates new socket and assigns it to re passing through lsm return or an error on failure re is set to null kern must be set to true if the socket resides in kernel space this function internally us gfp kernel",
            "id": "__sock_create"
        },
        "ieee80211_napi_add": {
            "ground_truth": "2",
            "function": "ieee80211_napi_add",
            "description": "ieee80211 napi add initialize mac80211 napi context driver doe not use napi see also netif napi add",
            "id": "ieee80211_napi_add"
        },
        "first_packet_length": {
            "ground_truth": "1",
            "function": "first_packet_length",
            "description": "net ipv4 udp return length of first packet in receive queue struct sock sk socket drop all bad checksum frame until valid one is found return the length of found skb or if none is found",
            "id": "first_packet_length"
        },
        "am35xx_clk_find_idlest": {
            "ground_truth": "3",
            "function": "am35xx_clk_find_idlest",
            "description": "driver clk ti clk 3xxx return clock ack info for am35xx ip the interface clock on am35xx ip reflects the clock idle status in the enable register itsel at bit offset of from the enable bit value of indicates that clock is enabled struct clk hw omap clk struct clk being enabled struct clk omap reg idlest reg void iomem to store cm idlest reg address into u8 idlest bit pointer to u8 to store the cm idlest bit shift into u8 idlest val pointer to u8 to store the cm idlest indicator",
            "id": "am35xx_clk_find_idlest"
        },
        "update_mgnt_tx_rate23a": {
            "ground_truth": "2",
            "function": "update_mgnt_tx_rate23a",
            "description": "following are some tx function for wifi mlme",
            "id": "update_mgnt_tx_rate23a"
        },
        "nfc_alloc_send_skb": {
            "ground_truth": "2",
            "function": "nfc_alloc_send_skb",
            "description": "net nfc core allocate skb for data exchange response struct nfc dev dev undescribed struct sock sk undescribed unsigned int flag undescribed unsigned int size size to allocate unsigned int err undescribed",
            "id": "nfc_alloc_send_skb"
        },
        "udp_poll": {
            "ground_truth": "1",
            "function": "udp_poll",
            "description": "net ipv4 udp wait for udp event struct file file file struct struct socket sock socket poll table wait poll table this is same a datagram poll except for the special case of blocking socket if application is using blocking fd and packet with checksum error is in the queue then it could get return from select indicating data available but then block when reading it add special case code to work around these arguably broken application",
            "id": "udp_poll"
        },
        " complete_change_console": {
            "ground_truth": "3",
            "function": " complete_change_console",
            "description": "driver tty vt vt ioctl performs the back end of vt switch called under the console semaphore",
            "id": " complete_change_console"
        },
        "z8530_sync_dma_close": {
            "ground_truth": "2",
            "function": "z8530_sync_dma_close",
            "description": "driver net wan z85230 close down dma struct net device dev network device to detach struct z8530 channel z8530 channel to move into discard mode shut down dma mode synchronous interface halt the dma and free the buffer",
            "id": "z8530_sync_dma_close"
        },
        "regulatory_hint_found_beacon": {
            "ground_truth": "2",
            "function": "regulatory_hint_found_beacon",
            "description": "net wireless reg hint beacon wa found on channel this informs the wireless core that beacon from an ap wa found on the channel provided this allows the wireless core to make educated guess on regulatory to help with world roaming this is only used for world roaming when we do not know our current location this is only useful on channel and on the ghz band a channel are already enabled by the world regulatory domain and on non radar ghz channel driver do not need to call this cfg80211 will do it for after scan on newly found bs if you cannot make use of this feature you can set the wiphy disable beacon hint to true struct wiphy wiphy the wireless device where the beacon wa found on struct ieee80211 channel beacon chan the channel on which the beacon wa found on gfp gfp context flag",
            "id": "regulatory_hint_found_beacon"
        },
        "snd_lookup_minor_data": {
            "ground_truth": "3",
            "function": "snd_lookup_minor_data",
            "description": "sound core sound get user data of registered device check that minor device with the specified type is registered and return it user data pointer this function increment the reference counter of the card instance if an associated instance with the given minor number and type is found the caller must call snd card unref appropriately later unsigned int minor the minor number int type device type sndrv device type xxx the user data pointer if the specified device is found null otherwise",
            "id": "snd_lookup_minor_data"
        },
        "ixgbe_reset_phy_fw": {
            "ground_truth": "1",
            "function": "ixgbe_reset_phy_fw",
            "description": "driver net ethernet intel ixgbe ixgbe x550 reset firmware controlled phys struct ixgbe hw hw pointer to hardware structure",
            "id": "ixgbe_reset_phy_fw"
        },
        "usb_autopm_put_interface_no_suspend": {
            "ground_truth": "3",
            "function": "usb_autopm_put_interface_no_suspend",
            "description": "include linux usb decrement usb interface is pm usage counter this routine decrement intf is usage counter but doe not carry out an autosuspend this routine can run in atomic context struct usb interface intf the usb interface whose counter should be decremented",
            "id": "usb_autopm_put_interface_no_suspend"
        },
        "device_reprobe": {
            "ground_truth": "3",
            "function": "device_reprobe",
            "description": "driver base bus remove driver for device and probe for new driver this function detaches the attached driver if any for the given device and restarts the driver probing process it is intended to use if probing criterion changed during device lifetime and driver attachment should change accordingly struct device dev the device to reprobe",
            "id": "device_reprobe"
        },
        "udp_ioctl": {
            "ground_truth": "1",
            "function": "udp_ioctl",
            "description": "net ipv4 udp ioctl request applicable to the udp protocol",
            "id": "udp_ioctl"
        },
        "device_release": {
            "ground_truth": "3",
            "function": "device_release",
            "description": "driver base core free device structure this is called once the reference count for the object reach we forward the call to the device is release method which should handle actually freeing the structure struct kobject kobj device is kobject",
            "id": "device_release"
        },
        " snd_device_get_state": {
            "ground_truth": "3",
            "function": " snd_device_get_state",
            "description": "sound core device get the current state of the given device return the current state of the given device object for the valid device either sndrv dev build sndrv dev registered or sndrv dev disconnected is returned or for non existing device is returned a an error struct snd card card the card instance void device data the data pointer to release",
            "id": " snd_device_get_state"
        },
        "ieee80211_set_key_tx_seq": {
            "ground_truth": "2",
            "function": "ieee80211_set_key_tx_seq",
            "description": "ieee80211 set key tx seq set key tx sequence counter this function allows driver to set the current tx iv pns for the given key this is useful when resuming from wowlan sleep and the device may have transmitted frame using the ptk reply to arp request note that this function may only be called when no tx processing can be done concurrently",
            "id": "ieee80211_set_key_tx_seq"
        },
        "split_key_pad_len": {
            "ground_truth": "3",
            "function": "split_key_pad_len",
            "description": "driver crypto caam key gen compute mdha split key pad length for given algorithm u32 hash hashing algorithm selection one of op alg algsel md5 sha1 sha224 sha384 sha512 mdha split key pad length",
            "id": "split_key_pad_len"
        },
        "rtw_xmitframe_coalesce23a": {
            "ground_truth": "2",
            "function": "rtw_xmitframe_coalesce23a",
            "description": "this sub routine will perform all the following remove header create wlan header based on the info in pxmitframe append sta is iv ext iv append llc move frag chunk from pframe to pxmitframe mem apply sw encrypt if necessary",
            "id": "rtw_xmitframe_coalesce23a"
        },
        "iwctl_siwmode": {
            "ground_truth": "2",
            "function": "iwctl_siwmode",
            "description": "wireless handler set operation mode",
            "id": "iwctl_siwmode"
        },
        "skb_push": {
            "ground_truth": "1",
            "function": "skb_push",
            "description": "net core skbuff add data to the start of buffer struct sk buff skb buffer to use unsigned int len amount of data to add this function extends the used data area of the buffer at the buffer start if this would exceed the total buffer headroom the kernel will panic pointer to the first byte of the extra data is returned",
            "id": "skb_push"
        },
        "_rtl88ee_rate_mapping": {
            "ground_truth": "2",
            "function": "_rtl88ee_rate_mapping",
            "description": "mac80211 is rate idx is like this 4g band rx status band ieee80211 band 2ghz rate rx status flag rx flag ht desc92c rate1m desc92c rate54m idx is rate rx status flag rx flag ht desc92c ratemcs0 desc92c ratemcs15 idx is 5g band rx status band ieee80211 band 5ghz rate rx status flag rx flag ht desc92c rate6m desc92c rate54m idx is rate rx status flag rx flag ht desc92c ratemcs0 desc92c ratemcs15 idx is",
            "id": "_rtl88ee_rate_mapping"
        },
        "tcp_orphan_retries": {
            "ground_truth": "1",
            "function": "tcp_orphan_retries",
            "description": "net ipv4 tcp timer return maximal number of retries on an orphaned socket struct sock sk pointer to the current socket bool alive bool socket alive state",
            "id": "tcp_orphan_retries"
        },
        "ip_local_deliver": {
            "ground_truth": "1",
            "function": "ip_local_deliver",
            "description": "net ipv4 ip input deliver ip packet to the higher protocol layer",
            "id": "ip_local_deliver"
        },
        "udp_lib_get_port": {
            "ground_truth": "1",
            "function": "udp_lib_get_port",
            "description": "net ipv4 udp udp lite port lookup for ipv4 and ipv6 struct sock sk socket struct in question unsigned short snum port number to look up unsigned int hash2 nulladdr af dependent hash value in secondary hash chain with null address",
            "id": "udp_lib_get_port"
        },
        "iwctl_siwap": {
            "ground_truth": "2",
            "function": "iwctl_siwap",
            "description": "wireless handler set ap mac address",
            "id": "iwctl_siwap"
        },
        "inet_accept": {
            "ground_truth": "1",
            "function": "inet_accept",
            "description": "net ipv4 af inet accept pending connection the tcp layer now give bsd semantics",
            "id": "inet_accept"
        },
        "sock_create_lite": {
            "ground_truth": "1",
            "function": "sock_create_lite",
            "description": "creates socket int family protocol family af inet int type communication type sock stream int protocol protocol struct socket re new socket creates new socket and assigns it to re passing through lsm the new socket initialization is not complete see kernel accept return or an error on failure re is set to null this function internally us gfp kernel",
            "id": "sock_create_lite"
        },
        "sock_release": {
            "ground_truth": "1",
            "function": "sock_release",
            "description": "tool testing selftests bpf progs udp limit close socket struct socket sock socket to close the socket is released from the protocol stack if it ha release callback and the inode is then released if the socket is bound to an inode not file",
            "id": "sock_release"
        },
        "nfc_allocate_device": {
            "ground_truth": "2",
            "function": "nfc_allocate_device",
            "description": "net nfc core allocate new nfc device struct nfc ops ops device operation u32 supported protocol nfc protocol supported by the device int tx headroom undescribed int tx tailroom undescribed",
            "id": "nfc_allocate_device"
        },
        "get_vlan": {
            "ground_truth": "2",
            "function": "get_vlan",
            "description": "net wireless nl80211 get vlan interface making sure it is running and on the right wiphy",
            "id": "get_vlan"
        },
        "hub_port_disable": {
            "ground_truth": "3",
            "function": "hub_port_disable",
            "description": "driver usb core hub usb doe not have similar link state a usb that will avoid negotiating connection with plugged in cable but will signal the host when the cable is unplugged disable remote wake and set link state to u3 for usb device",
            "id": "hub_port_disable"
        },
        "nfc_fw_download_done": {
            "ground_truth": "2",
            "function": "nfc_fw_download_done",
            "description": "net nfc core inform that firmware download wa completed struct nfc dev dev the nfc device to which firmware wa downloaded const char firmware name the firmware filename u32 result the positive value of standard errno value",
            "id": "nfc_fw_download_done"
        },
        "clk_scu_set_rate": {
            "ground_truth": "3",
            "function": "clk_scu_set_rate",
            "description": "driver clk imx clk scu clk scu set rate set rate for scu clock set clock frequency for scu clock return the scu protocol status hw clock to change rate for rate target rate for the clock parent rate rate of the clock parent not used for scu clock",
            "id": "clk_scu_set_rate"
        },
        "bus_register": {
            "ground_truth": "3",
            "function": "bus_register",
            "description": "driver base bus register driver core subsystem once we have that we register the bus with the kobject infrastructure then register the child subsystem it ha the device and driver that belong to the subsystem struct bus type bus bus to register",
            "id": "bus_register"
        },
        "tcp_remove_empty_skb": {
            "ground_truth": "1",
            "function": "tcp_remove_empty_skb",
            "description": "net ipv4 tcp in some case both sendpage and sendmsg could have added an skb to the write queue but failed adding payload on it we need to remove it to consume le memory but more importantly be able to generate epollout for edge trigger epoll user",
            "id": "tcp_remove_empty_skb"
        },
        "bind_store": {
            "ground_truth": "3",
            "function": "bind_store",
            "description": "driver base bus manually attach device to driver note the driver must want to bind to the device it is not possible to override the driver is id table",
            "id": "bind_store"
        },
        "sock_fasync": {
            "ground_truth": "1",
            "function": "sock_fasync",
            "description": "update the socket async list fasync list locking strategy fasync list is modified only under process context socket lock under semaphore fasync list is used under read lock sk sk callback lock or under socket lock",
            "id": "sock_fasync"
        },
        "sockfd_lookup": {
            "ground_truth": "1",
            "function": "sockfd_lookup",
            "description": "go from file number to it socket slot int fd file handle int err pointer to an error code return the file handle passed in is locked and the socket it is bound to is returned if an error occurs the err pointer is overwritten with negative errno code and null is returned the function check for both invalid handle and passing handle which is not socket on success the socket object pointer is returned",
            "id": "sockfd_lookup"
        },
        "_rtl8821ae_rate_mapping": {
            "ground_truth": "2",
            "function": "_rtl8821ae_rate_mapping",
            "description": "mac80211 is rate idx is like this 4g band rx status band ieee80211 band 2ghz rate rx status flag rx flag ht desc rate1m desc rate54m idx is rate rx status flag rx flag ht desc ratemcs0 desc ratemcs15 idx is 5g band rx status band ieee80211 band 5ghz rate rx status flag rx flag ht desc rate6m desc rate54m idx is rate rx status flag rx flag ht desc ratemcs0 desc ratemcs15 idx is",
            "id": "_rtl8821ae_rate_mapping"
        },
        "sk_set_memalloc": {
            "ground_truth": "1",
            "function": "sk_set_memalloc",
            "description": "net core sock set sock memalloc set sock memalloc on socket for access to emergency reserve it is the responsibility of the admin to adjust min free kbytes to meet the requirement struct sock sk socket to set it on",
            "id": "sk_set_memalloc"
        },
        "ioat_check_space_lock": {
            "ground_truth": "3",
            "function": "ioat_check_space_lock",
            "description": "driver dma ioat dma verify space and grab ring producer lock struct ioatdma chan ioat chan ioat channel ring to operate on int num descs allocation length",
            "id": "ioat_check_space_lock"
        },
        "whc_stop": {
            "ground_truth": "2",
            "function": "whc_stop",
            "description": "stop the wireless host controller stop device notification wait for pending transfer to stop put hc into stop state",
            "id": "whc_stop"
        },
        "driver_attach": {
            "ground_truth": "3",
            "function": "driver_attach",
            "description": "driver base dd try to bind driver to device walk the list of device that the bus ha on it and try to match the driver with each one if driver probe device return and the dev driver is set we have found compatible pair struct device driver drv driver",
            "id": "driver_attach"
        },
        "usb_set_isoch_delay": {
            "ground_truth": "3",
            "function": "usb_set_isoch_delay",
            "description": "driver usb core message usb set isoch delay informs the device of the packet transmit delay context in interrupt since this is an optional request we do not bother if it fails dev the device whose delay is to be informed",
            "id": "usb_set_isoch_delay"
        },
        "cfg80211_notify_new_peer_candidate": {
            "ground_truth": "2",
            "function": "cfg80211_notify_new_peer_candidate",
            "description": "net wireless nl80211 notify cfg80211 of new mesh peer candidate this function notifies cfg80211 that the mesh peer candidate ha been detected most likely via beacon or le likely via probe response cfg80211 then sends notification to userspace rfkill integration rfkill integration in cfg80211 is almost invisible to driver a cfg80211 automatically register an rfkill instance for each wireless device it know about soft kill is also translated into disconnecting and turning all interface off driver are expected to turn off the device when all interface are down however device may have hard rfkill line in which case they also need to interact with the rfkill subsystem via cfg80211 they can do this with few helper function documented here struct net device dev network device const u8 macaddr the mac address of the new candidate const u8 ie information element advertised by the peer candidate u8 ie len length of the information element buffer int sig dbm undescribed gfp gfp allocation flag",
            "id": "cfg80211_notify_new_peer_candidate"
        },
        "driver_deferred_probe_check_state": {
            "ground_truth": "3",
            "function": "driver_deferred_probe_check_state",
            "description": "driver base dd check deferred probe state driver or subsystem can opt in to calling this function instead of directly returning eprobe defer struct device dev device to check enodev if initcalls have completed and module are disabled etimedout if the deferred probe timeout wa set and ha expired and module are enabled eprobe defer in other case",
            "id": "driver_deferred_probe_check_state"
        },
        "__ieee80211_recalc_txpower": {
            "ground_truth": "2",
            "function": "__ieee80211_recalc_txpower",
            "description": "net mac80211 iface doc interface list locking the interface list in each struct ieee80211 local is protected three fold modification may only be done under the rtnl modification and reader are protected against each other by the iflist mtx modification are done in an rcu manner so atomic reader can traverse the list in rcu safe block a consequence read traversal of the list can be protected by either the rtnl the iflist mtx or rcu",
            "id": "__ieee80211_recalc_txpower"
        },
        "ieee80211_txq_may_transmit": {
            "ground_truth": "2",
            "function": "ieee80211_txq_may_transmit",
            "description": "net mac80211 tx check whether txq is allowed to transmit this function is used to check whether given txq is allowed to transmit by the airtime scheduler and can be used by driver to access the airtime fairness accounting without going using the scheduling order enfored by next txq return true if the airtime scheduler think the txq should be allowed to transmit and false if it should be throttled this function can also have the side effect of rotating the txq in the scheduler rotation which will eventually bring the deficit to positive and allow the station to transmit again the api ieee80211 txq may transmit also ensures that txq list will be aligned against driver is own round robin scheduler list it rotates the txq list till it make the requested node becomes the first entry in txq list thus both the txq list and driver is list are in sync if this function return true the driver is expected to schedule packet for transmission and then return the txq through ieee80211 return txq struct ieee80211 hw hw pointer a obtained from ieee80211 alloc hw struct ieee80211 txq txq pointer obtained from station or virtual interface",
            "id": "ieee80211_txq_may_transmit"
        },
        "pm_runtime_cancel_pending": {
            "ground_truth": "3",
            "function": "pm_runtime_cancel_pending",
            "description": "driver base power runtime deactivate suspend timer and cancel request struct device dev device to handle",
            "id": "pm_runtime_cancel_pending"
        },
        "ping_check_bind_addr": {
            "ground_truth": "1",
            "function": "ping_check_bind_addr",
            "description": "net ipv4 ping check the bind address and possibly modifies sk sk bound dev if",
            "id": "ping_check_bind_addr"
        },
        "tcp_register_ulp": {
            "ground_truth": "1",
            "function": "tcp_register_ulp",
            "description": "net ipv4 tcp ulp attach new upper layer protocol to the list of available protocol",
            "id": "tcp_register_ulp"
        },
        "bus_probe_device": {
            "ground_truth": "3",
            "function": "bus_probe_device",
            "description": "driver base bus probe driver for new device automatically probe for driver if the bus allows it struct device dev device to probe",
            "id": "bus_probe_device"
        },
        "pci_bus_alloc_resource": {
            "ground_truth": "3",
            "function": "pci_bus_alloc_resource",
            "description": "driver pci bus allocate resource from parent bus given the pci bus device resides on the size minimum address alignment and type try to find an acceptable resource allocation for specific device resource struct pci bus bus pci bus struct resource re resource to allocate resource size size size of resource to allocate resource size align alignment of resource to allocate resource size min minimum proc iomem address to allocate unsigned long type mask ioresource type flag resource size alignf void const struct resource resource size resource size resource alignment function void alignf data data argument for resource alignment function",
            "id": "pci_bus_alloc_resource"
        },
        "usb_autopm_put_interface_async": {
            "ground_truth": "3",
            "function": "usb_autopm_put_interface_async",
            "description": "include linux usb decrement usb interface is pm usage counter this routine doe much the same thing a usb autopm put interface it decrement intf is usage counter and schedule delayed autosuspend request if the counter is the difference is that it doe not perform any synchronization caller should hold private lock and handle all synchronization issue themselves typically driver would call this routine during an urb is completion handler if no more urbs were pending this routine can run in atomic context struct usb interface intf the usb interface whose counter should be decremented",
            "id": "usb_autopm_put_interface_async"
        },
        "tcp_init_sock": {
            "ground_truth": "1",
            "function": "tcp_init_sock",
            "description": "net ipv4 tcp address family independent initialization for tcp sock note lot of thing set to zero explicitly by call to sk alloc so need not be done here",
            "id": "tcp_init_sock"
        },
        "ice_set_clear_eir_bw": {
            "ground_truth": "1",
            "function": "ice_set_clear_eir_bw",
            "description": "driver net ethernet intel ice ice sched set or clear eir bw save or clear eir bandwidth bw in the passed param bw info struct ice bw type info bw info bandwidth type information structure u32 bw bandwidth in kbps kilo bit per sec",
            "id": "ice_set_clear_eir_bw"
        },
        "nci_recv_frame": {
            "ground_truth": "2",
            "function": "nci_recv_frame",
            "description": "net nfc nci core receive frame from nci driver struct nci dev ndev the nci device struct sk buff skb the sk buff to receive",
            "id": "nci_recv_frame"
        },
        "__sys_bind": {
            "ground_truth": "1",
            "function": "__sys_bind",
            "description": "bind name to socket nothing much to do here since it is the protocol is responsibility to handle the local address we move the socket address to kernel space before we call the protocol layer having also checked the address is ok",
            "id": "__sys_bind"
        },
        "ip6_protocol_deliver_rcu": {
            "ground_truth": "1",
            "function": "ip6_protocol_deliver_rcu",
            "description": "net ipv6 ip6 input deliver the packet to the host",
            "id": "ip6_protocol_deliver_rcu"
        },
        "mesh_set_ht_prot_mode": {
            "ground_truth": "2",
            "function": "mesh_set_ht_prot_mode",
            "description": "net mac80211 mesh plink set correct ht protection mode section of ieee describes the protection rule for ht mesh sta in mb three ht protection mode are supported for now non ht mixed mode 20mhz protection and no protection mode non ht mixed mode is selected if any non ht peer are present in our mb 20mhz protection mode is selected if all peer in our 40mhz mb support ht and atleast one ht20 peer is present otherwise no protection mode is selected struct ieee80211 sub if data sdata the mesh interface to handle",
            "id": "mesh_set_ht_prot_mode"
        },
        "packet_recvmsg": {
            "ground_truth": "1",
            "function": "packet_recvmsg",
            "description": "net packet af packet pull packet from our receive queue and hand it to the user if necessary we block",
            "id": "packet_recvmsg"
        },
        "rpm_resume": {
            "ground_truth": "3",
            "function": "rpm_resume",
            "description": "driver base power runtime carry out runtime resume of given device check if the device is runtime pm status allows it to be resumed cancel any scheduled or pending request if another resume ha been started earlier either return immediately or wait for it to finish depending on the rpm nowait and rpm async flag similarly if there is suspend running in parallel with this function either tell the other process to resume after suspending deferred resume or wait for it to finish if the rpm async flag is set then queue resume request otherwise run the runtime resume callback directly queue an idle notification for the device if the resume succeeded this function must be called under dev power lock with interrupt disabled struct device dev device to resume int rpmflags flag bit",
            "id": "rpm_resume"
        },
        "driver_deferred_probe_trigger": {
            "ground_truth": "3",
            "function": "driver_deferred_probe_trigger",
            "description": "driver base dd kick off re probing deferred device this function move all device from the pending list to the active list and schedule the deferred probe workqueue to process them it should be called anytime driver is successfully bound to device note there is race condition in multi threaded probe in the case where more than one device is probing at the same time it is possible for one probe to complete successfully while another is about to defer if the second depends on the first then it will get put on the pending list after the trigger event ha already occurred and will be stuck there the atomic wouldeferred trigger count is used to determine if successful trigger ha occurred in the midst of probing driver if the trigger count change in the midst of probe then deferred processing should be triggered again void no argument",
            "id": "driver_deferred_probe_trigger"
        },
        "ip_list_rcv": {
            "ground_truth": "1",
            "function": "ip_list_rcv",
            "description": "net ipv4 ip input receive list of ip packet",
            "id": "ip_list_rcv"
        },
        "udp_recvmsg": {
            "ground_truth": "1",
            "function": "udp_recvmsg",
            "description": "net ipv4 udp this should be easy if there is something there we return it otherwise we block",
            "id": "udp_recvmsg"
        },
        "__register_prot_hook": {
            "ground_truth": "1",
            "function": "__register_prot_hook",
            "description": "net packet af packet register prot hook must be invoked through register prot hook or from context in which asynchronous access to the packet socket is not possible packet create",
            "id": "__register_prot_hook"
        },
        "tcp_splice_read": {
            "ground_truth": "1",
            "function": "tcp_splice_read",
            "description": "net ipv4 tcp splice data from tcp socket to pipe will read page from given socket and fill them into pipe struct socket sock socket to splice from loff ppos position not valid struct pipe inode info pipe pipe to splice to size len number of byte to splice unsigned int flag splice modifier flag",
            "id": "tcp_splice_read"
        },
        "sock_create": {
            "ground_truth": "1",
            "function": "sock_create",
            "description": "creates socket int family protocol family af inet int type communication type sock stream int protocol protocol struct socket re new socket wrapper around sock create return or an error this function internally us gfp kernel",
            "id": "sock_create"
        },
        "snd_usb_create_streams": {
            "ground_truth": "3",
            "function": "snd_usb_create_streams",
            "description": "sound usb card parse audio control descriptor and create pcm midi stream",
            "id": "snd_usb_create_streams"
        },
        " tcp_in_quickack_mode": {
            "ground_truth": "1",
            "function": " tcp_in_quickack_mode",
            "description": "net ipv4 tcp input send acks quickly if quick count is not exhausted and the session is not interactive",
            "id": " tcp_in_quickack_mode"
        },
        "usb_autopm_get_interface_no_resume": {
            "ground_truth": "3",
            "function": "usb_autopm_get_interface_no_resume",
            "description": "include linux usb increment usb interface is pm usage counter this routine increment intf is usage counter but doe not carry out an autoresume this routine can run in atomic context struct usb interface intf the usb interface whose counter should be incremented",
            "id": "usb_autopm_get_interface_no_resume"
        },
        "__beiscsi_mcc_compl_status": {
            "ground_truth": "3",
            "function": "__beiscsi_mcc_compl_status",
            "description": "driver scsi be2iscsi be cmds beiscsi mcc compl status return the status of mcc completion phba driver private structure tag tag for the mbx command wrb the wrb used for the mbx command mbx cmd mem ptr to memory allocated for mbx cmd return success failure non zero",
            "id": "__beiscsi_mcc_compl_status"
        },
        "iwctl_giwessid": {
            "ground_truth": "2",
            "function": "iwctl_giwessid",
            "description": "wireless handler get essid",
            "id": "iwctl_giwessid"
        },
        "remove_id_store": {
            "ground_truth": "3",
            "function": "remove_id_store",
            "description": "driver pci pci driver remove pci device id from this driver remove dynamic pci device id to this driver struct device driver driver target device driver const char buf buffer for scanning device id data size count input size",
            "id": "remove_id_store"
        },
        "pmic_arb_write_data": {
            "ground_truth": "3",
            "function": "pmic_arb_write_data",
            "description": "driver spmi spmi pmic arb arb is register struct spmi pmic arb pmic arb undescribed const u8 buf buffer to write length must be bc u32 reg register is address u8 bc byte count range",
            "id": "pmic_arb_write_data"
        },
        "tcp_try_coalesce": {
            "ground_truth": "1",
            "function": "tcp_try_coalesce",
            "description": "net ipv4 tcp input try to merge skb to prior one before queueing skb from after to try to merge them to reduce overall memory use and queue length if cost is small packet in ofo or receive queue can stay long time better try to coalesce them right now to avoid future collapse return true if caller should free from instead of queueing it struct sock sk socket struct sk buff to prior buffer struct sk buff from buffer to add in queue bool fragstolen pointer to boolean",
            "id": "tcp_try_coalesce"
        },
        "device_add": {
            "ground_truth": "3",
            "function": "device_add",
            "description": "driver base core add device to device hierarchy this is part of device register though may be called separately iff device initialize ha been called separately this add dev to the kobject hierarchy via kobject add add it to the global and sibling list for the device then add it to the other relevant subsystem of the driver model do not call this routine or device register more than once for any device structure the driver model core is not designed to work with device that get unregistered and then spring back to life among other thing it is very hard to guarantee that all reference to the previous incarnation of dev have been dropped allocate and register fresh new struct device instead rule of thumb is if device add succeeds you should call device del when you want to get rid of it if device add ha not succeeded use only put device to drop the reference count note never directly free dev after calling this function even if it returned an error always use put device to give up your reference instead struct device dev device",
            "id": "device_add"
        },
        "skb_copy_ubufs": {
            "ground_truth": "1",
            "function": "skb_copy_ubufs",
            "description": "net core skbuff copy userspace skb frags buffer to kernel struct sk buff skb the skb to modify gfp gfp mask allocation priority this must be called on skbtx dev zerocopy skb it will copy all frags into kernel and drop the reference to userspace page if this function is called from an interrupt gfp mask must be gfp atomic return on success or negative error code on failure to allocate kernel memory to copy to",
            "id": "skb_copy_ubufs"
        },
        "inet_csk_clone_lock": {
            "ground_truth": "1",
            "function": "inet_csk_clone_lock",
            "description": "net ipv4 inet connection sock clone an inet socket and lock it clone const struct sock sk the socket to clone const struct request sock req request sock const gfp priority for allocation gfp kernel gfp atomic etc caller must unlock socket even in error path bh unlock sock newsk",
            "id": "inet_csk_clone_lock"
        },
        "smc_llc_cli_add_link_reject": {
            "ground_truth": "1",
            "function": "smc_llc_cli_add_link_reject",
            "description": "net smc smc llc prepare and send an add link reject response",
            "id": "smc_llc_cli_add_link_reject"
        },
        "netdev_stats_to_stats64": {
            "ground_truth": "1",
            "function": "netdev_stats_to_stats64",
            "description": "convert net device stats to rtnl link stats64 rtnl link stats64 ha all the same field in the same order a net device stats with only the type differing but rtnl link stats64 may have additional field at the end for newer counter",
            "id": "netdev_stats_to_stats64"
        },
        "prism2mgmt_pstr2bytearea": {
            "ground_truth": "2",
            "function": "prism2mgmt_pstr2bytearea",
            "description": "conversion function going between wlan message data type and prism2 data type",
            "id": "prism2mgmt_pstr2bytearea"
        },
        "dev_add_pack": {
            "ground_truth": "1",
            "function": "dev_add_pack",
            "description": "add packet handler struct packet type pt packet type declaration add protocol handler to the networking stack the passed type packet type is linked into kernel list and may not be freed until it ha been removed from the kernel list this call doe not sleep therefore it can not guarantee all cpu is that are in middle of receiving packet will see the new packet type until the next received packet",
            "id": "dev_add_pack"
        },
        "bpf_check_classic": {
            "ground_truth": "1",
            "function": "bpf_check_classic",
            "description": "net core filter verify socket filter code check the user is filter code if we let some ugly filter code slip through kaboom the filter must contain no reference or jump that are out of range no illegal instruction and must end with ret instruction all jump are forward a they are not signed return if the rule set is legal or einval if not const struct sock filter filter filter to verify unsigned int flen length of filter",
            "id": "bpf_check_classic"
        },
        "rebind_marked_interfaces": {
            "ground_truth": "3",
            "function": "rebind_marked_interfaces",
            "description": "driver usb core driver rebind driver to udev is marked interface these interface have the need binding flag set the caller must hold udev is device lock",
            "id": "rebind_marked_interfaces"
        },
        "usb_clear_halt": {
            "ground_truth": "3",
            "function": "usb_clear_halt",
            "description": "driver usb core message tell device to clear endpoint halt stall condition this is used to clear halt condition for bulk and interrupt endpoint a reported by urb completion status endpoint that are halted are sometimes referred to a being stalled such endpoint are unable to transmit or receive data until the halt status is cleared any urbs queued for such an endpoint should normally be unlinked by the driver before clearing the halt condition a described in section and of the usb spec note that control and isochronous endpoint do not halt although control endpoint report protocol stall for unsupported request using the same status code used to report true stall this call is synchronous and may not be used in an interrupt context struct usb device dev device whose endpoint is halted int pipe endpoint pipe being cleared in interrupt zero on success or else the status code returned by the underlying usb control msg call",
            "id": "usb_clear_halt"
        },
        "usb_enable_remote_wakeup": {
            "ground_truth": "1",
            "function": "usb_enable_remote_wakeup",
            "description": "driver usb core hub usb enable remote wakeup enable remote wakeup for device for usb device set the device is remote wakeup feature for usb device assume there is only one function on the device and enable remote wake for the first interface fixme if the interface association descriptor show there is more than one function udev target device",
            "id": "usb_enable_remote_wakeup"
        },
        "tcp_inet6_sk": {
            "ground_truth": "1",
            "function": "tcp_inet6_sk",
            "description": "net ipv6 tcp ipv6 helper returning the inet6 address from given tcp socket it can be used in tcp stack instead of inet6 sk sk this avoids dereference and allow compiler optimization it is specialized version of inet6 sk generic",
            "id": "tcp_inet6_sk"
        },
        "lcs_send_stoplan": {
            "ground_truth": "1",
            "function": "lcs_send_stoplan",
            "description": "driver s390 net lcs struct lcs card card undescribed u8 initiator undescribed",
            "id": "lcs_send_stoplan"
        },
        "emac_get_drvinfo": {
            "ground_truth": "1",
            "function": "emac_get_drvinfo",
            "description": "driver net ethernet ti davinci emac get emac driver information return emac driver information name and version struct net device ndev the davinci emac network adapter struct ethtool drvinfo info ethtool info structure containing name and version",
            "id": "emac_get_drvinfo"
        },
        "free_some_resources": {
            "ground_truth": "1",
            "function": "free_some_resources",
            "description": "driver net ethernet chelsio cxgb4 cxgb4 main free the following resource memory used for table msi msi net device resource fw is holding for u",
            "id": "free_some_resources"
        },
        "snd_device_free_all": {
            "ground_truth": "3",
            "function": "snd_device_free_all",
            "description": "sound core device release all the device on the card called from init",
            "id": "snd_device_free_all"
        },
        "ipc_tx_callback": {
            "ground_truth": "3",
            "function": "ipc_tx_callback",
            "description": "driver hid intel ish hid ishtp client ipc tx callback function send message over ipc either first time or on callback on previous message completion void prm pointer to client device instance",
            "id": "ipc_tx_callback"
        },
        "vt_ioctl": {
            "ground_truth": "3",
            "function": "vt_ioctl",
            "description": "driver tty vt vt ioctl we handle the console specific ioctl is here we allow the capability to modify any console not just the fg console",
            "id": "vt_ioctl"
        },
        "driver_probe_device": {
            "ground_truth": "3",
            "function": "driver_probe_device",
            "description": "driver base dd attempt to bind device driver together this function return enodev if the device is not registered if the device is bound successfully and otherwise this function must be called with dev lock held when called for usb interface dev parent lock must be held a well if the device ha parent runtime resume the parent before driver probing struct device driver drv driver to bind device to struct device dev device to try to bind to the driver",
            "id": "driver_probe_device"
        },
        "tun_get_user": {
            "ground_truth": "1",
            "function": "tun_get_user",
            "description": "driver net tun get packet from user space buffer",
            "id": "tun_get_user"
        },
        "snd_device_initialize": {
            "ground_truth": "3",
            "function": "snd_device_initialize",
            "description": "sound core init initialize struct device for sound device struct device dev device to initialize struct snd card card card to assign optional",
            "id": "snd_device_initialize"
        },
        "usb_put_dev": {
            "ground_truth": "3",
            "function": "usb_put_dev",
            "description": "driver usb core usb release use of the usb device structure must be called when user of device is finished with it when the last user of the device call this function the memory of the device is freed struct usb device dev device that is been disconnected",
            "id": "usb_put_dev"
        },
        "bfa_lps_sm_logout": {
            "ground_truth": "3",
            "function": "bfa_lps_sm_logout",
            "description": "driver scsi bfa bfa svc logout in progress awaiting firmware response",
            "id": "bfa_lps_sm_logout"
        },
        "subsys_find_device_by_id": {
            "ground_truth": "3",
            "function": "subsys_find_device_by_id",
            "description": "driver base bus find device with specific enumeration number check the hint is next object and if it is match return it directly otherwise fall back to full list search either way reference for the returned object is taken struct bus type subsys subsystem unsigned int id index id in struct device struct device hint device to check first",
            "id": "subsys_find_device_by_id"
        },
        "usb_cache_string": {
            "ground_truth": "3",
            "function": "usb_cache_string",
            "description": "driver usb core message read string descriptor and cache it for later use struct usb device udev the device whose string descriptor is being read int index the descriptor index pointer to kmalloc ed buffer containing the descriptor string or null if the index is or the string could not be read",
            "id": "usb_cache_string"
        },
        "gether_set_qmult": {
            "ground_truth": "3",
            "function": "gether_set_qmult",
            "description": "driver usb gadget function ether initialize an ethernet over usb link with multiplier this set the queue length multiplier of this ethernet over usb link for higher speed use longer queue struct net device net device representing this link unsigned qmult queue multiplier",
            "id": "gether_set_qmult"
        },
        "device_create_file": {
            "ground_truth": "3",
            "function": "device_create_file",
            "description": "driver base core create sysfs attribute file for device struct device dev device const struct device attribute attr device attribute descriptor",
            "id": "device_create_file"
        },
        "tcp_get_available_ulp": {
            "ground_truth": "1",
            "function": "tcp_get_available_ulp",
            "description": "net ipv4 tcp ulp build string with list of available upper layer protocl value",
            "id": "tcp_get_available_ulp"
        },
        "build_skb_around": {
            "ground_truth": "1",
            "function": "build_skb_around",
            "description": "net core skbuff build network buffer around provided skb struct sk buff skb sk buff provide by caller must be memset cleared void data data buffer provided by caller unsigned int frag size size of data or if head wa kmalloced",
            "id": "build_skb_around"
        },
        "device_is_bound": {
            "ground_truth": "3",
            "function": "device_is_bound",
            "description": "driver base dd check if device is bound to driver return true if passed device ha already finished probing successfully against driver this function must be called with the device lock held struct device dev device to check",
            "id": "device_is_bound"
        },
        "vt_disallocate_all": {
            "ground_truth": "3",
            "function": "vt_disallocate_all",
            "description": "driver tty vt vt ioctl deallocate all unused console but leave",
            "id": "vt_disallocate_all"
        },
        "api_chain_free": {
            "ground_truth": "1",
            "function": "api_chain_free",
            "description": "driver net ethernet huawei hinic hinic hw api cmd free api cmd specific chain struct hinic api cmd chain chain the api cmd specific chain to free",
            "id": "api_chain_free"
        },
        "deferred_probe_work_func": {
            "ground_truth": "3",
            "function": "deferred_probe_work_func",
            "description": "driver base dd deferred probe work func retry probing device in the active list",
            "id": "deferred_probe_work_func"
        },
        "netdev_set_eeprom": {
            "ground_truth": "1",
            "function": "netdev_set_eeprom",
            "description": "driver net ethernet micrel ksz884x write eeprom data this function modifies the eeprom data one byte at time return if successful otherwise an error code struct net device dev network device struct ethtool eeprom eeprom ethtool eeprom data structure u8 data data buffer",
            "id": "netdev_set_eeprom"
        },
        "usb_deauthorize_interface": {
            "ground_truth": "3",
            "function": "usb_deauthorize_interface",
            "description": "driver usb core message usb deauthorize interface deauthorize an usb interface intf usb interface structure",
            "id": "usb_deauthorize_interface"
        },
        "tcp_delack_timer": {
            "ground_truth": "1",
            "function": "tcp_delack_timer",
            "description": "net ipv4 tcp timer the tcp delayed ack timeout handler struct timer list pointer to the timer get casted to struct sock this function get indirectly called when the kernel timer for tcp packet of this socket expires call tcp delack timer handler to do the actual work nothing void",
            "id": "tcp_delack_timer"
        },
        "usb_enable_lpm": {
            "ground_truth": "3",
            "function": "usb_enable_lpm",
            "description": "driver usb core hub attempt to enable device initiated and hub initiated u1 and u2 entry the xhci host policy may prevent u1 or u2 from being enabled other caller may have disabled link pm so u1 and u2 entry will be disabled until the lpm disable count drop to zero caller must own the bandwidth mutex",
            "id": "usb_enable_lpm"
        },
        "r8712_os_recvbuf_resource_free": {
            "ground_truth": "3",
            "function": "r8712_os_recvbuf_resource_free",
            "description": "driver staging rtl8712 recv linux free o related resource in struct recv buf",
            "id": "r8712_os_recvbuf_resource_free"
        },
        "tun_validate": {
            "ground_truth": "1",
            "function": "tun_validate",
            "description": "driver net tun trivial set of netlink ops to allow deleting tun or tap device with netlink",
            "id": "tun_validate"
        },
        "__sys_recvfrom": {
            "ground_truth": "1",
            "function": "__sys_recvfrom",
            "description": "helper which do the actual work for syscalls",
            "id": "__sys_recvfrom"
        },
        "iwctl_siwencode": {
            "ground_truth": "2",
            "function": "iwctl_siwencode",
            "description": "wireless handler set encode mode",
            "id": "iwctl_siwencode"
        },
        "device_register": {
            "ground_truth": "3",
            "function": "device_register",
            "description": "driver base core register device with the system this happens in two clean step initialize the device and add it to the system the two step can be called separately but this is the easiest and most common you should only call the two helper separately if have clearly defined need to use and refcount the device before it is added to the hierarchy for more information see the kerneldoc for device initialize and device add note never directly free dev after calling this function even if it returned an error always use put device to give up the reference initialized in this function instead struct device dev pointer to the device structure",
            "id": "device_register"
        },
        "lpfcdiag_loop_get_xri": {
            "ground_truth": "1",
            "function": "lpfcdiag_loop_get_xri",
            "description": "driver scsi lpfc lpfc bsg obtains the transmit and receive id this function obtains the transmit and receive id required to send an unsolicited ct command with payload special lpfc fstype and cmdrsp flag are used to the unsolicted response handler is able to process the ct command sent on the same port struct lpfc hba phba pointer to hba context object uint16 rpi remote port login id uint16 txxri pointer to transmit exchange id uint16 rxxri pointer to response exchabge id",
            "id": "lpfcdiag_loop_get_xri"
        },
        "inet_twsk_bind_unhash": {
            "ground_truth": "1",
            "function": "inet_twsk_bind_unhash",
            "description": "net ipv4 inet timewait sock unhash timewait socket from bind hash struct inet timewait sock tw timewait socket struct inet hashinfo hashinfo hashinfo pointer unhash timewait socket from bind hash if hashed bind hash lock must be held by caller return if caller should call inet twsk put after lock release",
            "id": "inet_twsk_bind_unhash"
        },
        "tcp_recvmsg": {
            "ground_truth": "1",
            "function": "tcp_recvmsg",
            "description": "net ipv4 tcp this routine copy from sock struct into the user buffer technical note in we work on locked socket so that trick with seq access order and skb user are not required probably code can be easily improved even more",
            "id": "tcp_recvmsg"
        },
        "skb_pull": {
            "ground_truth": "1",
            "function": "skb_pull",
            "description": "net core skbuff remove data from the start of buffer struct sk buff skb buffer to use unsigned int len amount of data to remove this function remove data from the start of buffer returning the memory to the headroom pointer to the next data in the buffer is returned once the data ha been pulled future push will overwrite the old data",
            "id": "skb_pull"
        },
        "ieee80211_get_mmie_keyidx": {
            "ground_truth": "2",
            "function": "ieee80211_get_mmie_keyidx",
            "description": "net mac80211 rx get the bip key index from mmie return if this is not bip frame",
            "id": "ieee80211_get_mmie_keyidx"
        },
        "tun_put_user": {
            "ground_truth": "1",
            "function": "tun_put_user",
            "description": "driver net tun put packet to the user space buffer",
            "id": "tun_put_user"
        },
        "spum_status_process": {
            "ground_truth": "3",
            "function": "spum_status_process",
            "description": "driver crypto bcm spu process the status from spu response message u8 statp start of status word if status is good and response should be processed status indicates an error and response is invalid",
            "id": "spum_status_process"
        },
        "snd_usb_audio_create": {
            "ground_truth": "3",
            "function": "snd_usb_audio_create",
            "description": "sound usb card create chip instance and set it name",
            "id": "snd_usb_audio_create"
        },
        "tcp_process_tlp_ack": {
            "ground_truth": "1",
            "function": "tcp_process_tlp_ack",
            "description": "net ipv4 tcp input this routine deal with acks during tlp episode and end an episode by resetting tlp high seq ref tlp algorithm in draft ietf tcpm rack",
            "id": "tcp_process_tlp_ack"
        },
        "sta_info_update": {
            "ground_truth": "3",
            "function": "sta_info_update",
            "description": "driver staging rtl8723bs core rtw ap called tsr level for usb or sdio interface",
            "id": "sta_info_update"
        },
        "atalk_exit": {
            "ground_truth": "1",
            "function": "atalk_exit",
            "description": "net appletalk ddp no explicit module reference count manipulation is needed in the protocol socket layer set module reference count for u and interface reference counting is done by the network device layer ergo before the appletalk module can be removed all appletalk socket be closed from user space",
            "id": "atalk_exit"
        },
        "pci_find_resource": {
            "ground_truth": "3",
            "function": "pci_find_resource",
            "description": "driver pci pci return matching pci device resource go over standard pci resource bar and check if the given resource is partially or fully contained in any of them in that case the matching resource is returned null otherwise struct pci dev dev pci device to query struct resource re resource to look for",
            "id": "pci_find_resource"
        },
        "usb_audio_probe": {
            "ground_truth": "3",
            "function": "usb_audio_probe",
            "description": "sound usb card probe the active usb device note that this can be called multiple time per device when it includes multiple audio control interface thus we check the usb device pointer and creates the card instance only at the first time the successive call of this function will append the pcm interface to the corresponding card",
            "id": "usb_audio_probe"
        },
        "usb_put_intf": {
            "ground_truth": "3",
            "function": "usb_put_intf",
            "description": "driver usb core usb release use of the usb interface structure must be called when user of an interface is finished with it when the last user of the interface call this function the memory of the interface is freed struct usb interface intf interface that is been decremented",
            "id": "usb_put_intf"
        },
        "__pm_runtime_idle": {
            "ground_truth": "3",
            "function": "__pm_runtime_idle",
            "description": "include linux pm runtime entry point for runtime idle operation if the rpm get put flag is set decrement the device is usage count and return immediately if it is larger than zero then carry out an idle notification either synchronous or asynchronous this routine may be called in atomic context if the rpm async flag is set or if pm runtime irq safe ha been called struct device dev device to send idle notification for int rpmflags flag bit",
            "id": "__pm_runtime_idle"
        },
        "cfg80211_vendor_event_alloc_ucast": {
            "ground_truth": "2",
            "function": "cfg80211_vendor_event_alloc_ucast",
            "description": "include net cfg80211 alloc unicast vendor specific event skb this function allocates and pre fill an skb for an event to send to specific userland socket this socket would previously have been obtained by cfg80211 vendor cmd get sender and the caller must take care to register netlink notifier to see when the socket close if wdev null both the ifindex and identifier of the specified wireless device are added to the event message before the vendor data attribute when done filling the skb call cfg80211 vendor event with the skb to send the event struct wiphy wiphy the wiphy struct wireless dev wdev the wireless device unsigned int portid port id of the receiver int approxlen an upper bound of the length of the data that will be put into the skb int event idx index of the vendor event in the wiphy is vendor event gfp gfp allocation flag an allocated and pre filled skb null if any error happen",
            "id": "cfg80211_vendor_event_alloc_ucast"
        },
        "tcp_rack_detect_loss": {
            "ground_truth": "1",
            "function": "tcp_rack_detect_loss",
            "description": "net ipv4 tcp recovery rack loss detection ietf draft draft ietf tcpm rack mark packet lost if some packet sent later ha been acked the underlying idea is similar to the traditional dupthresh and fack but they look at different metric dupthresh ooo packet delivered packet count fack sequence delta to highest sacked sequence sequence space rack sent time delta to the latest delivered packet time domain the advantage of rack is it applies to both original and retransmitted packet and therefore is robust against tail loss another advantage is being more resilient to reordering by simply allowing some settling delay instead of tweaking the dupthresh when tcp rack detect loss detects some packet are lost and we are not already in the ca recovery state either tcp rack reo timeout or tcp time to recover is trick the loss is proven code path will make u enter the ca recovery state",
            "id": "tcp_rack_detect_loss"
        },
        "ip_rcv_core": {
            "ground_truth": "1",
            "function": "ip_rcv_core",
            "description": "net ipv4 ip input main ip receive routine",
            "id": "ip_rcv_core"
        },
        "reg_process_hint_core": {
            "ground_truth": "2",
            "function": "reg_process_hint_core",
            "description": "net wireless reg process core regulatory request the wireless subsystem can use this function to process regulatory request issued by the regulatory core struct regulatory request core request pending core regulatory request",
            "id": "reg_process_hint_core"
        },
        "bus_for_each_dev": {
            "ground_truth": "3",
            "function": "bus_for_each_dev",
            "description": "driver base bus device iterator iterate over bus is list of device and call fn for each passing it data if start is not null we use that device to begin iterating from we check the return of fn each time if it return anything other than we break out and return that value note the device that return non zero value is not retained in any way nor is it refcount incremented if the caller need to retain this data it should do so and increment the reference count in the supplied callback struct bus type bus bus type struct device start device to start iterating from void data data for the callback int fn struct device void function to be called for each device",
            "id": "bus_for_each_dev"
        },
        "bus_remove_device": {
            "ground_truth": "3",
            "function": "bus_remove_device",
            "description": "driver base bus remove device from bus remove device from all interface remove symlink from bus directory delete device from bus is list detach from it driver drop reference taken in bus add device struct device dev device to be removed",
            "id": "bus_remove_device"
        },
        "ping_rcv": {
            "ground_truth": "1",
            "function": "ping_rcv",
            "description": "net ipv4 ping all we need to do is get the socket",
            "id": "ping_rcv"
        },
        "qlafx00_error_entry": {
            "ground_truth": "0",
            "function": "qlafx00_error_entry",
            "description": "driver scsi qla2xxx qla mr process an error entry scsi qla host vha scsi driver ha context struct rsp que rsp response queue struct sts entry fx00 pkt entry pointer",
            "id": "qlafx00_error_entry"
        },
        "sock_def_wakeup": {
            "ground_truth": "1",
            "function": "sock_def_wakeup",
            "description": "net core sock default socket callback",
            "id": "sock_def_wakeup"
        },
        "__wusbhc_dev_disable": {
            "ground_truth": "2",
            "function": "__wusbhc_dev_disable",
            "description": "disconnect wusb device from the cluster in wireless usb disconnect is basically telling the device he is being disconnected and forgetting about him we send the device device disconnect ie wusb1 for m and then keep going we do not do much in case of error we always pretend we disabled the port and disconnected the device if physically the request did not get there many thing can fail in the way there the stack will reject the device is communication attempt",
            "id": "__wusbhc_dev_disable"
        },
        "tcp_may_update_window": {
            "ground_truth": "1",
            "function": "tcp_may_update_window",
            "description": "net ipv4 tcp input check that window update is acceptable the function assumes that snd una ack snd next",
            "id": "tcp_may_update_window"
        },
        "nf_tproxy_handle_time_wait6": {
            "ground_truth": "1",
            "function": "nf_tproxy_handle_time_wait6",
            "description": "net ipv6 netfilter nf tproxy ipv6 handle ipv6 tcp time wait reopen redirections we have to handle syn packet arriving to time wait socket differently instead of reopening the connection we should rather redirect the new connection to the proxy if there is listener socket present nf tproxy handle time wait6 consumes the socket reference passed in return the listener socket if there is one the time wait socket if no such listener is found or null if the tcp header is incomplete struct sk buff skb the skb being processed int tproto transport protocol int thoff transport protocol header offset struct net net network namespace const struct in6 addr laddr ipv6 address to redirect to const be16 lport tcp port to redirect to or zero struct sock sk the time wait tcp socket found by the lookup",
            "id": "nf_tproxy_handle_time_wait6"
        },
        "usbnet_skb_return": {
            "ground_truth": "3",
            "function": "usbnet_skb_return",
            "description": "driver net usb usbnet pass this packet up the stack updating it accounting some link protocol batch packet so their rx fixup path can return clone a well a just modify the original skb",
            "id": "usbnet_skb_return"
        },
        "find_vmap_lowest_match": {
            "ground_truth": "0",
            "function": "find_vmap_lowest_match",
            "description": "mm vmalloc find the first free block lowest start address in the tree that will accomplish the request corresponding to passing parameter",
            "id": "find_vmap_lowest_match"
        },
        "unmap_region": {
            "ground_truth": "0",
            "function": "unmap_region",
            "description": "driver scsi scsi debug get rid of page table information in the indicated region called with the mm semaphore held",
            "id": "unmap_region"
        },
        "zswap_enabled_param_set": {
            "ground_truth": "0",
            "function": "zswap_enabled_param_set",
            "description": "mm zswap enable disable zswap",
            "id": "zswap_enabled_param_set"
        },
        "build_thisnode_zonelists": {
            "ground_truth": "0",
            "function": "build_thisnode_zonelists",
            "description": "mm page alloc build gfp thisnode zonelists",
            "id": "build_thisnode_zonelists"
        },
        "follow_huge_addr": {
            "ground_truth": "0",
            "function": "follow_huge_addr",
            "description": "include linux hugetlb these function are overwritable if your architecture need it own behavior",
            "id": "follow_huge_addr"
        },
        "steal_suitable_fallback": {
            "ground_truth": "0",
            "function": "steal_suitable_fallback",
            "description": "mm page alloc this function implement actual steal behaviour if order is large enough we can steal whole pageblock if not we first move freepages in this pageblock to our migratetype and determine how many already allocated page are there in the pageblock with compatible migratetype if at least half of page are free or compatible we can change migratetype of the pageblock itself so page freed in the future will be put on the correct free list",
            "id": "steal_suitable_fallback"
        },
        "scan_object": {
            "ground_truth": "0",
            "function": "scan_object",
            "description": "mm kmemleak scan memory block corresponding to kmemleak object condition is that object use count",
            "id": "scan_object"
        },
        "__cache_free": {
            "ground_truth": "0",
            "function": "__cache_free",
            "description": "mm slab release an obj back to it cache if the obj ha constructed state it must be in this state before it is released called with disabled ints",
            "id": "__cache_free"
        },
        "add_to_free_list_tail": {
            "ground_truth": "0",
            "function": "add_to_free_list_tail",
            "description": "mm page alloc used for page not on another list",
            "id": "add_to_free_list_tail"
        },
        "stop_scan_thread": {
            "ground_truth": "0",
            "function": "stop_scan_thread",
            "description": "mm kmemleak stop the automatic memory scanning thread",
            "id": "stop_scan_thread"
        },
        "x509_akid_note_kid": {
            "ground_truth": "0",
            "function": "x509_akid_note_kid",
            "description": "crypto asymmetric key x509 cert parser note key identifier based authoritykeyidentifier",
            "id": "x509_akid_note_kid"
        },
        "print_unreferenced": {
            "ground_truth": "0",
            "function": "print_unreferenced",
            "description": "mm kmemleak printing of the unreferenced object information to the seq file the print unreferenced function must be called with the object lock held",
            "id": "print_unreferenced"
        },
        "set_iounmap_nonlazy": {
            "ground_truth": "0",
            "function": "set_iounmap_nonlazy",
            "description": "mm vmalloc called before call to iounmap if the caller want vm area struct is immediately freed",
            "id": "set_iounmap_nonlazy"
        },
        "migrate_pages": {
            "ground_truth": "0",
            "function": "migrate_pages",
            "description": "include linux migrate migrate page migrate the page specified in list to the free page supplied a the target for the page migration a the target of the page migration fails or null if no special handling is necessary page migration if any the function return after attempt or if no page are movable any more because the list ha become empty or no retryable page exist any more the caller should call putback movable page to return page to the lru or free list only if ret from the list of page to be migrated get new page the function used to allocate free page to be used put new page the function used to free target page if migration private private data to be passed on to get new page mode the migration mode that specifies the constraint for reason the reason for page migration return the number of page that were not migrated or an error code",
            "id": "migrate_pages"
        },
        "mem_cgroup_under_move": {
            "ground_truth": "0",
            "function": "mem_cgroup_under_move",
            "description": "mm memcontrol routine for checking mem is under move account or not checking cgroup is mc from or mc to or under hierarchy of moving cgroups this is for waiting at high memory pressure caused by move",
            "id": "mem_cgroup_under_move"
        },
        "__node_reclaim": {
            "ground_truth": "0",
            "function": "__node_reclaim",
            "description": "mm vmscan try to free up some page from this node through reclaim",
            "id": "__node_reclaim"
        },
        "sparse_decode_mem_map": {
            "ground_truth": "0",
            "function": "sparse_decode_mem_map",
            "description": "mm sparse decode mem map from the coded memmap",
            "id": "sparse_decode_mem_map"
        },
        "pin_user_pages_fast_only": {
            "ground_truth": "0",
            "function": "pin_user_pages_fast_only",
            "description": "mm gup this is the foll pin equivalent of get user page fast only behavior is the same except that this one set foll pin instead of foll get the api rule are the same too no negative value may be returned",
            "id": "pin_user_pages_fast_only"
        },
        "drain_zone_pages": {
            "ground_truth": "0",
            "function": "drain_zone_pages",
            "description": "mm page alloc called from the vmstat counter updater to drain pagesets of this currently executing processor on remote node after they have expired note that this function must be called with the thread pinned to single processor",
            "id": "drain_zone_pages"
        },
        "wp_huge_pmd": {
            "ground_truth": "0",
            "function": "wp_huge_pmd",
            "description": "mm memory inline is required to avoid gcc build error",
            "id": "wp_huge_pmd"
        },
        "page_check_dirty_writeback": {
            "ground_truth": "0",
            "function": "page_check_dirty_writeback",
            "description": "mm vmscan check if page is dirty or under writeback",
            "id": "page_check_dirty_writeback"
        },
        "refill_swap_slots_cache": {
            "ground_truth": "0",
            "function": "refill_swap_slots_cache",
            "description": "mm swap slot called with swap slot cache is alloc lock held",
            "id": "refill_swap_slots_cache"
        },
        "slab_pad_check": {
            "ground_truth": "0",
            "function": "slab_pad_check",
            "description": "mm slub check the pad byte at the end of slab page",
            "id": "slab_pad_check"
        },
        "interleave_nid": {
            "ground_truth": "0",
            "function": "interleave_nid",
            "description": "determine node number for interleave",
            "id": "interleave_nid"
        },
        "invalidate_exceptional_entry2": {
            "ground_truth": "0",
            "function": "invalidate_exceptional_entry2",
            "description": "mm truncate invalidate exceptional entry if clean this handle exceptional entry for invalidate inode pages2 so for dax it evicts only clean entry",
            "id": "invalidate_exceptional_entry2"
        },
        "test_pages_isolated": {
            "ground_truth": "0",
            "function": "test_pages_isolated",
            "description": "mm page isolation caller should ensure that requested range is in single zone",
            "id": "test_pages_isolated"
        },
        "dissolve_free_huge_pages": {
            "ground_truth": "0",
            "function": "dissolve_free_huge_pages",
            "description": "include linux hugetlb dissolve free hugepages in given pfn range used by memory hotplug to make specified memory block removable from the system note that this will dissolve free gigantic hugepage completely if any part of it lie within the given range also note that if dissolve free huge page return with an error all free hugepages that were dissolved before that error are lost",
            "id": "dissolve_free_huge_pages"
        },
        "via_set_rtc_time": {
            "ground_truth": "0",
            "function": "via_set_rtc_time",
            "description": "arch m68k mac misc set the current time to number of second since january this only work on machine with the via based pram rtc which is basically any machine with mac ii style adb",
            "id": "via_set_rtc_time"
        },
        "insert_vm_struct": {
            "ground_truth": "0",
            "function": "insert_vm_struct",
            "description": "insert vm structure into process list sorted by address and into the inode is mmap tree if vm file is non null then mmap rwsem is taken here",
            "id": "insert_vm_struct"
        },
        "migrate_page_move_mapping": {
            "ground_truth": "0",
            "function": "migrate_page_move_mapping",
            "description": "replace the page in the mapping the number of remaining reference must be for anonymous page without mapping for page with mapping for page with mapping and pageprivate set",
            "id": "migrate_page_move_mapping"
        },
        "trace_seq_reset": {
            "ground_truth": "0",
            "function": "trace_seq_reset",
            "description": "tool lib traceevent trace seq re initialize the trace seq structure struct trace seq pointer to the trace seq structure to reset",
            "id": "trace_seq_reset"
        },
        "vma_has_reserves": {
            "ground_truth": "0",
            "function": "vma_has_reserves",
            "description": "mm hugetlb return true if the vma ha associated reserve page",
            "id": "vma_has_reserves"
        },
        "assign_tag": {
            "ground_truth": "0",
            "function": "assign_tag",
            "description": "mm kasan common this function assigns tag to an object considering the following cache might have constructor which might save pointer to slab object somewhere in the object itself we preassign tag for each object in cache with constructor during slab creation and reuse the same tag each time particular object is allocated cache might be slab typesafe by rcu which mean object can be accessed after being freed we preassign tag for object in these cache a well for slab allocator we can not preassign tag randomly since the freelist is stored a an array of index instead of linked list assign tag based on object index so that object that are next to each other get different tag",
            "id": "assign_tag"
        },
        "zswap_frontswap_invalidate_area": {
            "ground_truth": "0",
            "function": "zswap_frontswap_invalidate_area",
            "description": "mm zswap free all zswap entry for the given swap type",
            "id": "zswap_frontswap_invalidate_area"
        },
        "exit_mmap": {
            "ground_truth": "0",
            "function": "exit_mmap",
            "description": "release all the mapping made in process is vm space",
            "id": "exit_mmap"
        },
        "slob_page_alloc": {
            "ground_truth": "0",
            "function": "slob_page_alloc",
            "description": "mm slob slob page alloc allocate slob block within given slob page sp try to find chunk of memory at least size byte big within page sp page to look in size size of the allocation align allocation alignment align offset offset in the allocated block that will be aligned page removed from list return parameter return pointer to memory if allocated null otherwise if the allocation fill up page then the page is removed from the freelist in this case page removed from list will be set to true set to false otherwise",
            "id": "slob_page_alloc"
        },
        "smk_set_cipso": {
            "ground_truth": "0",
            "function": "smk_set_cipso",
            "description": "security smack smackfs do the work for write for cipso and cipso2 accepts only one cipso rule per write call return number of byte written or error code a appropriate struct file file file pointer not actually used const char user buf where to get the data from size count byte sent loff ppos where to start int format smack cipso or smack cipso2",
            "id": "smk_set_cipso"
        },
        "__frontswap_store": {
            "ground_truth": "0",
            "function": "__frontswap_store",
            "description": "mm frontswap store data from page to frontswap and associate it with the page is swaptype and offset page must be locked and in the swap cache if frontswap already contains page with matching swaptype and offset the frontswap implementation may either overwrite the data and return success or invalidate the page from frontswap and return failure",
            "id": "__frontswap_store"
        },
        "mod_zone_state": {
            "ground_truth": "0",
            "function": "mod_zone_state",
            "description": "mm vmstat if we have cmpxchg local support then we do not need to incur the overhead that come with local irq save restore if we use this cpu cmpxchg mod state modifies the zone counter state through atomic per cpu operation overstep mode specifies how overstep should handled no overstepping overstepping half of threshold overstepping minus half of threshold",
            "id": "mod_zone_state"
        },
        "mlxbf_tmfifo_console_output_one": {
            "ground_truth": "0",
            "function": "mlxbf_tmfifo_console_output_one",
            "description": "driver platform mellanox mlxbf tmfifo copy one console packet into the output buffer",
            "id": "mlxbf_tmfifo_console_output_one"
        },
        "unreserve_highatomic_pageblock": {
            "ground_truth": "0",
            "function": "unreserve_highatomic_pageblock",
            "description": "mm page alloc used when an allocation is about to fail under memory pressure this potentially hurt the reliability of high order allocation when under intense memory pressure but failed atomic allocation should be easier to recover from than an oom if force is true try to unreserve pageblock even though highatomic pageblock is exhausted",
            "id": "unreserve_highatomic_pageblock"
        },
        "lazy_max_pages": {
            "ground_truth": "0",
            "function": "lazy_max_pages",
            "description": "mm vmalloc lazy max page is the maximum amount of virtual address space we gather up before attempting to purge with tlb flush there is tradeoff here larger number will cover more kernel page table and take slightly longer to purge but it will linearly reduce the number of global tlb flush that must be performed it would seem natural to scale this number up linearly with the number of cpu because vmapping activity could also scale linearly with the number of cpu however it is likely that in practice workload might be constrained in other way that mean vmap activity will not scale linearly with cpu also want to be conservative and not introduce big latency on huge system so go with le aggressive log scale it will still be an improvement over the old code and it will be simple to change the scale factor if we find that it becomes problem on bigger system",
            "id": "lazy_max_pages"
        },
        "lru_add_page_tail": {
            "ground_truth": "0",
            "function": "lru_add_page_tail",
            "description": "mm swap used by split huge page refcount",
            "id": "lru_add_page_tail"
        },
        "__must_hold": {
            "ground_truth": "0",
            "function": "__must_hold",
            "description": "must be called with resv lock acquired will drop lock to allocate entry",
            "id": "__must_hold"
        },
        "bootstrap": {
            "ground_truth": "0",
            "function": "bootstrap",
            "description": "mm slub used for early kmem cache structure that were allocated using the page allocator allocate them properly then fix up the pointer that may be pointing to the wrong kmem cache structure",
            "id": "bootstrap"
        },
        "get_mctgt_type_thp": {
            "ground_truth": "0",
            "function": "get_mctgt_type_thp",
            "description": "mm memcontrol we do not consider pmd mapped swapping or file mapped page because thp doe not support them for now caller should make sure that pmd trans huge pmd is true",
            "id": "get_mctgt_type_thp"
        },
        "anon_vma_fork": {
            "ground_truth": "0",
            "function": "anon_vma_fork",
            "description": "mm rmap attach vma to it own anon vma a well a to the anon vmas that the corresponding vma in the parent process is attached to return on success non zero on failure",
            "id": "anon_vma_fork"
        },
        "generic_fadvise": {
            "ground_truth": "0",
            "function": "generic_fadvise",
            "description": "posix fadv willneed could set pg referenced and posix fadv noreuse could deactivate the page and clear pg referenced",
            "id": "generic_fadvise"
        },
        "__delete_from_page_cache": {
            "ground_truth": "0",
            "function": "__delete_from_page_cache",
            "description": "mm filemap delete page from the page cache and free it caller ha to make sure the page is locked and that nobody else us it or that usage is safe the caller must hold the page lock",
            "id": "__delete_from_page_cache"
        },
        "__init": {
            "ground_truth": "0",
            "function": "__init",
            "description": "overwritten by architecture with more huge page size",
            "id": "__init"
        },
        "fill_contig_page_info": {
            "ground_truth": "0",
            "function": "fill_contig_page_info",
            "description": "mm vmstat calculate the number of free page in zone how many contiguous page are free and how many are large enough to satisfy an allocation of the target size note that this function make no attempt to estimate how many suitable free block there might be if movable page were migrated calculating that is possible but expensive and can be figured out from userspace",
            "id": "fill_contig_page_info"
        },
        "pmd_none_or_clear_bad_unless_trans_huge": {
            "ground_truth": "0",
            "function": "pmd_none_or_clear_bad_unless_trans_huge",
            "description": "used when setting automatic numa hinting protection where it is critical that numa hinting pmd is not confused with bad pmd",
            "id": "pmd_none_or_clear_bad_unless_trans_huge"
        },
        "do_munmap": {
            "ground_truth": "0",
            "function": "do_munmap",
            "description": "release mapping under nommu condition the chunk to be unmapped must be backed by single vma though it need not cover the whole vma",
            "id": "do_munmap"
        },
        "free_compound_page": {
            "ground_truth": "0",
            "function": "free_compound_page",
            "description": "mm page alloc higher order page are called compound page they are structured thusly the first page size page is called the head page and have pg head set the remaining page size page are called tail page pagetail is encoded in bit of page compound head the rest of bit is pointer to head page the first tail page is compound dtor hold the offset in array of compound page destructors see compound page dtors the first tail page is compound order hold the order of allocation this usage mean that zero order page may not be compound",
            "id": "free_compound_page"
        },
        "nfs_validate_transport_protocol": {
            "ground_truth": "0",
            "function": "nfs_validate_transport_protocol",
            "description": "f nfs f context sanity check the nfs transport protocol",
            "id": "nfs_validate_transport_protocol"
        },
        "assert_populated,": {
            "ground_truth": "0",
            "function": "assert_populated,",
            "description": "walk zone in node and print using callback if assert populated is true only use callback for zone that are populated",
            "id": "assert_populated,"
        },
        "cache_random_seq_destroy": {
            "ground_truth": "0",
            "function": "cache_random_seq_destroy",
            "description": "mm slab destroy the per cache random freelist sequence",
            "id": "cache_random_seq_destroy"
        },
        "offline_mem_sections": {
            "ground_truth": "0",
            "function": "offline_mem_sections",
            "description": "mm sparse mark all memory section within the pfn range a offline",
            "id": "offline_mem_sections"
        },
        "should_continue_reclaim": {
            "ground_truth": "0",
            "function": "should_continue_reclaim",
            "description": "mm vmscan reclaim compaction is used for high order allocation request it reclaims order page before compacting the zone should continue reclaim return true if more page should be reclaimed such that when the page allocator call try to compact page that it will have enough free page to succeed it will give up earlier than that if there is difficulty reclaiming page",
            "id": "should_continue_reclaim"
        },
        "hstate_next_node_to_free": {
            "ground_truth": "0",
            "function": "hstate_next_node_to_free",
            "description": "mm hugetlb helper for free pool huge page return the previously saved node this node from which to free huge page advance the next node id whether or not we find free huge page to free so that the next attempt to free address the next node",
            "id": "hstate_next_node_to_free"
        },
        "prealloc_shrinker": {
            "ground_truth": "0",
            "function": "prealloc_shrinker",
            "description": "mm vmscan add shrinker callback to be called from the vm",
            "id": "prealloc_shrinker"
        },
        "unregister_event": {
            "ground_truth": "0",
            "function": "unregister_event",
            "description": "unregister event callback will be called when userspace close the eventfd or on cgroup removing this callback must be set if you want provide notification functionality",
            "id": "unregister_event"
        },
        "get_ksm_page": {
            "ground_truth": "0",
            "function": "get_ksm_page",
            "description": "mm ksm get ksm page check if the page indicated by the stable node is still it ksm page despite having held no reference to it in which case we can trust the content of the page and it return the gotten page but if the page ha now been zapped remove the stale node from the stable tree and return null but beware the stable node is page might be being migrated you would expect the stable node to hold reference to the ksm page but if it increment the page is count swapping out ha to wait for ksmd to come around again before it can free the page which may take second or even minute much too unresponsive so instead we use keyhole reference access to the ksm page from the stable node peep out through it keyhole to see if that page still hold the right key pointing back to this stable node this relies on freeing pageanon page to reset it page mapping to null and relies on no other use of page to put something that might look like our key in page mapping is on it way to being freed but it is an anomaly to bear in mind",
            "id": "get_ksm_page"
        },
        "get_pages_per_zspage": {
            "ground_truth": "0",
            "function": "get_pages_per_zspage",
            "description": "mm zsmalloc we have to decide on how many page to link together to form zspage for each size class this is important to reduce wastage due to unusable space left at end of each zspage which is given a wastage zp class size usage zp wastage where zp zspage size page size where for example for size class of page size we should link together page size sized page to form zspage since then we can perfectly fit in such object",
            "id": "get_pages_per_zspage"
        },
        "cifs_query_mf_symlink": {
            "ground_truth": "0",
            "function": "cifs_query_mf_symlink",
            "description": "f cifs link smb protocol specific function",
            "id": "cifs_query_mf_symlink"
        },
        "early_memremap": {
            "ground_truth": "0",
            "function": "early_memremap",
            "description": "mm early ioremap remap memory",
            "id": "early_memremap"
        },
        "next_zone": {
            "ground_truth": "0",
            "function": "next_zone",
            "description": "mm mmzone next zone helper magic for for each zone",
            "id": "next_zone"
        },
        "__kmem_cache_shrink": {
            "ground_truth": "0",
            "function": "__kmem_cache_shrink",
            "description": "mm slub kmem cache shrink discard empty slab and promotes the slab filled up most to the head of the partial list new allocation will then fill those up and thus they can be removed from the partial list the slab with the least item are placed last this result in them being allocated from last increasing the chance that the last object are freed in them",
            "id": "__kmem_cache_shrink"
        },
        "kmemleak_boot_config": {
            "ground_truth": "0",
            "function": "kmemleak_boot_config",
            "description": "mm kmemleak allow boot time kmemleak disabling enabled by default",
            "id": "kmemleak_boot_config"
        },
        "kasan_release_vmalloc": {
            "ground_truth": "0",
            "function": "kasan_release_vmalloc",
            "description": "include linux kasan release the backing for the vmalloc region start end which lie within the free region free region start free region end this can be run lazily long after the region wa freed it run under vmap area lock so it is not safe to interact with the vmalloc vmap infrastructure how doe this work we have region that is page aligned labelled a that might not map onto the shadow in way that is page aligned start end aaaaaaaa aa aa aaaaaaaa vmalloc aaaaaa aaaaaaaa aa shadow first we align the start upwards and the end downwards so that the shadow of the region aligns with shadow page boundary in the example this give u the shadow page this is the shadow entirely covered by this allocation then we have the tricky bit we want to know if we can free the partially covered shadow page and in the example for this we are given the start and end of the free region that contains this allocation extending our previous example we could have free region start free region end start end ffffffff ffffffff aaaaaaaa aa aa aaaaaaaa ffffffff vmalloc ffaaaaaa aaaaaaaa aaf shadow once again we align the start of the free region up and the end of the free region down so that the shadow is page aligned so we can free page we know no allocation currently us anything in that page because all of it is in the vmalloc free region but we cannot free page because we can not be sure that the rest of it is unused we only consider page that contain part of the original region for freeing we do not try to free other page from the free region or we would end up trying to free huge chunk of virtual address space concurrency how do we know that we are not freeing page that is simultaneously being used for fresh allocation in kasan populate vmalloc pte we can have kasan release vmalloc and kasan populate vmalloc running at the same time while we run under free vmap area lock the population code doe not free vmap area lock instead operates to ensure that the larger range free region start free region end is safe because alloc vmap area and the per cpu region finding algorithm both run under free vmap area lock no space identified a free will become used while we are running this mean that so long a we are careful with alignment and only free shadow page entirely covered by the free region we will not run in to any trouble any simultaneous allocation will be for disjoint region",
            "id": "kasan_release_vmalloc"
        },
        "report_meminit": {
            "ground_truth": "0",
            "function": "report_meminit",
            "description": "init main report memory auto initialization state for this boot",
            "id": "report_meminit"
        },
        "make_gray_object": {
            "ground_truth": "0",
            "function": "make_gray_object",
            "description": "mm kmemleak mark an object permanently a gray colored so that it can no longer be reported a leak this is used in general to mark false positive",
            "id": "make_gray_object"
        },
        "slob_page_free": {
            "ground_truth": "0",
            "function": "slob_page_free",
            "description": "mm slob slob page free true for page on free slob page list",
            "id": "slob_page_free"
        },
        "add_scan_area": {
            "ground_truth": "0",
            "function": "add_scan_area",
            "description": "mm kmemleak add scanning area to the object if at least one such area is added kmemleak will only scan these range rather than the whole memory block",
            "id": "add_scan_area"
        },
        "kmemleak_do_cleanup": {
            "ground_truth": "0",
            "function": "kmemleak_do_cleanup",
            "description": "mm kmemleak stop the memory scanning thread and free the kmemleak internal object if no previous scan thread otherwise kmemleak may still have some useful information on memory leak",
            "id": "kmemleak_do_cleanup"
        },
        "setup_nr_node_ids": {
            "ground_truth": "0",
            "function": "setup_nr_node_ids",
            "description": "include linux mm figure out the number of possible node id",
            "id": "setup_nr_node_ids"
        },
        "__lock_page_or_retry": {
            "ground_truth": "0",
            "function": "__lock_page_or_retry",
            "description": "mm filemap return value page is locked mmap lock is still held page is not locked mmap lock ha been released mmap read unlock unless flag had both fault flag allow retry and fault flag retry nowait set in which case mmap lock is still held if neither allow retry nor killable are set will always return with the page locked and the mmap lock unperturbed",
            "id": "__lock_page_or_retry"
        },
        "memcg_event_wake": {
            "ground_truth": "0",
            "function": "memcg_event_wake",
            "description": "mm memcontrol get called on epollhup on eventfd when user close it called with wqh lock held and interrupt disabled",
            "id": "memcg_event_wake"
        },
        "is_sysrq_oom": {
            "ground_truth": "0",
            "function": "is_sysrq_oom",
            "description": "mm oom kill order mean the oom kill is required by sysrq otherwise only for display purpose",
            "id": "is_sysrq_oom"
        },
        "do_wp_page": {
            "ground_truth": "0",
            "function": "do_wp_page",
            "description": "mm memory this routine handle present page when user try to write to shared page it is done by copying the page to new address and decrementing the shared page counter for the old page note that this routine assumes that the protection check have been done by the caller the low level page fault routine in most case thus we can safely just mark it writable once we have done any necessary cow we also mark the page dirty at this point even though the page will change only once the write actually happens this avoids few race and potentially make it more efficient we enter with non exclusive mmap lock to exclude vma change but allow concurrent fault with pte both mapped and locked we return with mmap lock still held but pte unmapped and unlocked",
            "id": "do_wp_page"
        },
        "queue_pages_pte_range": {
            "ground_truth": "0",
            "function": "queue_pages_pte_range",
            "description": "scan through page checking if page follow certain condition and move them to the pagelist if they do queue page pte range ha three possible return value page are placed on the right node or queued successfully there is unmovable page and mpol mf move mpol mf strict were specified eio only mpol mf strict wa specified and an existing page wa already on node that doe not follow the policy",
            "id": "queue_pages_pte_range"
        },
        "compaction_restarting": {
            "ground_truth": "0",
            "function": "compaction_restarting",
            "description": "mm compaction return true if restarting compaction after many failure",
            "id": "compaction_restarting"
        },
        "move_freepages": {
            "ground_truth": "0",
            "function": "move_freepages",
            "description": "mm page alloc move the free page in range to the freelist tail of the requested type note that start page and end page are not aligned on pageblock boundary if alignment is required use move freepages block",
            "id": "move_freepages"
        },
        "mlock_migrate_page": {
            "ground_truth": "0",
            "function": "mlock_migrate_page",
            "description": "mm internal mlock migrate page called only from migrate misplaced transhuge page because that doe not go through the full procedure of migration ptes to migrate the mlocked page flag update statistic",
            "id": "mlock_migrate_page"
        },
        "find_extend_vma": {
            "ground_truth": "0",
            "function": "find_extend_vma",
            "description": "find vma we do not extend stack vmas under nommu condition",
            "id": "find_extend_vma"
        },
        "process_huge_page": {
            "ground_truth": "0",
            "function": "process_huge_page",
            "description": "mm memory process all subpages of the specified huge page with the specified operation the target subpage will be processed last to keep it cache line hot",
            "id": "process_huge_page"
        },
        "init_admin_reserve": {
            "ground_truth": "0",
            "function": "init_admin_reserve",
            "description": "initialise sysctl admin reserve kbytes the purpose of sysctl admin reserve kbytes is to allow the sys admin to log in and kill memory hogging process system with more than 256mb will reserve 8mb enough to recover with sshd bash and top in overcommit guess smaller system will only reserve of free page by default",
            "id": "init_admin_reserve"
        },
        "semctl_down": {
            "ground_truth": "0",
            "function": "semctl_down",
            "description": "this function handle some semctl command which require the rwsem to be held in write mode note no lock must be held the rwsem is taken inside this function",
            "id": "semctl_down"
        },
        "collect_procs_anon": {
            "ground_truth": "0",
            "function": "collect_procs_anon",
            "description": "mm memory failure collect process when the error hit an anonymous page",
            "id": "collect_procs_anon"
        },
        "free_pcp_prepare": {
            "ground_truth": "0",
            "function": "free_pcp_prepare",
            "description": "mm page alloc with debug vm enabled order page are checked immediately when being freed to pcp list with debug pagealloc also enabled they are also rechecked when moved from pcp list to free list",
            "id": "free_pcp_prepare"
        },
        "may_expand_vm": {
            "ground_truth": "0",
            "function": "may_expand_vm",
            "description": "return true if the calling process may expand it vm space by the passed number of page",
            "id": "may_expand_vm"
        },
        "get_any_partial": {
            "ground_truth": "0",
            "function": "get_any_partial",
            "description": "mm slub get page from somewhere search in increasing numa distance",
            "id": "get_any_partial"
        },
        "__frontswap_invalidate_area": {
            "ground_truth": "0",
            "function": "__frontswap_invalidate_area",
            "description": "mm frontswap invalidate all data from frontswap associated with all offset for the specified swaptype",
            "id": "__frontswap_invalidate_area"
        },
        "kfree_debugcheck": {
            "ground_truth": "0",
            "function": "kfree_debugcheck",
            "description": "mm slab perform extra freeing check detect bad pointer poison red zone checking",
            "id": "kfree_debugcheck"
        },
        "set_slob": {
            "ground_truth": "0",
            "function": "set_slob",
            "description": "mm slob encode the given size and next info into free slob block",
            "id": "set_slob"
        },
        "page_cache_delete_batch": {
            "ground_truth": "0",
            "function": "page_cache_delete_batch",
            "description": "mm filemap page cache delete batch delete several page from page cache the function walk over mapping page and remove page passed in pvec from the mapping the function expects pvec to be sorted by page index and is optimised for it to be dense it tolerates hole in pvec mapping entry at those index are not modified the function expects only thp head page to be present in the the function expects the page lock to be held mapping the mapping to which page belong pvec pagevec with page to delete pvec",
            "id": "page_cache_delete_batch"
        },
        "madvise_need_mmap_write": {
            "ground_truth": "0",
            "function": "madvise_need_mmap_write",
            "description": "any behaviour which result in change to the vma vm flag need to take mmap lock for writing others which simply traverse vmas need to only take it for reading",
            "id": "madvise_need_mmap_write"
        },
        "smack_audit_rule_match": {
            "ground_truth": "0",
            "function": "smack_audit_rule_match",
            "description": "security smack smack lsm audit given object the core audit hook it is used to take the decision of whether to audit or not to audit given object u32 secid security id for identifying the object to test u32 field audit rule flag given from user space u32 op required testing operator void vrule smack internal rule presentation",
            "id": "smack_audit_rule_match"
        },
        "shmem_replace_entry": {
            "ground_truth": "0",
            "function": "shmem_replace_entry",
            "description": "mm shmem replace item expected in xarray by new item while holding xa lock",
            "id": "shmem_replace_entry"
        },
        "memcg_to_vmpressure": {
            "ground_truth": "0",
            "function": "memcg_to_vmpressure",
            "description": "mm memcontrol some nice accessors for the vmpressure",
            "id": "memcg_to_vmpressure"
        },
        "mlock_fixup": {
            "ground_truth": "0",
            "function": "mlock_fixup",
            "description": "mlock fixup handle mlock all munlock all request filter out special vmas vm locked never get set for these and munlock is no op however for some special vmas we go ahead and populate the ptes for vmas that pas the filter merge split a appropriate",
            "id": "mlock_fixup"
        },
        "sum_zone_node_page_state": {
            "ground_truth": "0",
            "function": "sum_zone_node_page_state",
            "description": "mm vmstat determine the per node value of stat item this function is called frequently in numa machine so try to be a frugal a possible",
            "id": "sum_zone_node_page_state"
        },
        "pagetypeinfo_showblockcount": {
            "ground_truth": "0",
            "function": "pagetypeinfo_showblockcount",
            "description": "mm vmstat print out the number of pageblocks for each migratetype",
            "id": "pagetypeinfo_showblockcount"
        },
        "init_cache_random_seq": {
            "ground_truth": "0",
            "function": "init_cache_random_seq",
            "description": "mm slub pre initialize the random sequence cache",
            "id": "init_cache_random_seq"
        },
        "validate_nommu_regions": {
            "ground_truth": "0",
            "function": "validate_nommu_regions",
            "description": "validate the region tree the caller must hold the region lock",
            "id": "validate_nommu_regions"
        },
        "migrate_to_node": {
            "ground_truth": "0",
            "function": "migrate_to_node",
            "description": "migrate page from one node to target node return error or the number of page not migrated",
            "id": "migrate_to_node"
        },
        "cvmx_spi_restart_interface": {
            "ground_truth": "0",
            "function": "cvmx_spi_restart_interface",
            "description": "driver staging octeon octeon stub with it corespondant system int interface the identifier of the packet interface to configure and use a spi interface cvmx spi mode mode the operating mode for the spi interface the interface can operate a full duplex both tx and rx data path active or a halfplex either the tx data path is active or the rx data path is active but not both int timeout timeout to wait for clock synchronization in second return zero on success negative of failure",
            "id": "cvmx_spi_restart_interface"
        },
        "defer_init": {
            "ground_truth": "0",
            "function": "defer_init",
            "description": "mm page alloc return true when the remaining initialisation should be deferred until later in the boot cycle when it can be parallelised",
            "id": "defer_init"
        },
        "copy_vma": {
            "ground_truth": "0",
            "function": "copy_vma",
            "description": "copy the vma structure to new location in the same mm prior to moving page table entry to effect an mremap move",
            "id": "copy_vma"
        },
        "__alloc_pages_direct_reclaim": {
            "ground_truth": "0",
            "function": "__alloc_pages_direct_reclaim",
            "description": "mm page alloc the really slow allocator path where we enter direct reclaim",
            "id": "__alloc_pages_direct_reclaim"
        },
        "ksys_shmdt": {
            "ground_truth": "0",
            "function": "ksys_shmdt",
            "description": "detach and kill segment if marked destroyed the work is done in shm close",
            "id": "ksys_shmdt"
        },
        "__rmqueue_smallest": {
            "ground_truth": "0",
            "function": "__rmqueue_smallest",
            "description": "mm page alloc go through the free list for the given migratetype and remove the smallest available page from the freelists",
            "id": "__rmqueue_smallest"
        },
        "deferred_init_pages": {
            "ground_truth": "0",
            "function": "deferred_init_pages",
            "description": "mm page alloc initialize struct page we minimize pfn page lookup and scheduler check by performing it only once every pageblock nr page return number of page initialized",
            "id": "deferred_init_pages"
        },
        "compaction_ready": {
            "ground_truth": "0",
            "function": "compaction_ready",
            "description": "mm vmscan return true if compaction should go ahead for costly order request or the allocation would already succeed without compaction return false if we should reclaim first",
            "id": "compaction_ready"
        },
        "next_node_allowed": {
            "ground_truth": "0",
            "function": "next_node_allowed",
            "description": "mm hugetlb common helper function for hstate next node to alloc free we may have allocated or freed huge page based on different node allowed previously so next node to alloc free might be outside of node allowed ensure that we use an allowed node for alloc or free",
            "id": "next_node_allowed"
        },
        "take_page_off_buddy": {
            "ground_truth": "0",
            "function": "take_page_off_buddy",
            "description": "mm page alloc take page that will be marked a poisoned off the buddy allocator",
            "id": "take_page_off_buddy"
        },
        "print_bad_pte": {
            "ground_truth": "0",
            "function": "print_bad_pte",
            "description": "mm memory this function is called to print an error when bad pte is found for example we might have pfn mapped pte in region that doe not allow it the calling function must still handle the error",
            "id": "print_bad_pte"
        },
        "__cvmx_helper_xaui_link_get": {
            "ground_truth": "0",
            "function": "__cvmx_helper_xaui_link_get",
            "description": "arch mips cavium octeon executive cvmx helper xaui auto negotiation the result of this function may not match octeon is link config if auto negotiation ha changed since the last call to cvmx helper link set return link state int ipd port ipd pko port to query",
            "id": "__cvmx_helper_xaui_link_get"
        },
        "memory_failure_queue_kick": {
            "ground_truth": "0",
            "function": "memory_failure_queue_kick",
            "description": "mm memory failure process memory failure work queued on the specified cpu used to avoid return to userspace racing with the memory failure workqueue",
            "id": "memory_failure_queue_kick"
        },
        "__delete_object": {
            "ground_truth": "0",
            "function": "__delete_object",
            "description": "mm kmemleak mark the object a not allocated and schedule rcu freeing via put object",
            "id": "__delete_object"
        },
        "mapping_needs_writeback": {
            "ground_truth": "0",
            "function": "mapping_needs_writeback",
            "description": "mm filemap return true if writeback might be needed or already in progress",
            "id": "mapping_needs_writeback"
        },
        "pgdat_balanced": {
            "ground_truth": "0",
            "function": "pgdat_balanced",
            "description": "mm vmscan return true if there is an eligible zone balanced for the request order and highest zoneidx",
            "id": "pgdat_balanced"
        },
        "get_rkey": {
            "ground_truth": "0",
            "function": "get_rkey",
            "description": "f reiserfs stree get delimiting key of the buffer at the path and it right neighbor",
            "id": "get_rkey"
        },
        "alloc_large_system_hash": {
            "ground_truth": "0",
            "function": "alloc_large_system_hash",
            "description": "mm page alloc allocate large system hash table from bootmem it is assumed that the hash table must contain an exact power of quantity of entry limit is the number of hash bucket not the total allocation size",
            "id": "alloc_large_system_hash"
        },
        "migrate_page_states": {
            "ground_truth": "0",
            "function": "migrate_page_states",
            "description": "include linux migrate copy the page to it new location",
            "id": "migrate_page_states"
        },
        "shmem_seek_hole_data": {
            "ground_truth": "0",
            "function": "shmem_seek_hole_data",
            "description": "mm shmem llseek seek data or seek hole through the page cache",
            "id": "shmem_seek_hole_data"
        },
        "page_address_in_vma": {
            "ground_truth": "0",
            "function": "page_address_in_vma",
            "description": "mm rmap at what user virtual address is page expected in vma caller should check the page is actually part of the vma",
            "id": "page_address_in_vma"
        },
        "get_partial_node": {
            "ground_truth": "0",
            "function": "get_partial_node",
            "description": "mm slub try to allocate partial slab from specific node",
            "id": "get_partial_node"
        },
        "__gfp_pfmemalloc_flags": {
            "ground_truth": "0",
            "function": "__gfp_pfmemalloc_flags",
            "description": "mm page alloc distinguish request which really need access to full memory reserve from oom victim which can live with portion of it",
            "id": "__gfp_pfmemalloc_flags"
        },
        "fallback_alloc": {
            "ground_truth": "0",
            "function": "fallback_alloc",
            "description": "mm slab fallback function if there wa no memory available and no object on certain node and fall back is permitted first we scan all the available node for available object if that fails then we perform an allocation without specifying node this allows the page allocator to do it reclaim fallback magic we then insert the slab into the proper nodelist and then allocate from it",
            "id": "fallback_alloc"
        },
        "free_object_rcu": {
            "ground_truth": "0",
            "function": "free_object_rcu",
            "description": "mm kmemleak rcu callback to free kmemleak object",
            "id": "free_object_rcu"
        },
        "get_init_ra_size": {
            "ground_truth": "0",
            "function": "get_init_ra_size",
            "description": "set the initial window size round to next power of and square for small size for medium and for large for 128k page max ra page 32k initial page 128k initial",
            "id": "get_init_ra_size"
        },
        "find_zone_movable_pfns_for_nodes": {
            "ground_truth": "0",
            "function": "find_zone_movable_pfns_for_nodes",
            "description": "mm page alloc find the pfn the movable zone begin in each node kernel memory is spread evenly between node a long a the node have enough memory when they do not some node will have more kernelcore than others",
            "id": "find_zone_movable_pfns_for_nodes"
        },
        "mlock_vma_page": {
            "ground_truth": "0",
            "function": "mlock_vma_page",
            "description": "mm internal must be called with vma is mmap lock held for read or write and page locked",
            "id": "mlock_vma_page"
        },
        "alloc_pool_huge_page": {
            "ground_truth": "0",
            "function": "alloc_pool_huge_page",
            "description": "mm hugetlb allocates fresh page to the hugetlb allocator pool in the node interleaved manner",
            "id": "alloc_pool_huge_page"
        },
        "pagevec_move_tail": {
            "ground_truth": "0",
            "function": "pagevec_move_tail",
            "description": "mm swap pagevec move tail must be called with irq disabled otherwise this may cause nasty race",
            "id": "pagevec_move_tail"
        },
        "init_list": {
            "ground_truth": "0",
            "function": "init_list",
            "description": "driver medium platform mtk vcodec vdec vdec vp8 if swap the static kmem cache node with kmalloced memory",
            "id": "init_list"
        },
        "swap_do_scheduled_discard": {
            "ground_truth": "0",
            "function": "swap_do_scheduled_discard",
            "description": "doing discard actually after cluster discard is finished the cluster will be added to free cluster list caller should hold si lock",
            "id": "swap_do_scheduled_discard"
        },
        "try_grab_compound_head": {
            "ground_truth": "0",
            "function": "try_grab_compound_head",
            "description": "mm gup try grab compound head attempt to elevate page is refcount by flag dependent amount grab name in this file mean look at flag to decide whether to use foll pin or foll get behavior when incrementing the page is refcount either foll pin or foll get or neither must be set but not both at the same time that is true throughout the get user page and pin user page apis case foll get page is refcount will be incremented by foll pin page is refcount will be incremented by gup pin counting bias return head page with refcount appropriately incremented for success or null upon failure if neither foll get nor foll pin wa set that is considered failure and furthermore likely bug in the caller so warning is also emitted",
            "id": "try_grab_compound_head"
        },
        "overlap_memmap_init": {
            "ground_truth": "0",
            "function": "overlap_memmap_init",
            "description": "mm page alloc if zone is zone movable but memory is mirrored it is an overlapped init",
            "id": "overlap_memmap_init"
        },
        "kmem_getpages": {
            "ground_truth": "0",
            "function": "kmem_getpages",
            "description": "mm slab interface to system is page allocator no need to hold the kmem cache node list lock if we requested dmaable memory we will get it even if we did not request dmaable memory we might get it but that would be relatively rare and ignorable",
            "id": "kmem_getpages"
        },
        "in_reclaim_compaction": {
            "ground_truth": "0",
            "function": "in_reclaim_compaction",
            "description": "mm vmscan use reclaim compaction for costly allocs or under memory pressure",
            "id": "in_reclaim_compaction"
        },
        "move_to_new_page": {
            "ground_truth": "0",
            "function": "move_to_new_page",
            "description": "move page to newly allocated page the page is locked and all ptes have been successfully removed the new page will have replaced the old page if this function is successful return value error code migratepage success success",
            "id": "move_to_new_page"
        },
        "inactive_is_low": {
            "ground_truth": "0",
            "function": "inactive_is_low",
            "description": "mm vmscan the inactive anon list should be small enough that the vm never ha to do too much work the inactive file list should be small enough to leave most memory to the established workingset on the scan resistant active list but large enough to avoid thrashing the aggregate readahead window both inactive list should also be large enough that each inactive page ha chance to be referenced again before it is reclaimed if that fails and refaulting is observed the inactive list grows the inactive ratio is the target ratio of active to inactive page on this lru maintained by the pageout code an inactive ratio of mean or of the page are kept on the inactive list total target max memory ratio inactive 10mb 5mb 100mb 50mb 1gb 250mb 10gb 9gb 100gb 3gb 1tb 10gb 10tb 32gb",
            "id": "inactive_is_low"
        },
        "shared_policy_replace": {
            "ground_truth": "0",
            "function": "shared_policy_replace",
            "description": "replace policy range",
            "id": "shared_policy_replace"
        },
        "prom_seek": {
            "ground_truth": "0",
            "function": "prom_seek",
            "description": "do seek operation on the device described by the passed integer descriptor",
            "id": "prom_seek"
        },
        "get_freelist": {
            "ground_truth": "0",
            "function": "get_freelist",
            "description": "mm slub check the page freelist of page and either transfer the freelist to the per cpu freelist or deactivate the page the page is still frozen if the return value is not null if this function return null then the page ha been unfrozen this function must be called with interrupt disabled",
            "id": "get_freelist"
        },
        "arch_reserved_kernel_pages": {
            "ground_truth": "0",
            "function": "arch_reserved_kernel_pages",
            "description": "arch powerpc kernel fadump return the number of page that arch ha reserved but is not known to alloc large system hash",
            "id": "arch_reserved_kernel_pages"
        },
        "free_pte_range": {
            "ground_truth": "0",
            "function": "free_pte_range",
            "description": "mm memory note this doe not free the actual page themselves that ha been handled earlier when unmapping all the memory region",
            "id": "free_pte_range"
        },
        "zswap_get_swap_cache_page": {
            "ground_truth": "0",
            "function": "zswap_get_swap_cache_page",
            "description": "mm zswap zswap get swap cache page this is an adaption of read swap cache async this function try to find page with the given swap entry in the swapper space address space the swap cache if the page is found it is returned in retpage otherwise page is allocated added to the swap cache and returned in retpage if success the swap cache page is returned in retpage return zswap swapcache exist if page wa already in the swap cache return zswap swapcache new if the new page need to be populated the new page is added to swapcache and locked return zswap swapcache fail on error",
            "id": "zswap_get_swap_cache_page"
        },
        "read_24x7_sys_info": {
            "ground_truth": "0",
            "function": "read_24x7_sys_info",
            "description": "arch powerpc perf hv 24x7 read 24x7 sys info retrieve the number of socket and chip per socket and core per chip detail through the get system parameter rtas call",
            "id": "read_24x7_sys_info"
        },
        "note_kasan_page_table": {
            "ground_truth": "0",
            "function": "note_kasan_page_table",
            "description": "mm ptdump this is an optimization for kasan case since all kasan page table eventually point to the kasan early shadow page we could call note page right away without walking through lower level page table this save u dozen of second minute for level config while checking for mapping or reading kernel page table debugfs file",
            "id": "note_kasan_page_table"
        },
        "rmqueue_pcplist": {
            "ground_truth": "0",
            "function": "rmqueue_pcplist",
            "description": "mm page alloc lock and remove page from the per cpu list",
            "id": "rmqueue_pcplist"
        },
        "purge_vmap_area_lazy": {
            "ground_truth": "0",
            "function": "purge_vmap_area_lazy",
            "description": "mm vmalloc kick off purge of the outstanding lazy area",
            "id": "purge_vmap_area_lazy"
        },
        "migrate_vma_collect": {
            "ground_truth": "0",
            "function": "migrate_vma_collect",
            "description": "migrate vma collect collect page over range of virtual address this will walk the cpu page table for each virtual address backed by valid page it update the src array and take reference on the page in order to pin the page until we lock it and unmap it migrate migrate struct containing all migration information",
            "id": "migrate_vma_collect"
        },
        "find_and_get_object": {
            "ground_truth": "0",
            "function": "find_and_get_object",
            "description": "mm kmemleak look up an object in the object search tree and increase it use count",
            "id": "find_and_get_object"
        },
        "move_pages_to_lru": {
            "ground_truth": "0",
            "function": "move_pages_to_lru",
            "description": "mm vmscan this move page from list to corresponding lru list we move them the other way if the page is referenced by one or more process from rmap if the page are mostly unmapped the processing is fast and it is appropriate to hold zone lru lock across the whole operation but if the page are mapped the processing is slow page referenced so we should drop zone lru lock around each page it is impossible to balance this so instead we remove the page from the lru while processing them it is safe to rely on pg active against the non lru page in here because nobody will play with that bit on non lru page the downside is that we have to touch page refcount against each page but we had to alter page flag anyway return the number of page moved to the given lruvec",
            "id": "move_pages_to_lru"
        },
        "kmem_cache_alloc_bulk": {
            "ground_truth": "0",
            "function": "kmem_cache_alloc_bulk",
            "description": "mm slub note that interrupt must be enabled when calling this function",
            "id": "kmem_cache_alloc_bulk"
        },
        "wp_clean_test_walk": {
            "ground_truth": "0",
            "function": "wp_clean_test_walk",
            "description": "mm mapping dirty helper wp clean test walk the pagewalk test walk callback wo not perform dirty tracking on cow read only or hugetlb vmas",
            "id": "wp_clean_test_walk"
        },
        "mpol_shared_policy_lookup": {
            "ground_truth": "0",
            "function": "mpol_shared_policy_lookup",
            "description": "include linux mempolicy find shared policy intersecting idx",
            "id": "mpol_shared_policy_lookup"
        },
        "remove_vma": {
            "ground_truth": "0",
            "function": "remove_vma",
            "description": "close vm structure and free it returning the next",
            "id": "remove_vma"
        },
        "writeout_period": {
            "ground_truth": "0",
            "function": "writeout_period",
            "description": "mm page writeback on idle system we can be called long after we scheduled because we use deferred timer so count with missed period",
            "id": "writeout_period"
        },
        "all_vm_events": {
            "ground_truth": "0",
            "function": "all_vm_events",
            "description": "include linux vmstat accumulate the vm event counter across all cpu the result is unavoidably approximate it can change during and after execution of this function",
            "id": "all_vm_events"
        },
        "parse_early_param": {
            "ground_truth": "0",
            "function": "parse_early_param",
            "description": "init main arch code call this early on or if not just before other parsing",
            "id": "parse_early_param"
        },
        "early_calculate_totalpages": {
            "ground_truth": "0",
            "function": "early_calculate_totalpages",
            "description": "mm page alloc early calculate totalpages sum page in active region for movable zone populate memory for calculating usable node",
            "id": "early_calculate_totalpages"
        },
        "do_memsw_account": {
            "ground_truth": "0",
            "function": "do_memsw_account",
            "description": "mm memcontrol whether legacy memory swap accounting is active",
            "id": "do_memsw_account"
        },
        "fragmentation_index": {
            "ground_truth": "0",
            "function": "fragmentation_index",
            "description": "mm vmstat same a fragmentation index but allocs contig page info on stack",
            "id": "fragmentation_index"
        },
        "init_unavailable_mem": {
            "ground_truth": "0",
            "function": "init_unavailable_mem",
            "description": "mm page alloc only struct page that are backed by physical memory are zeroed and initialized by going through init single page but there are some struct page which are reserved in memblock allocator and their field may be accessed for example page to pfn on some configuration access flag we must explicitly initialize those struct page this function also address similar issue where struct page are left uninitialized because the physical address range is not covered by memblock memory or memblock reserved that could happen when memblock layout is manually configured via memmap or when the highest physical address max pfn doe not end on section boundary",
            "id": "init_unavailable_mem"
        },
        "sparse_encode_early_nid": {
            "ground_truth": "0",
            "function": "sparse_encode_early_nid",
            "description": "mm sparse during early boot before section mem map is used for an actual mem map we use section mem map to store the section is numa node this keep u from having to use another data structure the node information is cleared just before we store the real mem map",
            "id": "sparse_encode_early_nid"
        },
        "put_cpu_partial": {
            "ground_truth": "0",
            "function": "put_cpu_partial",
            "description": "mm slub put page that wa just frozen in slab free get partial node into partial page slot if available if we did not find slot then simply move all the partial to the per node partial list",
            "id": "put_cpu_partial"
        },
        "drain_pages_zone": {
            "ground_truth": "0",
            "function": "drain_pages_zone",
            "description": "mm page alloc drain pcplists of the indicated processor and zone the processor must either be the current processor and the thread pinned to the current processor or processor that is not online",
            "id": "drain_pages_zone"
        },
        "access_process_vm": {
            "ground_truth": "0",
            "function": "access_process_vm",
            "description": "mm memory access another process address space source target buffer must be kernel space",
            "id": "access_process_vm"
        },
        "gup_get_pte": {
            "ground_truth": "0",
            "function": "gup_get_pte",
            "description": "mm gup warning only to be used in the get user page fast implementation with get user page fast we walk down the pagetables without taking any lock for this we would like to load the pointer atomically but sometimes that is not possible without expensive cmpxchg8b on x86 pae what we do have is the guarantee that pte will only either go from not present to present or present to not present or both it will not switch to completely different present page without tlb flush in between something that we are blocking by holding interrupt off setting ptes from not present to present go ptep pte high smp wmb ptep pte low and present to not present go ptep pte low smp wmb ptep pte high we must ensure here that the load of pte low see iff pte high see we load pte high after loading pte low which ensures we do not see an older value of pte high then we recheck pte low which ensures that we have not picked up changed pte high we might have gotten rubbish value from pte low and pte high but we are guaranteed that pte low will not have the present bit set unless it is because get user page fast only operates on present ptes we are safe",
            "id": "gup_get_pte"
        },
        "__do_notify": {
            "ground_truth": "0",
            "function": "__do_notify",
            "description": "the next function is only to split too long sys mq timedsend",
            "id": "__do_notify"
        },
        "merge_or_add_vmap_area": {
            "ground_truth": "0",
            "function": "merge_or_add_vmap_area",
            "description": "mm vmalloc merge de allocated chunk of va memory with previous and next free block if coalesce is not done new free area is inserted if va ha been merged it is freed please note it can return null in case of overlap range followed by warn report despite it is buggy behaviour system can be alive and keep ongoing",
            "id": "merge_or_add_vmap_area"
        },
        "compaction_defer_reset": {
            "ground_truth": "0",
            "function": "compaction_defer_reset",
            "description": "mm compaction update defer tracking counter after successful compaction of given order which mean an allocation either succeeded alloc success true or is expected to succeed",
            "id": "compaction_defer_reset"
        },
        "set_buffer_control": {
            "ground_truth": "0",
            "function": "set_buffer_control",
            "description": "driver infiniband hw hfi1 chip the number of credit on the vls may be changed while everything is live but the following algorithm must be followed due to how the hardware is actually implemented in particular return credit status is the only correct status check if reducing global shared credit limit or any shared limit changing set global shared credit limit use all vl mask0 all vls that are changing either dedicated or shared limit set shared limit mask0 spin until return credit status use all vl all vl mask0 if changing any dedicated limit mask1 all vls that are lowering dedicated limit lower dedicated limit mask1 spin until return credit status mask1 raise dedicated limit raise shared limit raise global shared credit limit lower if the new limit is lower set the limit to the new value raise if the new limit is higher than the current value may be changed earlier in the algorithm set the new limit to the new value",
            "id": "set_buffer_control"
        },
        "wp_clean_post_vma": {
            "ground_truth": "0",
            "function": "wp_clean_post_vma",
            "description": "mm mapping dirty helper wp clean post vma the pagewalk post vma callback the post vma callback performs the tlb flush and call necessary mmu notifiers",
            "id": "wp_clean_post_vma"
        },
        "sh_console_setup": {
            "ground_truth": "0",
            "function": "sh_console_setup",
            "description": "arch sh kernel sh bios setup initial baud bit parity we do two thing here construct cflag setting for the first r open initialize the serial port return non zero if we did not find serial port",
            "id": "sh_console_setup"
        },
        "__free_one": {
            "ground_truth": "0",
            "function": "__free_one",
            "description": "mm slab alien lock must be held by alien caller",
            "id": "__free_one"
        },
        "allocate_file_region_entries": {
            "ground_truth": "0",
            "function": "allocate_file_region_entries",
            "description": "mm hugetlb must be called with resv lock acquired will drop lock to allocate entry",
            "id": "allocate_file_region_entries"
        },
        "pagetypeinfo_showfree": {
            "ground_truth": "0",
            "function": "pagetypeinfo_showfree",
            "description": "mm vmstat print out the free page at each order for each migatetype",
            "id": "pagetypeinfo_showfree"
        },
        "need_update": {
            "ground_truth": "0",
            "function": "need_update",
            "description": "mm vmstat check if the diffs for certain cpu indicate that an update is needed",
            "id": "need_update"
        },
        "unregister_shrinker": {
            "ground_truth": "0",
            "function": "unregister_shrinker",
            "description": "mm vmscan remove one",
            "id": "unregister_shrinker"
        },
        "huge_pmd_unshare": {
            "ground_truth": "0",
            "function": "huge_pmd_unshare",
            "description": "include linux hugetlb unmap huge page backed by shared pte hugetlb pte page is ref counted at the time of mapping if pte is shared indicated by page count unmap is achieved by clearing pud and decrementing the ref count if count the pte page is not shared called with page table lock held and mmap rwsem held in write mode return successfully unmapped shared pte page the underlying pte page is not shared or it is the last user",
            "id": "huge_pmd_unshare"
        },
        "slab_unmergeable": {
            "ground_truth": "0",
            "function": "slab_unmergeable",
            "description": "mm slab common find mergeable slab cache",
            "id": "slab_unmergeable"
        },
        "start_scan_thread": {
            "ground_truth": "0",
            "function": "start_scan_thread",
            "description": "mm kmemleak start the automatic memory scanning thread this function must be called with the scan mutex held",
            "id": "start_scan_thread"
        },
        "pcpu_update_empty_pages": {
            "ground_truth": "0",
            "function": "pcpu_update_empty_pages",
            "description": "mm percpu pcpu update empty page update empty page counter this is used to keep track of the empty page now based on the premise md block cover page the hint update function recognize if block is made full or broken to calculate delta for keeping track of free page chunk chunk of interest nr nr of empty page",
            "id": "pcpu_update_empty_pages"
        },
        "local_memory_node": {
            "ground_truth": "0",
            "function": "local_memory_node",
            "description": "include linux mmzone return node id of node used for local allocation first node id of first zone in arg node is generic zonelist used for initializing percpu numa mem which is used primarily for kernel allocation so use gfp kernel flag to locate zonelist",
            "id": "local_memory_node"
        },
        "clear_page_mlock": {
            "ground_truth": "0",
            "function": "clear_page_mlock",
            "description": "mm internal clear the page is pagemlocked this can be useful in situation where we want to unconditionally remove page from the pagecache on truncation or freeing it is legal to call this function for any page mlocked or not if called for page that is still mapped by mlocked vmas all we do is revert to lazy lru behaviour semantics are not broken",
            "id": "clear_page_mlock"
        },
        "account_page_cleaned": {
            "ground_truth": "0",
            "function": "account_page_cleaned",
            "description": "mm page writeback helper function for deaccounting dirty page without writeback caller must hold lock page memcg",
            "id": "account_page_cleaned"
        },
        "update_lru_sizes": {
            "ground_truth": "0",
            "function": "update_lru_sizes",
            "description": "mm vmscan update lru size after isolating page the lru size update must be complete before mem cgroup update lru size due to sanity check",
            "id": "update_lru_sizes"
        },
        "find_vma": {
            "ground_truth": "0",
            "function": "find_vma",
            "description": "look up the first vma in which addr resides null if none should be called with mm mmap lock at least held readlocked",
            "id": "find_vma"
        },
        "init_cache_node_node": {
            "ground_truth": "0",
            "function": "init_cache_node_node",
            "description": "mm slab allocates and initializes node for node on each slab cache used for either memory or cpu hotplug if memory is being hot added the kmem cache node will be allocated off node since memory is not yet online for the new node when hotplugging memory or cpu existing node are not replaced if already in use must hold slab mutex",
            "id": "init_cache_node_node"
        },
        "swap_duplicate": {
            "ground_truth": "0",
            "function": "swap_duplicate",
            "description": "include linux swap increase reference count of swap entry by return for success or enomem if swap count continuation is required but could not be atomically allocated return just a if it succeeded if swap duplicate fails for another reason einval or enoent which might occur if page table entry ha got corrupted",
            "id": "swap_duplicate"
        },
        "try_to_unmap_one": {
            "ground_truth": "0",
            "function": "try_to_unmap_one",
            "description": "mm rmap arg enum ttu flag will be passed to this argument",
            "id": "try_to_unmap_one"
        },
        "prepare_kswapd_sleep": {
            "ground_truth": "0",
            "function": "prepare_kswapd_sleep",
            "description": "mm vmscan prepare kswapd for sleeping this verifies that there are no process waiting in throttle direct reclaim and that watermark have been met return true if kswapd is ready to sleep",
            "id": "prepare_kswapd_sleep"
        },
        "action_result": {
            "ground_truth": "0",
            "function": "action_result",
            "description": "mm memory failure dirty clean indication is not accurate due to the possibility of setting pg dirty outside page lock see also comment above set page dirty",
            "id": "action_result"
        },
        "setup_object_debug": {
            "ground_truth": "0",
            "function": "setup_object_debug",
            "description": "mm slub object debug check for alloc free path",
            "id": "setup_object_debug"
        },
        "extfrag_show": {
            "ground_truth": "0",
            "function": "extfrag_show",
            "description": "mm vmstat display fragmentation index for order that allocation would fail for",
            "id": "extfrag_show"
        },
        "change_prot_numa": {
            "ground_truth": "0",
            "function": "change_prot_numa",
            "description": "this is used to mark range of virtual address to be inaccessible these are later cleared by numa hinting fault depending on these fault page may be migrated for better numa placement this is assuming that numa fault are handled using prot none if an architecture make different choice it will need further change to the core",
            "id": "change_prot_numa"
        },
        "hex_dump_object": {
            "ground_truth": "0",
            "function": "hex_dump_object",
            "description": "mm kmemleak printing of the object hex dump to the seq file the number of line to be printed is limited to hex max line to prevent seq file spamming the actual number of printed byte depends on hex row size it must be called with the object lock held",
            "id": "hex_dump_object"
        },
        "flowctrl": {
            "ground_truth": "0",
            "function": "flowctrl",
            "description": "struct caif dev common data shared between caif driver and stack supplied by caif core stack and is used by caif link layer to send flow stop to caif core the flow information will be distributed to all client of caif low latency this member is set by caif link layer device in order to indicate if this device is high bandwidth or low latency device is set by caif link layer in order to indicate if the interface receives fragmented frame that must be assembled by caif core layer is set if the physical interface is using frame checksum on the caif frame is set if the caif link layer expects caif frame to start with the stx byte this structure is shared between the caif driver and the caif stack it is used by the device to register it behavior caif core layer must set the member flowctrl in order to supply caif link layer with the flow control function flowctrl flow control callback function this function is link select profile of device either high bandwidth or use frag caif frame may be fragmented use fcs indicate if frame checksum fcs is used use stx indicate start of frame extension stx in use",
            "id": "flowctrl"
        },
        "freeque": {
            "ground_truth": "0",
            "function": "freeque",
            "description": "freeque wake up waiter on the sender and receiver waiting queue remove the message queue from message queue id idr and clean up all the message associated with this queue msg id rwsem writer and the spinlock for this message queue are held before freeque is called msg id rwsem remains locked on exit",
            "id": "freeque"
        },
        "__pagevec_lru_add": {
            "ground_truth": "0",
            "function": "__pagevec_lru_add",
            "description": "mm swap add the passed page to the lru then drop the caller is refcount on them reinitialises the caller is pagevec",
            "id": "__pagevec_lru_add"
        },
        "alternate_node_alloc": {
            "ground_truth": "0",
            "function": "alternate_node_alloc",
            "description": "mm slab try allocating on another node if pfa spread slab is mempolicy is set if we are in interrupt then process context including cpusets and mempolicy may not apply and should not be used for allocation policy",
            "id": "alternate_node_alloc"
        },
        "size_to_chunks": {
            "ground_truth": "0",
            "function": "size_to_chunks",
            "description": "mm z3fold convert an allocation size in byte to size in zbud chunk",
            "id": "size_to_chunks"
        },
        "sparse_init": {
            "ground_truth": "0",
            "function": "sparse_init",
            "description": "mm sparse allocate the accumulated non linear section allocate mem map for each and record the physical to section mapping",
            "id": "sparse_init"
        },
        "get_zspage_inuse": {
            "ground_truth": "0",
            "function": "get_zspage_inuse",
            "description": "mm zsmalloc protected by class lock",
            "id": "get_zspage_inuse"
        },
        "__mpol_put": {
            "ground_truth": "0",
            "function": "__mpol_put",
            "description": "slow path of mpol destructor",
            "id": "__mpol_put"
        },
        "stable_tree_search": {
            "ground_truth": "0",
            "function": "stable_tree_search",
            "description": "mm ksm stable tree search search for page inside the stable tree this function check if there is page inside the stable tree with identical content to the page that we are scanning right now this function return the stable tree node of identical content if found null otherwise",
            "id": "stable_tree_search"
        },
        "cache_estimate": {
            "ground_truth": "0",
            "function": "cache_estimate",
            "description": "mm slab calculate the number of object and left over byte for given buffer size",
            "id": "cache_estimate"
        },
        "lock_anon_vma_root": {
            "ground_truth": "0",
            "function": "lock_anon_vma_root",
            "description": "mm rmap this is useful helper function for locking the anon vma root a we traverse the vma anon vma chain looping over anon vma is that have the same vma such anon vma is should have the same root so you would expect to see just single mutex lock for the whole traversal",
            "id": "lock_anon_vma_root"
        },
        "alloc_vmap_area": {
            "ground_truth": "0",
            "function": "alloc_vmap_area",
            "description": "mm vmalloc allocate region of kva of the specified size and alignment within the vstart and vend",
            "id": "alloc_vmap_area"
        },
        "hugetlb_cgroup_migrate": {
            "ground_truth": "0",
            "function": "hugetlb_cgroup_migrate",
            "description": "include linux hugetlb cgroup hugetlb lock will make sure parallel cgroup rmdir will not happen when we migrate hugepages",
            "id": "hugetlb_cgroup_migrate"
        },
        "pcpu_stats_area_dealloc": {
            "ground_truth": "0",
            "function": "pcpu_stats_area_dealloc",
            "description": "mm percpu internal pcpu stats area dealloc decrement allocation stats context pcpu lock chunk the location of the area being deallocated",
            "id": "pcpu_stats_area_dealloc"
        },
        "__check_object_size": {
            "ground_truth": "0",
            "function": "__check_object_size",
            "description": "mm usercopy validates that the given object is not bogus address fully contained by stack or stack frame when available fully within slab object or object whitelist area when available not in kernel text",
            "id": "__check_object_size"
        },
        "truncate_exceptional_pvec_entries": {
            "ground_truth": "0",
            "function": "truncate_exceptional_pvec_entries",
            "description": "mm truncate unconditionally remove exceptional entry usually called from truncate path note that the pagevec may be altered by this function by removing exceptional entry similar to what pagevec remove exceptionals doe",
            "id": "truncate_exceptional_pvec_entries"
        },
        "do_mprotect_pkey": {
            "ground_truth": "0",
            "function": "do_mprotect_pkey",
            "description": "pkey when doing legacy mprotect",
            "id": "do_mprotect_pkey"
        },
        "can_follow_write_pmd": {
            "ground_truth": "0",
            "function": "can_follow_write_pmd",
            "description": "mm huge memory foll force can write to even unwritable pmd is but only after we have gone through cow cycle and they are dirty",
            "id": "can_follow_write_pmd"
        },
        "__frontswap_invalidate_page": {
            "ground_truth": "0",
            "function": "__frontswap_invalidate_page",
            "description": "mm frontswap invalidate any data from frontswap associated with the specified swaptype and offset so that subsequent get will fail",
            "id": "__frontswap_invalidate_page"
        },
        "memory_present": {
            "ground_truth": "0",
            "function": "memory_present",
            "description": "mm sparse record memory area against node",
            "id": "memory_present"
        },
        "wq_add": {
            "ground_truth": "0",
            "function": "wq_add",
            "description": "add current to info wait sr before element with smaller prio",
            "id": "wq_add"
        },
        "complexmode_tryleave": {
            "ground_truth": "0",
            "function": "complexmode_tryleave",
            "description": "try to leave the mode that disallows simple operation caller must own sem perm lock",
            "id": "complexmode_tryleave"
        },
        "slab_dead_cpu": {
            "ground_truth": "0",
            "function": "slab_dead_cpu",
            "description": "mm slab this is called for failed online attempt and for successful offline even if all the cpu of node are down we do not free the kmem cache node of any cache this to avoid race between cpu down and kmalloc allocation from another cpu for memory from the node of the cpu going down the kmem cache node structure is usually allocated from kmem cache create and get destroyed at kmem cache destroy",
            "id": "slab_dead_cpu"
        },
        "should_reclaim_retry": {
            "ground_truth": "0",
            "function": "should_reclaim_retry",
            "description": "mm page alloc check whether it make sense to retry the reclaim to make forward progress for the given allocation request we give up when we either have tried max reclaim retries in row without success or when we could not even meet the watermark if we reclaimed all remaining page on the lru list return true if retry is viable or false to enter the oom path",
            "id": "should_reclaim_retry"
        },
        "vma_replace_policy": {
            "ground_truth": "0",
            "function": "vma_replace_policy",
            "description": "apply policy to single vma this must be called with the mmap lock held for writing",
            "id": "vma_replace_policy"
        },
        "unmap_ref_private": {
            "ground_truth": "0",
            "function": "unmap_ref_private",
            "description": "mm hugetlb this is called when the original mapper is failing to cow map private mappping it owns the reserve page for the intention is to unmap the page from other vmas and let the child be sigkilled if they are faulting the same region",
            "id": "unmap_ref_private"
        },
        "next_freelist_entry": {
            "ground_truth": "0",
            "function": "next_freelist_entry",
            "description": "mm slub get the next entry on the pre computed freelist randomized",
            "id": "next_freelist_entry"
        },
        "wakeup_kswapd": {
            "ground_truth": "0",
            "function": "wakeup_kswapd",
            "description": "mm vmscan zone is low on free memory or too fragmented for high order memory if kswapd should reclaim direct reclaim is deferred wake it up for the zone is pgdat it will wake up kcompactd after reclaiming memory if kswapd reclaim ha failed or is not needed still wake up kcompactd if only compaction is needed",
            "id": "wakeup_kswapd"
        },
        "hugetlb_cgroup_css_offline": {
            "ground_truth": "0",
            "function": "hugetlb_cgroup_css_offline",
            "description": "mm hugetlb cgroup force the hugetlb cgroup to empty the hugetlb resource by moving them to the parent cgroup",
            "id": "hugetlb_cgroup_css_offline"
        },
        "PageHeadHuge": {
            "ground_truth": "0",
            "function": "PageHeadHuge",
            "description": "mm hugetlb pageheadhuge only return true for hugetlbfs head page but not for normal or transparent huge page",
            "id": "PageHeadHuge"
        },
        "pcpu_stats_save_ai": {
            "ground_truth": "0",
            "function": "pcpu_stats_save_ai",
            "description": "mm percpu internal for debug purpose we do not care about the flexible array",
            "id": "pcpu_stats_save_ai"
        },
        "mem_cgroup_shrink_node": {
            "ground_truth": "0",
            "function": "mem_cgroup_shrink_node",
            "description": "mm vmscan only used by soft limit reclaim do not reuse for anything else",
            "id": "mem_cgroup_shrink_node"
        },
        "show_free_areas": {
            "ground_truth": "0",
            "function": "show_free_areas",
            "description": "mm page alloc show free area list used inside shift scroll lock stuff we also calculate the percentage fragmentation we do this by counting the memory on each free list with the exception of the first item on the list bit in filter show mem filter node suppress node that are not allowed by current is cpuset",
            "id": "show_free_areas"
        },
        "round_hint_to_min": {
            "ground_truth": "0",
            "function": "round_hint_to_min",
            "description": "if hint addr is le than mmap min addr change hint to be a low a possible but still greater than mmap min addr",
            "id": "round_hint_to_min"
        },
        "init_zbud_page": {
            "ground_truth": "0",
            "function": "init_zbud_page",
            "description": "mm zbud initializes the zbud header of newly allocated zbud page",
            "id": "init_zbud_page"
        },
        "walk_page_range_novma": {
            "ground_truth": "0",
            "function": "walk_page_range_novma",
            "description": "mm pagewalk similar to walk page range but can walk any page table even if they are not backed by vmas because unusual entry may be walked this function will also not lock the ptes for the pte entry callback this is useful for walking the kernel page table or page table for firmware",
            "id": "walk_page_range_novma"
        },
        "free_swap_and_cache": {
            "ground_truth": "0",
            "function": "free_swap_and_cache",
            "description": "free the swap entry like above but also try to free the page cache entry if it is the last user",
            "id": "free_swap_and_cache"
        },
        "anon_vma_interval_tree_pre_update_vma": {
            "ground_truth": "0",
            "function": "anon_vma_interval_tree_pre_update_vma",
            "description": "vma ha some anon vma assigned and is already inserted on that anon vma is interval tree before updating the vma is vm start vm end vm pgoff field the vma must be removed from the anon vma is interval tree using anon vma interval tree pre update vma after the update the vma will be reinserted using anon vma interval tree post update vma the entire update must be protected by exclusive mmap lock and by the root anon vma is mutex",
            "id": "anon_vma_interval_tree_pre_update_vma"
        },
        "try_to_unuse": {
            "ground_truth": "0",
            "function": "try_to_unuse",
            "description": "if the boolean frontswap is true only unuse page to unuse page page to unuse mean all page ignored if frontswap is false",
            "id": "try_to_unuse"
        },
        "get_pkmap_entries_count": {
            "ground_truth": "0",
            "function": "get_pkmap_entries_count",
            "description": "arch xtensa include asm highmem get the number of pkmap entry of the given color if no free slot is found after checking that many entry kmap will sleep waiting for someone to call kunmap and free pkmap slot",
            "id": "get_pkmap_entries_count"
        },
        "force_page_cache_ra": {
            "ground_truth": "0",
            "function": "force_page_cache_ra",
            "description": "chunk the readahead into megabyte unit so that we do not pin too much memory at once",
            "id": "force_page_cache_ra"
        },
        "rmap_walk_locked": {
            "ground_truth": "0",
            "function": "rmap_walk_locked",
            "description": "mm rmap like rmap walk but caller hold relevant rmap lock",
            "id": "rmap_walk_locked"
        },
        "do_sync_mmap_readahead": {
            "ground_truth": "0",
            "function": "do_sync_mmap_readahead",
            "description": "mm filemap synchronous readahead happens when we do not even find page in the page cache at all we do not want to perform io under the mmap sem so if we have to drop the mmap sem we return the file that wa pinned in order for u to do that if we did not pin file then we return null the file that is returned need to be fput ed when we are done with it",
            "id": "do_sync_mmap_readahead"
        },
        "mmap_init": {
            "ground_truth": "0",
            "function": "mmap_init",
            "description": "initialise the percpu counter for vm and region record slab",
            "id": "mmap_init"
        },
        "free_swap_count_continuations": {
            "ground_truth": "0",
            "function": "free_swap_count_continuations",
            "description": "free swap count continuation swapoff free all the continuation page appended to the swap map after swap map is quiesced before vfree ing it",
            "id": "free_swap_count_continuations"
        },
        "init_user_reserve": {
            "ground_truth": "0",
            "function": "init_user_reserve",
            "description": "initialise sysctl user reserve kbytes this is intended to prevent user from starting single memory hogging process such that they cannot recover kill the hog in overcommit never mode the default value is min of free memory 128mb 128mb is enough to recover with sshd login bash and top kill",
            "id": "init_user_reserve"
        },
        "repair_env_string": {
            "ground_truth": "0",
            "function": "repair_env_string",
            "description": "init main change nul term back to to make param the whole string",
            "id": "repair_env_string"
        },
        "migrate_prep_local": {
            "ground_truth": "0",
            "function": "migrate_prep_local",
            "description": "include linux migrate do the necessary work of migrate prep but not if it involves other cpu",
            "id": "migrate_prep_local"
        },
        "quarantine_remove_cache": {
            "ground_truth": "0",
            "function": "quarantine_remove_cache",
            "description": "mm kasan quarantine free all quarantined object belonging to cache",
            "id": "quarantine_remove_cache"
        },
        "altera_cvp_v2_clear_state": {
            "ground_truth": "0",
            "function": "altera_cvp_v2_clear_state",
            "description": "driver fpga altera cvp cvp version2 function recent intel fpgas use credit mechanism to throttle incoming bitstreams and different method of clearing the state",
            "id": "altera_cvp_v2_clear_state"
        },
        "fault_dirty_shared_page": {
            "ground_truth": "0",
            "function": "fault_dirty_shared_page",
            "description": "mm memory handle dirtying of page in shared file mapping on write fault the function expects the page to be locked and unlocks it",
            "id": "fault_dirty_shared_page"
        },
        "kmemleak_clear": {
            "ground_truth": "0",
            "function": "kmemleak_clear",
            "description": "mm kmemleak we use grey instead of black to ensure we can do future scan on the same object if we did not do future scan these black object could potentially contain reference to newly allocated object in the future and we would end up with false positive",
            "id": "kmemleak_clear"
        },
        "default_kernel_zone_for_pfn": {
            "ground_truth": "0",
            "function": "default_kernel_zone_for_pfn",
            "description": "mm memory hotplug return default kernel memory zone for the given pfn range if no kernel zone cover this pfn range it will automatically go to the zone normal",
            "id": "default_kernel_zone_for_pfn"
        },
        "walk_zones_in_node": {
            "ground_truth": "0",
            "function": "walk_zones_in_node",
            "description": "mm vmstat walk zone in node and print using callback if assert populated is true only use callback for zone that are populated",
            "id": "walk_zones_in_node"
        },
        "tg3_msi_1shot": {
            "ground_truth": "0",
            "function": "tg3_msi_1shot",
            "description": "one shot msi handler chip automatically disables interrupt after sending msi so driver doe not have to do it",
            "id": "tg3_msi_1shot"
        },
        "PageHugeTemporary": {
            "ground_truth": "0",
            "function": "PageHugeTemporary",
            "description": "mm hugetlb internal hugetlb specific page flag do not use outside of the hugetlb code",
            "id": "PageHugeTemporary"
        },
        "br_mrp_in_frame": {
            "ground_truth": "1",
            "function": "br_mrp_in_frame",
            "description": "net bridge br mrp determin if the frame type is an interconnect frame",
            "id": "br_mrp_in_frame"
        },
        "vnt_get_next_tbtt": {
            "ground_truth": "0",
            "function": "vnt_get_next_tbtt",
            "description": "driver staging vt6656 card description read nic tsf counter get nexttbtt from adjusted tsf and beacon interval parameter in tsf current tsf counter beacon interval beacon interval out tsf current tsf counter return value tsf value of next beacon",
            "id": "vnt_get_next_tbtt"
        },
        "shmem_punch_compound": {
            "ground_truth": "0",
            "function": "shmem_punch_compound",
            "description": "mm shmem check whether hole punch or truncation need to split huge page returning true if no split wa required or the split ha been successful eviction or truncation to size should never need to split huge page but in rare case might do so if shmem undo range failed to trylock on head and then succeeded to trylock on tail split can only succeed when there are no additional reference on the huge page so the split below relies upon find get entry having stopped when it found subpage of the huge page without getting further reference",
            "id": "shmem_punch_compound"
        },
        "section_mark_present": {
            "ground_truth": "0",
            "function": "section_mark_present",
            "description": "mm sparse there are number of time that we loop over nr mem section looking for section present on each but when we have very large physical address space nr mem section can also be very large which make the loop quite long keeping track of this give u an easy way to break out of those loop early",
            "id": "section_mark_present"
        },
        "lookup_object": {
            "ground_truth": "0",
            "function": "lookup_object",
            "description": "lib debugobjects look up memory block metadata kmemleak object in the object search tree based on pointer value if alias is only value pointing to the beginning of the memory block are allowed the kmemleak lock must be held when calling this function",
            "id": "lookup_object"
        },
        "find_biggest_section_pfn": {
            "ground_truth": "0",
            "function": "find_biggest_section_pfn",
            "description": "mm memory hotplug find the biggest valid pfn in the range start pfn end pfn",
            "id": "find_biggest_section_pfn"
        },
        "do_page_add_anon_rmap": {
            "ground_truth": "0",
            "function": "do_page_add_anon_rmap",
            "description": "mm rmap special version of the above for do swap page which often run into page that are exclusively owned by the current process everybody else should continue to use page add anon rmap above",
            "id": "do_page_add_anon_rmap"
        },
        "dirty_writeback_centisecs_handler": {
            "ground_truth": "0",
            "function": "dirty_writeback_centisecs_handler",
            "description": "mm page writeback sysctl handler for proc sys vm dirty writeback centisecs",
            "id": "dirty_writeback_centisecs_handler"
        },
        "init_zswap": {
            "ground_truth": "0",
            "function": "init_zswap",
            "description": "mm zswap module init and exit",
            "id": "init_zswap"
        },
        "vma_wants_writenotify": {
            "ground_truth": "0",
            "function": "vma_wants_writenotify",
            "description": "some shared mapping will want the page marked read only to track write event if so we will downgrade vm page prot to the private version using protection map without the vm shared bit",
            "id": "vma_wants_writenotify"
        },
        "remove_migration_pte": {
            "ground_truth": "0",
            "function": "remove_migration_pte",
            "description": "restore potential migration pte to working pte entry",
            "id": "remove_migration_pte"
        },
        "munmap_vma_range": {
            "ground_truth": "0",
            "function": "munmap_vma_range",
            "description": "munmap vma range munmap vmas that overlap range find all the vm area struct that overlap from start to mm the mm struct start the start of the range len the length of the range pprev pointer to the pointer that will be set to previous vm area struct rb link the rb node rb parent the parent rb node end and munmap them set pprev to the previous vm area struct return enomem on munmap failure or on success",
            "id": "munmap_vma_range"
        },
        "zone_statistics": {
            "ground_truth": "0",
            "function": "zone_statistics",
            "description": "mm page alloc update numa hit miss statistic must be called with interrupt disabled",
            "id": "zone_statistics"
        },
        "get_next_ra_size": {
            "ground_truth": "0",
            "function": "get_next_ra_size",
            "description": "get the previous window size ramp it up and return it a the new window size",
            "id": "get_next_ra_size"
        },
        "__do_munmap": {
            "ground_truth": "0",
            "function": "__do_munmap",
            "description": "munmap is split into main part this part which find what need doing and the area themselves which do the work this now handle partial unmappings jeremy fitzhardinge",
            "id": "__do_munmap"
        },
        "is_dump_unreclaim_slabs": {
            "ground_truth": "0",
            "function": "is_dump_unreclaim_slabs",
            "description": "mm oom kill print out unreclaimble slab info when unreclaimable slab amount is greater than all user memory lru page",
            "id": "is_dump_unreclaim_slabs"
        },
        "delete_vma_from_mm": {
            "ground_truth": "0",
            "function": "delete_vma_from_mm",
            "description": "delete vma from it owning mm struct and address space",
            "id": "delete_vma_from_mm"
        },
        "__zswap_pool_current": {
            "ground_truth": "0",
            "function": "__zswap_pool_current",
            "description": "mm zswap pool function",
            "id": "__zswap_pool_current"
        },
        "pagetypeinfo_showmixedcount": {
            "ground_truth": "0",
            "function": "pagetypeinfo_showmixedcount",
            "description": "mm vmstat print out the number of pageblocks for each migratetype that contain page of other type this give an indication of how well fallback are being contained by rmqueue fallback it requires information from page owner to determine what is going on",
            "id": "pagetypeinfo_showmixedcount"
        },
        "kernel_move_pages": {
            "ground_truth": "0",
            "function": "kernel_move_pages",
            "description": "move list of page in the address space of the currently executing process",
            "id": "kernel_move_pages"
        },
        "update_refs": {
            "ground_truth": "0",
            "function": "update_refs",
            "description": "mm kmemleak update an object is reference object lock must be held by the caller",
            "id": "update_refs"
        },
        "select_bad_process": {
            "ground_truth": "0",
            "function": "select_bad_process",
            "description": "mm oom kill simple selection loop we choose the process with the highest number of point in case scan wa aborted oc chosen is set to",
            "id": "select_bad_process"
        },
        "unlink_file_vma": {
            "ground_truth": "0",
            "function": "unlink_file_vma",
            "description": "unlink file based vm structure from it interval tree to hide vma from rmap and vmtruncate before freeing it page table",
            "id": "unlink_file_vma"
        },
        "cvmx_spi_clock_detect_cb": {
            "ground_truth": "0",
            "function": "cvmx_spi_clock_detect_cb",
            "description": "arch mips cavium octeon executive cvmx spi return zero on success non zero error code on failure will cause spi initialization to abort int interface the identifier of the packet interface to configure and use a spi interface cvmx spi mode mode the operating mode for the spi interface the interface can operate a full duplex both tx and rx data path active or a halfplex either the tx data path is active or the rx data path is active but not both int timeout timeout to wait for clock synchronization in second",
            "id": "cvmx_spi_clock_detect_cb"
        },
        "__asan_allocas_unpoison": {
            "ground_truth": "0",
            "function": "__asan_allocas_unpoison",
            "description": "mm kasan generic emitted by compiler to unpoison alloca ed area when the stack unwinds",
            "id": "__asan_allocas_unpoison"
        },
        "slabs_destroy": {
            "ground_truth": "0",
            "function": "slabs_destroy",
            "description": "mm slab update the size of the cache before calling slab destroy a it may recursively call kfree",
            "id": "slabs_destroy"
        },
        "check_sync_rss_stat": {
            "ground_truth": "0",
            "function": "check_sync_rss_stat",
            "description": "mm memory sync counter once per page fault",
            "id": "check_sync_rss_stat"
        },
        "get_kernel_pages": {
            "ground_truth": "0",
            "function": "get_kernel_pages",
            "description": "mm swap get kernel page pin kernel page in memory should be at least nr segs long kiov an array of struct kvec structure nr segs number of segment to pin write pinning for read write currently ignored page array that receives pointer to the page pinned return number of page pinned this may be fewer than the number requested if nr page is or negative return if no page were pinned return errno each page returned must be released with put page call when it is finished with",
            "id": "get_kernel_pages"
        },
        "try_to_merge_with_ksm_page": {
            "ground_truth": "0",
            "function": "try_to_merge_with_ksm_page",
            "description": "mm ksm try to merge with ksm page like try to merge two page but no new kernel page is allocated kpage must already be ksm page this function return if the page were merged efault otherwise",
            "id": "try_to_merge_with_ksm_page"
        },
        "kswapd_stop": {
            "ground_truth": "0",
            "function": "kswapd_stop",
            "description": "mm vmscan called by memory hotplug when all memory in node is offlined caller must hold mem hotplug begin end",
            "id": "kswapd_stop"
        },
        "__get_user_pages_locked": {
            "ground_truth": "0",
            "function": "__get_user_pages_locked",
            "description": "mm gup please note that this function unlike get user page will not return for nr page without foll nowait",
            "id": "__get_user_pages_locked"
        },
        "kmemleak_init": {
            "ground_truth": "0",
            "function": "kmemleak_init",
            "description": "include linux kmemleak kmemleak initialization",
            "id": "kmemleak_init"
        },
        "__cancel_dirty_page": {
            "ground_truth": "0",
            "function": "__cancel_dirty_page",
            "description": "mm page writeback this cancel just the dirty bit on the kernel page itself it doe not actually remove dirty bit on any mmap is that may be around it also leaf the page tagged dirty so any sync activity will still find it on the dirty list and in particular clear page dirty for io will still look at the dirty bit in the vm doing this should normally only ever be done when page is truncated and is not actually mapped anywhere at all however f buffer doe this when it notice that somebody ha cleaned out all the buffer on page without actually doing it through the vm can you say ext3 is horribly ugly thought you could",
            "id": "__cancel_dirty_page"
        },
        "shuffle_freelist": {
            "ground_truth": "0",
            "function": "shuffle_freelist",
            "description": "mm slub shuffle the freelist initialization state based on pre computed list return true if the list wa successfully shuffled false otherwise",
            "id": "shuffle_freelist"
        },
        "shrink_zones": {
            "ground_truth": "0",
            "function": "shrink_zones",
            "description": "mm vmscan this is the direct reclaim path for page allocating process we only try to reclaim page from zone which will satisfy the caller is allocation request if zone is deemed to be full of pinned page then just give it light scan then give up on it",
            "id": "shrink_zones"
        },
        "node_pagecache_reclaimable": {
            "ground_truth": "0",
            "function": "node_pagecache_reclaimable",
            "description": "mm vmscan work out how many page cache page we can reclaim in this reclaim mode",
            "id": "node_pagecache_reclaimable"
        },
        "mark_page_accessed": {
            "ground_truth": "0",
            "function": "mark_page_accessed",
            "description": "mm swap mark page a having seen activity inactive unreferenced inactive referenced inactive referenced active unreferenced active unreferenced active referenced when newly allocated page is not yet visible so safe for non atomic ops setpagereferenced page may be substituted for mark page accessed page",
            "id": "mark_page_accessed"
        },
        "freelist_ptr": {
            "ground_truth": "0",
            "function": "freelist_ptr",
            "description": "mm slub return freelist pointer ptr with hardening this is obfuscated with an xor of the address where the pointer is held and per cache random number",
            "id": "freelist_ptr"
        },
        "drain_mmlist": {
            "ground_truth": "0",
            "function": "drain_mmlist",
            "description": "after successful try to unuse if no swap is now in use we know we can empty the mmlist swap lock must be held on entry and exit note that mmlist lock nest inside swap lock and an mm must be added to the mmlist just after page duplicate before would be racy",
            "id": "drain_mmlist"
        },
        "tomoyo_get_group": {
            "ground_truth": "0",
            "function": "tomoyo_get_group",
            "description": "security tomoyo memory allocate memory for struct tomoyo path group struct tomoyo number group return pointer to struct tomoyo group on success null otherwise struct tomoyo acl param param pointer to struct tomoyo acl param const u8 idx index number",
            "id": "tomoyo_get_group"
        },
        "reusable_anon_vma": {
            "ground_truth": "0",
            "function": "reusable_anon_vma",
            "description": "do some basic sanity checking to see if we can re use the anon vma from old the vma is are in vm order one of them will be the same a old the other will be the new one that is trying to share the anon vma note this run with mm sem held for reading so it is possible that the anon vma of old is concurrently in the process of being set up by another page fault trying to merge that but that is ok if it is being set up that automatically mean that it will be singleton acceptable for merging so we can do all of this optimistically but we do that read once to make sure that we never re load the pointer iow that the list is singular test on the anon vma chain only matter for the istable anon vma case ie the thing we want to avoid is to return an anon vma that is complex due to having gone through fork we also make sure that the two vma is are compatible adjacent and with the same memory policy that is all stable even with just read lock on the mm sem",
            "id": "reusable_anon_vma"
        },
        "get_pkmap_color": {
            "ground_truth": "0",
            "function": "get_pkmap_color",
            "description": "arch xtensa include asm highmem determine color of virtual address where the page should be mapped",
            "id": "get_pkmap_color"
        },
        "alloc_page_interleave": {
            "ground_truth": "0",
            "function": "alloc_page_interleave",
            "description": "allocate page in interleaved policy own path because it need to do special accounting",
            "id": "alloc_page_interleave"
        },
        "register_event": {
            "ground_truth": "0",
            "function": "register_event",
            "description": "register event callback will be used to add new userspace waiter for change related to this event use eventfd signal on eventfd to send notification to userspace",
            "id": "register_event"
        },
        "next_random_slot": {
            "ground_truth": "0",
            "function": "next_random_slot",
            "description": "mm slab get the next entry on the list and randomize it using random shift",
            "id": "next_random_slot"
        },
        "free_swap_cache": {
            "ground_truth": "0",
            "function": "free_swap_cache",
            "description": "mm swap state if we are the only user then try to free up the swap cache it ok to check for pageswapcache without the page lock here because we are going to recheck again inside try to free swap with the lock marcelo",
            "id": "free_swap_cache"
        },
        "generic_file_readonly_mmap": {
            "ground_truth": "0",
            "function": "generic_file_readonly_mmap",
            "description": "mm filemap this is for filesystems which do not implement writepage",
            "id": "generic_file_readonly_mmap"
        },
        "generic_error_remove_page": {
            "ground_truth": "0",
            "function": "generic_error_remove_page",
            "description": "mm truncate used to get rid of page on hardware memory corruption",
            "id": "generic_error_remove_page"
        },
        "detach_vmas_to_be_unmapped": {
            "ground_truth": "0",
            "function": "detach_vmas_to_be_unmapped",
            "description": "create list of vma is touched by the unmap removing them from the mm is vma list a we go",
            "id": "detach_vmas_to_be_unmapped"
        },
        "check_panic_on_oom": {
            "ground_truth": "0",
            "function": "check_panic_on_oom",
            "description": "mm oom kill determines whether the kernel must panic because of the panic on oom sysctl",
            "id": "check_panic_on_oom"
        },
        "do_early_param": {
            "ground_truth": "0",
            "function": "do_early_param",
            "description": "init main check for early params",
            "id": "do_early_param"
        },
        "pcpu_set_page_chunk": {
            "ground_truth": "0",
            "function": "pcpu_set_page_chunk",
            "description": "mm percpu set the pointer to chunk in page struct",
            "id": "pcpu_set_page_chunk"
        },
        "inc_cluster_info_page": {
            "ground_truth": "0",
            "function": "inc_cluster_info_page",
            "description": "the cluster corresponding to page nr will be used the cluster will be removed from free cluster list and it usage counter will be increased",
            "id": "inc_cluster_info_page"
        },
        "delete_vma": {
            "ground_truth": "0",
            "function": "delete_vma",
            "description": "destroy vma record",
            "id": "delete_vma"
        },
        "kswapd_shrink_node": {
            "ground_truth": "0",
            "function": "kswapd_shrink_node",
            "description": "mm vmscan kswapd shrink node of page that are at or below the highest usable zone that is currently unbalanced return true if kswapd scanned at least the requested number of page to reclaim or if the lack of progress wa due to page under writeback this is used to determine if the scanning priority need to be raised",
            "id": "kswapd_shrink_node"
        },
        "kill_proc": {
            "ground_truth": "0",
            "function": "kill_proc",
            "description": "mm memory failure send all the process who have the page mapped signal action optional if they are not immediately affected by the error action required if error happened in current execution context",
            "id": "kill_proc"
        },
        "__purge_vmap_area_lazy": {
            "ground_truth": "0",
            "function": "__purge_vmap_area_lazy",
            "description": "mm vmalloc purge all lazily freed vmap area",
            "id": "__purge_vmap_area_lazy"
        },
        "interleave_nodes": {
            "ground_truth": "0",
            "function": "interleave_nodes",
            "description": "do dynamic interleaving for process",
            "id": "interleave_nodes"
        },
        "reserve_highatomic_pageblock": {
            "ground_truth": "0",
            "function": "reserve_highatomic_pageblock",
            "description": "mm page alloc reserve pageblock for exclusive use of high order atomic allocation if there are no empty page block that contain page with suitable order",
            "id": "reserve_highatomic_pageblock"
        },
        "writeout": {
            "ground_truth": "0",
            "function": "writeout",
            "description": "writeback page to clean the dirty state",
            "id": "writeout"
        },
        "zswap_frontswap_store": {
            "ground_truth": "0",
            "function": "zswap_frontswap_store",
            "description": "mm zswap frontswap hook",
            "id": "zswap_frontswap_store"
        },
        "boot_loader_read_mem32": {
            "ground_truth": "0",
            "function": "boot_loader_read_mem32",
            "description": "sound pci asihpi hpi6205 bootloader utility function",
            "id": "boot_loader_read_mem32"
        },
        "get_page_from_freelist": {
            "ground_truth": "0",
            "function": "get_page_from_freelist",
            "description": "mm page alloc get page from freelist go through the zonelist trying to allocate page",
            "id": "get_page_from_freelist"
        },
        "unusable_free_index": {
            "ground_truth": "0",
            "function": "unusable_free_index",
            "description": "mm vmstat return an index indicating how much of the available free memory is unusable for an allocation of the requested size",
            "id": "unusable_free_index"
        },
        "buffer_migrate_page": {
            "ground_truth": "0",
            "function": "buffer_migrate_page",
            "description": "migration function for page with buffer this function can only be used if the underlying filesystem guarantee that no other reference to page exist for example attached buffer head are accessed only under page lock",
            "id": "buffer_migrate_page"
        },
        "pcpu_stats_chunk_alloc": {
            "ground_truth": "0",
            "function": "pcpu_stats_chunk_alloc",
            "description": "mm percpu internal pcpu stats chunk alloc increment chunk stats",
            "id": "pcpu_stats_chunk_alloc"
        },
        "apparmor_socket_accept": {
            "ground_truth": "0",
            "function": "apparmor_socket_accept",
            "description": "security apparmor lsm check perm before accepting new connection struct socket sock undescribed struct socket newsock undescribed note while newsock is created and ha some information the accept ha not been done",
            "id": "apparmor_socket_accept"
        },
        "swp_swapcount": {
            "ground_truth": "0",
            "function": "swp_swapcount",
            "description": "include linux swap how many reference to entry are currently swapped out this considers count continued so it return exact answer",
            "id": "swp_swapcount"
        },
        "pageset_set_high": {
            "ground_truth": "0",
            "function": "pageset_set_high",
            "description": "mm page alloc pageset set high set the high water mark for hot per cpu pagelist to the value high for the pageset",
            "id": "pageset_set_high"
        },
        "validate_mmap_request": {
            "ground_truth": "0",
            "function": "validate_mmap_request",
            "description": "determine whether mapping should be permitted and if so what sort of mapping we are capable of supporting",
            "id": "validate_mmap_request"
        },
        "port_ptp_read": {
            "ground_truth": "0",
            "function": "port_ptp_read",
            "description": "access port scoped precision time protocol register",
            "id": "port_ptp_read"
        }
    }
}